#!/usr/bin/env python

global continue_run
global new_MATRIX_run
NLO_plus_loop_induced = True
#continue_run = False
#continue_run = True

#{{{ global definitions

global process_dir
global run_dir
global munich_dir
global start_dir
global no_run_folder
global out
global run_folder
global unique_parameters
global ordered_parameters
global renamed_parameters
global new_parameters
global special_parameters
global default_parameters
global mandatory_parameters
global MATRIX_parameters
global ord_params_keywords
global model_mappings_to_MUNICH
global ckm_processes
global LO_photon_induced_processes
global verbose
global run_modes
global run_mode
global parameter_list
global model_list
global nr_cores
global process_name
global config_list
global run # only needed for exit handler
global pre_run_settings
global resummation_map_NLO
global resummation_map_NNLO
global processes_with_loop_induced

#}}}
#{{{ general imports
import os
import glob
import atexit
import copy
import time
import datetime
import sys
import shutil
import multiprocessing
import subprocess
import select
import fnmatch
import tarfile
import math
import signal
import argparse
from os.path import join as pjoin
from collections import defaultdict
from random import randint
# no need anymore for numpy
#import numpy as np 
#}}}

#{{{ hard-coded lists and inputs before imports

# list of proper processes, that have been implemented and tested
script_link_dir = os.path.dirname(os.path.abspath(__file__)) # needed to determine process from name
process_dir = os.path.dirname(script_link_dir) # needed to determine existing runs
no_run_folder = ["default.grid.final","default.grid.final.tar","default.MATRIX","default.MATRIX.tar","batch","input","log","result","output","bin"]
proper_process_names = ["pph21","ppz01","ppeexa03","ppaa02","ppeex02","ppnenexa03","ppeeexex04","ppemexmx04","ppemexmx04NLOgg","ppemxnmnex04","ppemxnmnex04NLOgg","ppemxnmnex04EW","ppexnea03","ppenexa03","ppnenex02","ppexne02nockm","ppenex02nockm","ppexne02","ppenex02","ppemexnmx04","ppeexmxnm04","pphh22","ppeexexne04","ppeeexnex04","ppemexnmx04EW","ppeexmxnm04EW","ppeexexne04EW","ppeeexnex04EW","ppeexnenex04","ppeexnenex04NLOgg", "ppeexnmnmx04","ppeexnmnmx04NLOgg","ppzz02","ppw01nockm","ppwx01nockm","ppw01","ppwx01","ppwxw02","ppttx20","ppbbx20","pphjj41heft"]

processes_with_loop_induced = ["ppaa02","ppzz02","ppeexa03","ppnenexa03","ppeeexex04","ppemexmx04","ppemexmx04NLOgg","ppemxnmnex04","ppemxnmnex04NLOgg","ppemxnmnex04EW","ppeexnenex04","ppeexnenex04NLOgg", "ppeexnmnmx04", "ppeexnmnmx04NLOgg", "ppwxw02"]
#}}}
#{{{ import own modules
sys.path.append(pjoin(os.path.dirname(os.path.realpath(__file__)),"modules"))
from handle_cluster import *
from handle_lhapdf import lhapdf
from handle_output import banner,output_saver
from handle_folder import which
# import instances of different classes
from initialize_classes import out, prc, log, run_name, edit_input, fold, res, Tee, cite
import combine_distributions
from handle_gnuplot import gnuplot

#{{{ def: exit_handler(signal,frame)
def exit_handler(signal,frame):
    try:
        if run.runmode == "cluster" and run.jobs_started:
            open(pjoin(fold.run_folder_path,"cluster","job_ids_started.list"), 'a').close() # make sure this file exists (is at least empty)
            out.print_error_no_stop('Terminating all runs...')
            cluster.terminate_cluster_runs()
    except:
        pass
    try:
        # this removes the file that prevents running two instances at the same time
        out.print_error_no_stop('Removing lock file...')
        log.code_running_remove()
    except:
        pass
    out.print_error_no_stop('Exiting...')
    sys.exit(0)
#}}}
#{{{ def: atexit_handler()
def atexit_handler():
    try:
        if run.runmode == "cluster" and run.jobs_started:
            open(pjoin(fold.run_folder_path,"cluster","job_ids_started.list"), 'a').close() # make sure this file exists (is at least empty)
            cluster.terminate_cluster_runs()
    except:
        pass
    try:
        # this removes the file that prevents running two instances at the same time
        log.code_running_remove()
    except:
        pass
#}}}
#{{{ def control_c_handler(signal,frame)
def control_c_handler(signal,frame):
    # this is to kill cluster jobs which are still running 
    # when stopping the main code with ctrl-c
    print('You pressed ctrl-c!')
    exit_handler(signal,frame)
    sys.exit(0)
#}}}
signal.signal(signal.SIGINT,control_c_handler)
signal.signal(signal.SIGTERM,exit_handler)
atexit.register(atexit_handler)
#{{{ def multidim_dict(n)

def multidim_dict(n):
# extremely flexible multidimensional dictionary
  """ Creates an n-dimension dictionary where the n-th dimension is of type 'type'
  """  
  return defaultdict(lambda:multidim_dict(n-1))

#}}}
#from blessings import Terminal

# try:
#     from blessings import Terminal
#     print "test"
# except:
#     print "test"
#     pass

#}}}
#{{{ initial parameters and paths set
# hard-coded switch to turn on verbose mode
verbose = False
# important paths that are used all over the code
script_dir = os.path.dirname(os.path.realpath(__file__))
#script_link_dir = os.path.dirname(os.path.abspath(__file__)) # already above
#process_dir = os.path.dirname(script_link_dir) # already above
run_dir = os.path.dirname(process_dir)
munich_dir = os.path.dirname(script_dir)
start_dir = os.getcwd()
default_run = "default.grid.final"
new_MATRIX_run = False
if not os.path.exists(default_run):
    default_run = "default.MATRIX"
    new_MATRIX_run = True
default_input = "default.input.final"
if not os.path.exists(pjoin("input",default_input)):
    default_input = "default.input.MATRIX"
# we separate five classes of parameters that must be wrapped from the
# MATRIX to the MUNICH inputs
# !! Attention !! These list must be complete to avoid input parameters 
#                 which are wrong/not known

# 1. parameters that exist in both input files and are unique
# 2do: remove process_class completely ???
unique_parameters  = ["process_class","E","coll_choice","scale_fact","scale_ren","jet_algorithm","jet_R_definition","jet_R","photon_recombination","photon_R_definition","photon_R","photon_E_threshold_ratio","frixione_isolation","frixione_n","frixione_epsilon","frixione_delta_0","pdf_content_modify","pdf_selection","define_pT missing"] # add back later "switch_TSV","relative_central_scale_TSV","factor_scale_TSV","n_scale_TSV"
predefined_particles = ["jet","ljet","bjet","photon","lep","lm","lp","e","em","ep","mu","mum","mup","z","w","wm","wp","h","nua","nu","nux","nea","ne","nex","nma","nm","nmx"]
for particle in predefined_particles:
    unique_parameters.append("define_pT %s" % particle)
    unique_parameters.append("define_eta %s" % particle)
    unique_parameters.append("define_y %s" % particle)
    unique_parameters.append("n_observed_min %s" % particle)
    unique_parameters.append("n_observed_max %s" % particle)

# 2. new parameters that do not exist in, but have an effect on the MUNICH
#    input file, where the order is important
ordered_parameters = ["LHAPDF_LO","PDFsubset_LO","LHAPDF_NLO","PDFsubset_NLO","LHAPDF_NNLO","PDFsubset_NNLO"]
# these keywords are defined, so that one can uniquely connect the paramaters in MATRIX input with the ordered inputs in MUNICH
ord_params_keyword = {"LHAPDF_LO": ["type_perturbative_order","LO","LHAPDFname"],"LHAPDF_NLO": ["type_perturbative_order","NLO","LHAPDFname"],"LHAPDF_NNLO": ["type_perturbative_order","NNLO","LHAPDFname"],"LHAPDF_NNNLO": ["type_perturbative_order","NNNLO","LHAPDFname"],"PDFsubset_LO": ["type_perturbative_order","LO","LHAPDFsubset"],"PDFsubset_NLO": ["type_perturbative_order","NLO","LHAPDFsubset"],"PDFsubset_NNLO": ["type_perturbative_order","NNLO","LHAPDFsubset"],"PDFsubset_NNNLO": ["type_perturbative_order","NNNLO","LHAPDFsubset"]}
# 3. parameters that exist in both input files but have different names, these are added to file_parameter.dat if not exist
renamed_parameters = ["factor_central_scale","variation_factor","scale_res","dynamic_scale_res","factor_scale_res"]
renamed_parameter_mappings = {}
renamed_parameter_mappings["factor_central_scale"] = "prefactor_CV"
renamed_parameter_mappings["variation_factor"]     = "variation_factor_CV"
renamed_parameter_mappings["scale_res"]            = "user_cut Qres"
renamed_parameter_mappings["dynamic_scale_res"]    = "user_switch dynamical_Qres"
renamed_parameter_mappings["factor_scale_res"]     = "user_cut Qres_prefactor"
# 4. new parameters that do not concern the MUNICH input file directly
MATRIX_parameters  = ["run_LO","precision_LO","run_NLO_QCD","run_NLO_EW","add_NLL","precision_NLO_QCD","precision_NLO_EW","run_NNLO_QCD","add_NLO_EW","add_NNLL","precision_NNLO_QCD","precision_added_EW","max_time_per_job","save_previous_result","save_previous_log","include_pre_in_results","NLO_subtraction_method","check_interval","random_seed","print_out_interval","photon_induced",
# for backward compatibility:
"run_NLO","precision_NLO","run_NNLO","precision_NNLO","accuracy_LO","accuracy_NLO","accuracy_NNLO"]
# 5. special parameters that require a specific treatment
special_parameters = ["frixione_fixed_ET_max","dynamic_scale","switch_distribution","flavor_scheme","scale_variation","switch_off_shell","reduce_workload","switch_qT_accuracy","improve_mappings_for_single_V","only_gg_chan","loop_induced"]
# 6. user-defined parameters is a special class of parameters which depend on the process and are handled separately; these MUST all be set in the parameter.dat !
user_parameters = {}
user_parameters["pph21"] = []
user_parameters["pphh22"] = []
user_parameters["ppz01"] = []
user_parameters["ppnenex02"] = []
user_parameters["ppexne02nockm"] = []
user_parameters["ppenex02nockm"] = []
user_parameters["ppexne02"] = []
user_parameters["ppenex02"] = []
user_parameters["ppzz02"] = []
user_parameters["ppw01nockm"] = []
user_parameters["ppwx01nockm"] = []
user_parameters["ppw01"] = []
user_parameters["ppwx01"] = []
user_parameters["ppwxw02"] = []
user_parameters["ppttx20"] = []
user_parameters["ppbbx20"] = []
user_parameters["pphjj41heft"] = ["user_switch M_jetjet","user_cut min_M_jetjet","user_cut max_M_jetjet","user_switch absdy_jetjet","user_cut min_absdy_jetjet","user_cut max_absdy_jetjet"]
user_parameters["ppaa02"] = ["user_switch M_gamgam","user_cut min_M_gamgam","user_cut max_M_gamgam","user_switch pT_gam_1st","user_cut min_pT_gam_1st","user_switch gap_eta_gam","user_cut gap_min_eta_gam","user_cut gap_max_eta_gam","user_switch R_gamgam","user_cut min_R_gamgam"]
user_parameters["ppeexa03"] = ["user_switch M_leplep","user_cut min_M_leplep","user_cut min_M_lepgam","user_cut min_M_lepgam","user_switch R_lepgam","user_cut min_R_lepgam","user_switch R_lepjet","user_cut min_R_lepjet","user_switch R_gamjet","user_cut min_R_gamjet"]
user_parameters["ppeex02"] = ["user_switch M_leplep","user_cut min_M_leplep","user_cut max_M_leplep","user_switch R_leplep","user_cut min_R_leplep","user_switch lepton_cuts","user_cut min_pT_lep_1st","user_cut min_pT_lep_2nd"]
user_parameters["ppeeexex04"] = ["user_switch lepton_identification","user_switch M_Zrec","user_cut min_M_Zrec","user_cut max_M_Zrec","user_cut min_M_Z1","user_cut max_M_Z1","user_switch M_leplep_OSSF","user_cut min_M_leplep_OSSF","user_switch R_leplep","user_cut min_R_leplep","user_switch lepton_cuts","user_cut min_pT_lep_1st","user_cut max_eta_lep_1st","user_cut min_pT_lep_2nd","user_cut max_eta_lep_2nd","user_cut min_pT_lep_3rd","user_cut max_eta_lep_3rd","user_cut min_pT_lep_4th","user_cut max_eta_lep_4th","user_cut extra_eta_lep","user_int n_observed_min_lep_extra","user_int n_observed_max_lep_extra","user_switch M_4lep","user_cut min_M_4lep","user_cut max_M_4lep","user_cut min_delta_M_4lep","user_cut max_delta_M_4lep","user_switch lep_iso","user_cut lep_iso_delta_0","user_cut lep_iso_epsilon"]
user_parameters["ppemexmx04"] = ["user_switch M_Zrec","user_cut min_M_Zrec","user_cut max_M_Zrec","user_cut min_M_Z1","user_cut max_M_Z1","user_switch R_leplep","user_cut min_R_leplep","user_switch lepton_cuts","user_cut min_pT_lep_1st","user_cut max_eta_lep_1st","user_cut min_pT_lep_2nd","user_cut max_eta_lep_2nd","user_cut min_pT_lep_3rd","user_cut max_eta_lep_3rd","user_cut min_pT_lep_4th","user_cut max_eta_lep_4th","user_cut extra_eta_lep","user_int n_observed_min_lep_extra","user_int n_observed_max_lep_extra","user_switch electron_cuts","user_cut min_pT_e_1st","user_cut max_eta_e_1st","user_cut min_pT_e_2nd","user_cut max_eta_e_2nd","user_cut extra_eta_e","user_int n_observed_min_e_extra","user_int n_observed_max_e_extra","user_switch muon_cuts","user_cut min_pT_mu_1st","user_cut max_eta_mu_1st","user_cut min_pT_mu_2nd","user_cut max_eta_mu_2nd","user_cut extra_eta_mu","user_int n_observed_min_mu_extra","user_int n_observed_max_mu_extra","user_switch M_4lep","user_cut min_M_4lep","user_cut max_M_4lep","user_cut min_delta_M_4lep","user_cut max_delta_M_4lep","user_switch lep_iso","user_cut lep_iso_delta_0","user_cut lep_iso_epsilon"]
user_parameters["ppemexmx04NLOgg"] = ["user_switch M_Zrec","user_cut min_M_Zrec","user_cut max_M_Zrec","user_cut min_M_Z1","user_cut max_M_Z1","user_switch R_leplep","user_cut min_R_leplep","user_switch lepton_cuts","user_cut min_pT_lep_1st","user_cut max_eta_lep_1st","user_cut min_pT_lep_2nd","user_cut max_eta_lep_2nd","user_cut min_pT_lep_3rd","user_cut max_eta_lep_3rd","user_cut min_pT_lep_4th","user_cut max_eta_lep_4th","user_cut extra_eta_lep","user_int n_observed_min_lep_extra","user_int n_observed_max_lep_extra","user_switch electron_cuts","user_cut min_pT_e_1st","user_cut max_eta_e_1st","user_cut min_pT_e_2nd","user_cut max_eta_e_2nd","user_cut extra_eta_e","user_int n_observed_min_e_extra","user_int n_observed_max_e_extra","user_switch muon_cuts","user_cut min_pT_mu_1st","user_cut max_eta_mu_1st","user_cut min_pT_mu_2nd","user_cut max_eta_mu_2nd","user_cut extra_eta_mu","user_int n_observed_min_mu_extra","user_int n_observed_max_mu_extra","user_switch M_4lep","user_cut min_M_4lep","user_cut max_M_4lep","user_cut min_delta_M_4lep","user_cut max_delta_M_4lep","user_switch lep_iso","user_cut lep_iso_delta_0","user_cut lep_iso_epsilon"]
user_parameters["ppnenexa03"] = ["user_switch R_gamjet","user_cut min_R_gamjet"]
user_parameters["ppexnea03"] = ["user_switch R_gamjet","user_cut min_R_gamjet","user_switch R_lepgam","user_cut min_R_lepgam","user_switch R_lepjet","user_cut min_R_lepjet"]
user_parameters["ppenexa03"] = user_parameters["ppexnea03"]
user_parameters["ppemxnmnex04"] = ["user_switch M_leplep","user_cut min_M_leplep","user_cut max_M_leplep","user_switch R_leplep","user_cut min_R_leplep","user_switch pT_lep_1st","user_cut min_pT_lep_1st","user_switch gap_eta_e","user_cut gap_min_eta_e","user_cut gap_max_eta_e","user_switch rel_pT_miss","user_cut min_rel_pT_miss","user_switch R_ejet","user_cut min_R_ejet","user_switch pT_W","user_cut min_pT_W","user_cut max_pT_W"]#,"user_switch M_4lep","user_cut max_M_4lep"]
user_parameters["ppemxnmnex04NLOgg"] = ["user_switch M_leplep","user_cut min_M_leplep","user_cut max_M_leplep","user_switch R_leplep","user_cut min_R_leplep","user_switch pT_lep_1st","user_cut min_pT_lep_1st","user_switch gap_eta_e","user_cut gap_min_eta_e","user_cut gap_max_eta_e","user_switch rel_pT_miss","user_cut min_rel_pT_miss","user_switch R_ejet","user_cut min_R_ejet","user_switch pT_W","user_cut min_pT_W","user_cut max_pT_W"]#,"user_switch M_4lep","user_cut max_M_4lep"]
user_parameters["ppemxnmnex04EW"] = ["user_switch M_leplep","user_cut min_M_leplep","user_cut max_M_leplep","user_switch R_leplep","user_cut min_R_leplep","user_switch pT_lep_1st","user_cut min_pT_lep_1st","user_switch gap_eta_e","user_cut gap_min_eta_e","user_cut gap_max_eta_e","user_switch rel_pT_miss","user_cut min_rel_pT_miss","user_switch R_ejet","user_cut min_R_ejet","user_switch pT_W","user_cut min_pT_W","user_cut max_pT_W"]#,"user_switch M_4lep","user_cut max_M_4lep"]
user_parameters["ppeexexne04"] = ["user_switch lepton_identification","user_switch lepW_cuts","user_cut min_pT_lepW","user_cut max_eta_lepW","user_switch M_Zrec","user_cut min_M_Zrec","user_cut max_M_Zrec","user_switch delta_M_Zrec_MZ","user_cut max_delta_M_Zrec_MZ","user_switch R_leplep","user_cut min_R_leplep","user_switch R_lepZlepZ","user_cut min_R_lepZlepZ","user_switch R_lepZlepW","user_cut min_R_lepZlepW","user_switch MT_Wrec","user_cut min_MT_Wrec","user_switch lepZ_cuts","user_cut min_pT_lepZ_1st","user_cut min_pT_lepZ_2nd","user_switch M_leplep_OSSF","user_cut min_M_leplep_OSSF","user_switch delta_M_lepleplep_MZ","user_cut min_delta_M_lepleplep_MZ","user_switch lepton_cuts","user_cut min_pT_lep_1st","user_cut min_pT_lep_2nd"]
user_parameters["ppeeexnex04"] = ["user_switch lepton_identification","user_switch lepW_cuts","user_cut min_pT_lepW","user_cut max_eta_lepW","user_switch M_Zrec","user_cut min_M_Zrec","user_cut max_M_Zrec","user_switch delta_M_Zrec_MZ","user_cut max_delta_M_Zrec_MZ","user_switch R_leplep","user_cut min_R_leplep","user_switch R_lepZlepZ","user_cut min_R_lepZlepZ","user_switch R_lepZlepW","user_cut min_R_lepZlepW","user_switch MT_Wrec","user_cut min_MT_Wrec","user_switch lepZ_cuts","user_cut min_pT_lepZ_1st","user_cut min_pT_lepZ_2nd","user_switch M_leplep_OSSF","user_cut min_M_leplep_OSSF","user_switch delta_M_lepleplep_MZ","user_cut min_delta_M_lepleplep_MZ","user_switch lepton_cuts","user_cut min_pT_lep_1st","user_cut min_pT_lep_2nd"]
user_parameters["ppemexnmx04"] = ["user_switch M_Zrec","user_cut min_M_Zrec","user_cut max_M_Zrec","user_switch delta_M_Zrec_MZ","user_cut max_delta_M_Zrec_MZ","user_switch R_leplep","user_cut min_R_leplep","user_switch delta_M_lepleplep_MZ","user_cut min_delta_M_lepleplep_MZ","user_switch R_lepZlepZ","user_cut min_R_lepZlepZ","user_switch R_lepZlepW","user_cut min_R_lepZlepW","user_switch MT_Wrec","user_cut min_MT_Wrec","user_switch electron_cuts","user_cut min_pT_e_1st","user_cut min_pT_e_2nd","user_switch muon_cuts","user_cut min_pT_mu_1st","user_cut min_pT_mu_2nd","user_switch delta_M_lepleplep_MZ","user_cut min_delta_M_lepleplep_MZ","user_switch lepton_cuts","user_cut min_pT_lep_1st","user_cut min_pT_lep_2nd","user_switch leading_lepton_cuts","user_cut min_pT_1st_if_e","user_cut min_pT_1st_if_mu"]
user_parameters["ppeexmxnm04"] = ["user_switch M_Zrec","user_cut min_M_Zrec","user_cut max_M_Zrec","user_switch delta_M_Zrec_MZ","user_cut max_delta_M_Zrec_MZ","user_switch R_leplep","user_cut min_R_leplep","user_switch delta_M_lepleplep_MZ","user_cut min_delta_M_lepleplep_MZ","user_switch R_lepZlepZ","user_cut min_R_lepZlepZ","user_switch R_lepZlepW","user_cut min_R_lepZlepW","user_switch MT_Wrec","user_cut min_MT_Wrec","user_switch electron_cuts","user_cut min_pT_e_1st","user_cut min_pT_e_2nd","user_switch muon_cuts","user_cut min_pT_mu_1st","user_cut min_pT_mu_2nd","user_switch delta_M_lepleplep_MZ","user_cut min_delta_M_lepleplep_MZ","user_switch lepton_cuts","user_cut min_pT_lep_1st","user_cut min_pT_lep_2nd","user_switch leading_lepton_cuts","user_cut min_pT_1st_if_e","user_cut min_pT_1st_if_mu"]

user_parameters["ppeexexne04EW"] = ["user_switch lepton_identification","user_switch lepW_cuts","user_cut min_pT_lepW","user_cut max_eta_lepW","user_switch M_Zrec","user_cut min_M_Zrec","user_cut max_M_Zrec","user_switch delta_M_Zrec_MZ","user_cut max_delta_M_Zrec_MZ","user_switch R_leplep","user_cut min_R_leplep","user_switch R_lepZlepZ","user_cut min_R_lepZlepZ","user_switch R_lepZlepW","user_cut min_R_lepZlepW","user_switch MT_Wrec","user_cut min_MT_Wrec","user_switch lepZ_cuts","user_cut min_pT_lepZ_1st","user_cut min_pT_lepZ_2nd","user_switch M_leplep_OSSF","user_cut min_M_leplep_OSSF","user_switch delta_M_lepleplep_MZ","user_cut min_delta_M_lepleplep_MZ","user_switch lepton_cuts","user_cut min_pT_lep_1st","user_cut min_pT_lep_2nd"]
user_parameters["ppeeexnex04EW"] = ["user_switch lepton_identification","user_switch lepW_cuts","user_cut min_pT_lepW","user_cut max_eta_lepW","user_switch M_Zrec","user_cut min_M_Zrec","user_cut max_M_Zrec","user_switch delta_M_Zrec_MZ","user_cut max_delta_M_Zrec_MZ","user_switch R_leplep","user_cut min_R_leplep","user_switch R_lepZlepZ","user_cut min_R_lepZlepZ","user_switch R_lepZlepW","user_cut min_R_lepZlepW","user_switch MT_Wrec","user_cut min_MT_Wrec","user_switch lepZ_cuts","user_cut min_pT_lepZ_1st","user_cut min_pT_lepZ_2nd","user_switch M_leplep_OSSF","user_cut min_M_leplep_OSSF","user_switch delta_M_lepleplep_MZ","user_cut min_delta_M_lepleplep_MZ","user_switch lepton_cuts","user_cut min_pT_lep_1st","user_cut min_pT_lep_2nd"]
user_parameters["ppemexnmx04EW"] = ["user_switch M_Zrec","user_cut min_M_Zrec","user_cut max_M_Zrec","user_switch delta_M_Zrec_MZ","user_cut max_delta_M_Zrec_MZ","user_switch R_leplep","user_cut min_R_leplep","user_switch delta_M_lepleplep_MZ","user_cut min_delta_M_lepleplep_MZ","user_switch R_lepZlepZ","user_cut min_R_lepZlepZ","user_switch R_lepZlepW","user_cut min_R_lepZlepW","user_switch MT_Wrec","user_cut min_MT_Wrec","user_switch electron_cuts","user_cut min_pT_e_1st","user_cut min_pT_e_2nd","user_switch muon_cuts","user_cut min_pT_mu_1st","user_cut min_pT_mu_2nd","user_switch delta_M_lepleplep_MZ","user_cut min_delta_M_lepleplep_MZ","user_switch lepton_cuts","user_cut min_pT_lep_1st","user_cut min_pT_lep_2nd","user_switch leading_lepton_cuts","user_cut min_pT_1st_if_e","user_cut min_pT_1st_if_mu"]
user_parameters["ppeexmxnm04EW"] = ["user_switch M_Zrec","user_cut min_M_Zrec","user_cut max_M_Zrec","user_switch delta_M_Zrec_MZ","user_cut max_delta_M_Zrec_MZ","user_switch R_leplep","user_cut min_R_leplep","user_switch delta_M_lepleplep_MZ","user_cut min_delta_M_lepleplep_MZ","user_switch R_lepZlepZ","user_cut min_R_lepZlepZ","user_switch R_lepZlepW","user_cut min_R_lepZlepW","user_switch MT_Wrec","user_cut min_MT_Wrec","user_switch electron_cuts","user_cut min_pT_e_1st","user_cut min_pT_e_2nd","user_switch muon_cuts","user_cut min_pT_mu_1st","user_cut min_pT_mu_2nd","user_switch delta_M_lepleplep_MZ","user_cut min_delta_M_lepleplep_MZ","user_switch lepton_cuts","user_cut min_pT_lep_1st","user_cut min_pT_lep_2nd","user_switch leading_lepton_cuts","user_cut min_pT_1st_if_e","user_cut min_pT_1st_if_mu"]


# ============
# user_parameters["ppemxnmnex04EW"] = ["user_switch M_leplep","user_cut min_M_leplep","user_cut max_M_leplep","user_switch R_leplep","user_cut min_R_leplep","user_switch pT_lep_1st","user_cut min_pT_lep_1st","user_switch gap_eta_e","user_cut gap_min_eta_e","user_cut gap_max_eta_e","user_switch rel_pT_miss","user_cut min_rel_pT_miss","user_switch R_ejet","user_cut min_R_ejet","user_switch pT_W","user_cut min_pT_W","user_cut max_pT_W"]#,"user_switch M_4lep","user_cut max_M_4lep"]
# user_parameters["ppemexnmx04EW"] = ["user_switch M_Zrec","user_cut min_M_Zrec","user_cut max_M_Zrec","user_switch delta_M_Zrec_MZ","user_cut max_delta_M_Zrec_MZ","user_switch R_leplep","user_cut min_R_leplep","user_switch delta_M_lepleplep_MZ","user_cut min_delta_M_lepleplep_MZ","user_switch R_lepZlepZ","user_cut min_R_lepZlepZ","user_switch R_lepZlepW","user_cut min_R_lepZlepW","user_switch MT_Wrec","user_cut min_MT_Wrec","user_switch electron_cuts","user_cut min_pT_1st_e","user_cut min_pT_2nd_e"]
# user_parameters["ppeexmxnm04EW"] = ["user_switch M_Zrec","user_cut min_M_Zrec","user_cut max_M_Zrec","user_switch delta_M_Zrec_MZ","user_cut max_delta_M_Zrec_MZ","user_switch R_leplep","user_cut min_R_leplep","user_switch delta_M_lepleplep_MZ","user_cut min_delta_M_lepleplep_MZ","user_switch R_lepZlepZ","user_cut min_R_lepZlepZ","user_switch R_lepZlepW","user_cut min_R_lepZlepW","user_switch MT_Wrec","user_cut min_MT_Wrec","user_switch electron_cuts","user_cut min_pT_1st_e","user_cut min_pT_2nd_e"]
# user_parameters["ppeexexne04"] = ["user_switch lepton_identification","user_switch W_lep_cuts","user_cut min_pT_W_lep","user_cut max_eta_W_lep","user_switch M_Zrec","user_cut min_M_Zrec","user_cut max_M_Zrec","user_switch delta_M_Zrec_MZ","user_cut max_delta_M_Zrec_MZ","user_switch R_leplep","user_cut min_R_leplep","user_switch R_lepZlepZ","user_cut min_R_lepZlepZ","user_switch R_lepZlepW","user_cut min_R_lepZlepW","user_switch MT_Wrec","user_cut min_MT_Wrec","user_switch Z_lep_cuts","user_cut min_pT_1st_Z_lep","user_cut min_pT_2nd_Z_lep"]
# user_parameters["ppeeexnex04"] = ["user_switch lepton_identification","user_switch W_lep_cuts","user_cut min_pT_W_lep","user_cut max_eta_W_lep","user_switch M_Zrec","user_cut min_M_Zrec","user_cut max_M_Zrec","user_switch delta_M_Zrec_MZ","user_cut max_delta_M_Zrec_MZ","user_switch R_leplep","user_cut min_R_leplep","user_switch R_lepZlepZ","user_cut min_R_lepZlepZ","user_switch R_lepZlepW","user_cut min_R_lepZlepW","user_switch MT_Wrec","user_cut min_MT_Wrec","user_switch Z_lep_cuts","user_cut min_pT_1st_Z_lep","user_cut min_pT_2nd_Z_lep"]
# user_parameters["ppemexnmx04"] = ["user_switch M_Zrec","user_cut min_M_Zrec","user_cut max_M_Zrec","user_switch delta_M_Zrec_MZ","user_cut max_delta_M_Zrec_MZ","user_switch R_leplep","user_cut min_R_leplep","user_switch delta_M_lepleplep_MZ","user_cut min_delta_M_lepleplep_MZ","user_switch R_lepZlepZ","user_cut min_R_lepZlepZ","user_switch R_lepZlepW","user_cut min_R_lepZlepW","user_switch MT_Wrec","user_cut min_MT_Wrec","user_switch electron_cuts","user_cut min_pT_1st_e","user_cut min_pT_2nd_e"]
# user_parameters["ppeexmxnm04"] = ["user_switch M_Zrec","user_cut min_M_Zrec","user_cut max_M_Zrec","user_switch delta_M_Zrec_MZ","user_cut max_delta_M_Zrec_MZ","user_switch R_leplep","user_cut min_R_leplep","user_switch delta_M_lepleplep_MZ","user_cut min_delta_M_lepleplep_MZ","user_switch R_lepZlepZ","user_cut min_R_lepZlepZ","user_switch R_lepZlepW","user_cut min_R_lepZlepW","user_switch MT_Wrec","user_cut min_MT_Wrec","user_switch electron_cuts","user_cut min_pT_1st_e","user_cut min_pT_2nd_e"]
# user_parameters["ppeexexne04EW"] = ["user_switch lepton_identification","user_switch W_lep_cuts","user_cut min_pT_W_lep","user_cut max_eta_W_lep","user_switch M_Zrec","user_cut min_M_Zrec","user_cut max_M_Zrec","user_switch delta_M_Zrec_MZ","user_cut max_delta_M_Zrec_MZ","user_switch R_leplep","user_cut min_R_leplep","user_switch R_lepZlepZ","user_cut min_R_lepZlepZ","user_switch R_lepZlepW","user_cut min_R_lepZlepW","user_switch MT_Wrec","user_cut min_MT_Wrec","user_switch Z_lep_cuts","user_cut min_pT_1st_Z_lep","user_cut min_pT_2nd_Z_lep"]
# user_parameters["ppeeexnex04EW"] = ["user_switch lepton_identification","user_switch W_lep_cuts","user_cut min_pT_W_lep","user_cut max_eta_W_lep","user_switch M_Zrec","user_cut min_M_Zrec","user_cut max_M_Zrec","user_switch delta_M_Zrec_MZ","user_cut max_delta_M_Zrec_MZ","user_switch R_leplep","user_cut min_R_leplep","user_switch R_lepZlepZ","user_cut min_R_lepZlepZ","user_switch R_lepZlepW","user_cut min_R_lepZlepW","user_switch MT_Wrec","user_cut min_MT_Wrec","user_switch Z_lep_cuts","user_cut min_pT_1st_Z_lep","user_cut min_pT_2nd_Z_lep"]
# >>>>>>> develop_EW

user_parameters["ppeexnenex04"] = ["user_switch M_leplep","user_cut min_M_leplep","user_cut max_M_leplep","user_switch M_leplepnunu","user_cut min_M_leplepnunu","user_cut max_M_leplepnunu","user_cut min_delta_M_leplepnunu","user_cut max_delta_M_leplepnunu"]
user_parameters["ppeexnenex04NLOgg"] = ["user_switch M_leplep","user_cut min_M_leplep","user_cut max_M_leplep","user_switch M_leplepnunu","user_cut min_M_leplepnunu","user_cut max_M_leplepnunu","user_cut min_delta_M_leplepnunu","user_cut max_delta_M_leplepnunu"]
user_parameters["ppeexnmnmx04"] = ["user_switch M_leplep","user_cut min_M_leplep","user_cut max_M_leplep","user_switch M_leplepnunu","user_cut min_M_leplepnunu","user_cut max_M_leplepnunu","user_cut min_delta_M_leplepnunu","user_cut max_delta_M_leplepnunu"]
user_parameters["ppeexnmnmx04NLOgg"] = ["user_switch M_leplep","user_cut min_M_leplep","user_cut max_M_leplep","user_switch M_leplepnunu","user_cut min_M_leplepnunu","user_cut max_M_leplepnunu","user_cut min_delta_M_leplepnunu","user_cut max_delta_M_leplepnunu"]
# 2do: now also define mandatory cuts, which must be set, and its value restrictions


# define mandatory parameters to be cross checked if in parameter.dat
mandatory_parameters = ["E","coll_choice","save_previous_result","save_previous_log","NLO_subtraction_method"]
# define default values for parameters
default_parameters = {}
default_parameters["NLO_subtraction_method"] = 1 # use CS subtraction
default_parameters["loop_induced"] = 0 # no loop-induced contribution
default_parameters["max_time_per_job"] = 24 # hours
default_parameters["LHAPDF_LO"]   = "NNPDF30_lo_as_0118"
default_parameters["LHAPDF_NLO"]  = "NNPDF30_nlo_as_0118"
default_parameters["LHAPDF_NNLO"] = "NNPDF30_nnlo_as_0118"
# interval (in seconds) how often the status (queued/running/finished jobs) is printed out
default_parameters["print_out_interval"] = 60*5
# interval (in seconds) how often the jobs on the cluster are checked (and stopped/restarted/etc.)
default_parameters["check_interval"] = 30#60*2
#default_parameters["switch_qT_accuracy"] = "-99"

# the VT2 contribution for some processes is to slow to run 50000 events in the pre-run; 
# for these processes we can hardcode in the dictionary below (from experience) how many 
# events are roughly needed to achieve 0.001% relative precision to the total NNLO cross 
# section and how long (in seconds) this will take: [events,seconds]
VT2_use_default_runtime = {}
#VT2_use_default_runtime["ppeeexex04"] = [100000,100000] # adjust from experience (note: every main run gives you new information about the runtimes so that you can improve)
# NNLO.QT-CS/24/VT2.QCD    dd~_ememepep              751834         358419     0.00017508925      5.5958941  0.00097978093
# NNLO.QT-CS/24/VT2.QCD    uu~_ememepep             1810483         893342     0.00027170381      5.5958941   0.0015204258
# below all model parameter are defined as a mapping from the SLHA format to the name how they are defined in MUNICH

# new idea: use higher parallelization and events for extrapolation runs and define specific settings for different contributions
# to be able to do it later process specific we do a dictionary of a dictionary of a list [parallelization,events]
default_settings = {}
default_settings["loop"]    = [5,50000]
default_settings["VT2.QCD"] = [5,1000000] # [50,100000] 
default_settings["RVA.QCD"] = [1,1000000]  # [10,100000]
default_settings["RRA.QCD"] = [ 1,200000]
# default_settings["RA.QEW"] = [1,50000] # [50,100000] 
# default_settings["CA.QEW"] = [1,50000]  # [10,100000]
default_settings["VA.QEW"] = [25,50000]
pre_run_settings = {}
pre_run_settings["default"] = default_settings
# for process specific settings use the default settings simply overwrite the ones that need to be changed
ppeeexex04_settings = copy.copy(default_settings)
ppeeexex04_settings["VT2.QCD"] = [100,100000]
ppeeexex04_settings["RVA.QCD"] = [ 20,100000]
ppeeexex04_settings["L2RT.QCD"] = [ 10,50000]
ppeeexex04_settings["L2VT.QCD"] = [ 100,25000]
ppeeexex04_settings["L2RA.QCD"] = [ 10,50000]
ppeeexex04_settings["L2VA.QCD"] = [ 100,25000]
# and add them to a process specific key
pre_run_settings["ppeeexex04"] = ppeeexex04_settings
ppemxnmnex04_settings = copy.copy(default_settings)
ppemxnmnex04_settings["VT2.QCD"] = [100,100000]
ppemxnmnex04_settings["RVA.QCD"] = [ 20,100000]
ppemxnmnex04_settings["L2RT.QCD"] = [ 10,50000]
ppemxnmnex04_settings["L2VT.QCD"] = [ 100,25000]
ppemxnmnex04_settings["L2RA.QCD"] = [ 10,50000]
ppemxnmnex04_settings["L2VA.QCD"] = [ 100,25000]
# process specific settings for pphjj
pphjj41heft_settings = copy.copy(default_settings)
pphjj41heft_settings["VA.QCD"] = [2,10000]
pre_run_settings["pphjj41heft"] = pphjj41heft_settings
# process specific settings for ppttx20
ppttx20_settings = copy.copy(default_settings)
ppttx20_settings["RVA.QCD"] = [5,500000]
ppttx20_settings["VT2.QCD"] = [1,100000]
ppttx20_settings["RRA.QCD"] = [2,1000000]
ppttx20_settings["CT2.QCD"] = [1,500000]
ppttx20_settings["RCA.QCD"] = [1,5000000]
pre_run_settings["ppttx20"] = ppttx20_settings

pre_run_settings["ppemxnmnex04"] = ppemxnmnex04_settings
pre_run_settings["ppemxnmnex04EW"] = ppemxnmnex04_settings
pre_run_settings["ppemxnmnex04NLOgg"] = ppemxnmnex04_settings
pre_run_settings["ppeexa03"] = ppeeexex04_settings
pre_run_settings["ppnenexa03"] = ppeeexex04_settings
pre_run_settings["ppemexnmx04"] = ppeeexex04_settings
pre_run_settings["ppenexa03"] = ppeeexex04_settings
pre_run_settings["ppexnea03"] = ppeeexex04_settings
pre_run_settings["ppemxnmnex04"] = ppeeexex04_settings
pre_run_settings["ppemexmx04"] = ppeeexex04_settings
pre_run_settings["ppemexmx04NLOgg"] = ppeeexex04_settings
pre_run_settings["ppeexmxnm04"] = ppeeexex04_settings
pre_run_settings["ppeexexne04"] = ppeeexex04_settings
pre_run_settings["ppeeexnex04"] = ppeeexex04_settings
pre_run_settings["ppeexnenex04"] = ppeeexex04_settings
pre_run_settings["ppeexnenex04NLOgg"] = ppeeexex04_settings
pre_run_settings["ppeexnmnmx04"] = ppeeexex04_settings
pre_run_settings["ppeexnmnmx04NLOgg"] = ppeeexex04_settings
pre_run_settings["ppzz02"] = ppeeexex04_settings
pre_run_settings["ppwxw02"] = ppeeexex04_settings
pre_run_settings_copy = copy.copy(pre_run_settings)
for process in pre_run_settings_copy:
    pre_run_settings[process+"EW"] = pre_run_settings[process]

model_mappings_to_MUNICH = multidim_dict(2)
#[Block][number]=parameter_name_in_MUNICH
model_mappings_to_MUNICH["MASS"][1]="M_d"
model_mappings_to_MUNICH["MASS"][2]="M_u"
model_mappings_to_MUNICH["MASS"][3]="M_s"
model_mappings_to_MUNICH["MASS"][4]="M_c"
model_mappings_to_MUNICH["MASS"][5]="M_b"
model_mappings_to_MUNICH["MASS"][6]="M_t"
model_mappings_to_MUNICH["MASS"][11]="M_e"
model_mappings_to_MUNICH["MASS"][12]="M_ve"
model_mappings_to_MUNICH["MASS"][13]="M_mu"
model_mappings_to_MUNICH["MASS"][14]="M_vm"
model_mappings_to_MUNICH["MASS"][15]="M_tau"
model_mappings_to_MUNICH["MASS"][16]="M_vt"
model_mappings_to_MUNICH["MASS"][23]="M_Z"
model_mappings_to_MUNICH["MASS"][24]="M_W"
model_mappings_to_MUNICH["MASS"][25]="M_H"
model_mappings_to_MUNICH["SMINPUTS"][1]="1/alpha_e_MZ"
model_mappings_to_MUNICH["SMINPUTS"][2]="G_F"
model_mappings_to_MUNICH["SMINPUTS"][111]="1/alpha_e_0"
model_mappings_to_MUNICH["EWINPUTS"][1]="ew_scheme"
model_mappings_to_MUNICH["EWINPUTS"][2]="use_adapted_ew_coupling"
model_mappings_to_MUNICH["EWINPUTS"][3]="use_cms"
model_mappings_to_MUNICH["CKM"][11]="V_du"
model_mappings_to_MUNICH["CKM"][12]="V_su"
model_mappings_to_MUNICH["CKM"][13]="V_bu"
model_mappings_to_MUNICH["CKM"][21]="V_dc"
model_mappings_to_MUNICH["CKM"][22]="V_sc"
model_mappings_to_MUNICH["CKM"][23]="V_bc"
model_mappings_to_MUNICH["CKM"][31]="V_dt"
model_mappings_to_MUNICH["CKM"][32]="V_st"
model_mappings_to_MUNICH["CKM"][33]="V_bt"
model_mappings_to_MUNICH["VCKMIN"][1]="theta_c" # Cabibbo angle
# 2do? Currently have all decay widths at all orders (LO, NLO and NNLO) the same value
model_mappings_to_MUNICH["DECAY"][6]="Gamma_t"
model_mappings_to_MUNICH["DECAY"][23]="Gamma_Z"
model_mappings_to_MUNICH["DECAY"][24]="Gamma_W"
model_mappings_to_MUNICH["DECAY"][25]="Gamma_H"
# define all processes with CKM matrix
ckm_processes = ["ppexne02","ppenex02","ppw01","ppwx01"]
LO_photon_induced_processes = ["ppemxnmnex04EW"]
# mappings which contributions are replaced in resummation mode
# NLO+NLL mappings
resummation_map_NLO = {}
resummation_map_NLO["born"] = "NLL_LO"
resummation_map_NLO["VT.QCD"]   = "NLL_NLO"
resummation_map_NLO["CT.QCD"]   = "CT"
# NNLO+NNLL mappings
resummation_map_NNLO = {}      
resummation_map_NNLO["born"] = "NNLL_LO"
resummation_map_NNLO["VT.QCD"]   = "NNLL_NLO"
resummation_map_NNLO["CT.QCD"]   = "CT"
resummation_map_NNLO["VT2.QCD"]  = "NNLL_NNLO"
resummation_map_NNLO["CT2.QCD"]   = "CT2"

# possible run_modes (that are already working)
run_modes = []
run_modes.append("run")                 # normal run
run_modes.append("run_without_pre")     # same "run", but without pre run using pre-defined runtime.dat file
run_modes.append("run_grid")            # do only grid run
run_modes.append("run_pre")             # do only pre run (grid_run must be already there)
run_modes.append("run_pre_and_main")    # do only main run (grid_run and pre run must be already there)
run_modes.append("run_main")            # do only main run (grid_run and pre run must be already there)
run_modes.append("run_main_without_pre") # same as "run_main", but without pre run using pre-defined runtime.dat file
run_modes.append("run_results")         # do only results run
run_modes.append("run_gnuplot")           # do only gnuplotting
run_modes.append("setup_run")           # do only set up the folder (no runs)
run_modes.append("delete_run")          # removes the run folder including all inputs/logs/results
run_modes.append("tar_run")             # create .tar archive of run folder including all inputs/logs/results
edit_input.run_modes = run_modes

#}}}

# parse command line arguments after default inputs
parser = argparse.ArgumentParser(description='MATRIX.')
parser.add_argument('run_folder', metavar='<run folder>', nargs='?', default="", help='run folder, must start with run_')
parser.add_argument('--input_dir', dest='input_dir', action='store', default=default_input, help='Specify directory inside input folder from where template MATRIX input files are taken (default: use \"%s\" folder)' % default_input)
parser.add_argument('--run_mode', dest='run_mode', action='store', default="", help='Specify run mode (RUN_MODE=\"run\"/\"run_grid\"/\"run_pre\"/\"run_pre_and_main\"/\"run_result\"/"run_gnuplot\")')
parser.add_argument('--delete_run',dest='delete_run', action='store_true', help='Remove run folder (including input/log/result).')
parser.add_argument('--setup_run',dest='setup_run', action='store_true', help='Setup the run folder, but not start running.')
parser.add_argument('--tar_run',dest='tar_run', action='store_true', help='Create .tar archive of run folder (including input/log/result).')
parser.add_argument('--change_name_to', dest='new_name', action='store', default="", help='Rename run folder (including input/log/result).')
parser.add_argument('--copy_run_from', dest='existing_run', action='store', default="", help='Copy run folder from existing run (including input/log/result).')
parser.add_argument('-c','--continue',dest='continue_run', action='store_true', help='Continue the previous run from the specified run_mode; important: make sure the inputs are consistent!')
args = parser.parse_args()    


#{{{ class: OutputSaver
# class OutputSaver(object):
#     def __init__(self,outfile):
#         self.terminal = sys.stdout
#         self.log = open(outfile, "w")

#     def write(self, message):
#         self.terminal.write(message)
#         self.log.write(message)  

#     def write_only_file(self, message):
#         self.log.write(message)  

#     def flush(self):
#         #this flush method is needed for python 3 compatibility.
#         #this handles the flush command by doing nothing.
#         #you might want to specify some extra behavior here.
#         pass    
#}}}
#{{{ class: inputs
class inputs():
    """Class to readin user inputs, wrap them and adjust MUNICH inputs"""
#{{{ def: __init__(self)
# in case any default inputs are required
#    def __init__(self):
#}}}
#{{{ def: input_change_entry(self,file_path,parameter,value)
    def input_change_entry(self,file_path,parameter,value):
# function to change a single parameter (all occurences) of MUNICH input files
# if paramater does not exist in this file it is not set
# prop 2do: depending on parameter one could allow only for certain value types
        with open(file_path+".replace",'w') as new_file:
            with open(file_path, 'r') as in_file:
                for in_line in in_file:
                    line=in_line.strip() # strip removes all spaces (including tabs and newlines)
                    # if any line starts with %, # or is an emtpy line (disregarding spaces) it is a comment line and should be skipped
                    if line=="" or line[0]=="%" or line[0]=="#": 
                        new_file.write(in_line)
                        continue
                    # split line by "=" and remove spaces again from string
                    # [0] gives you the parameter (before "="-sign)
                    file_parameter=line.split('=',1)[0].strip()
                    # [1] gives you the value (after "="-sign); remove trailing % or # and everything after in this case
                    file_value=line.split('=',1)[1].split("%")[0].split("#")[0].strip()
                    if file_parameter == parameter:
                        new_file.write("%s = %s \n" %(file_parameter,value))
                    else:
                        new_file.write(in_line)
        os.remove(file_path)
        shutil.move(file_path+".replace",file_path)
#}}}
#{{{ def: input_set_entry(self,file_path,parameter,value)
    def input_set_entry(self,file_path,parameter,value):
# function to set a single parameter (all occurences) of MUNICH input files
# if paramater does not exist in this file it added at the end of 
# prop 2do: depending on parameter one could allow only for certain value types
        if not os.path.isfile(file_path):
            open(file_path, 'a').close()
        parameter_set = False
        with open(file_path+".replace",'w') as new_file:
            with open(file_path, 'r') as in_file:
                for in_line in in_file:
                    line=in_line.strip() # strip removes all spaces (including tabs and newlines)
                    # if any line starts with %, # or is an emtpy line (disregarding spaces) it is a comment line and should be skipped
                    if line=="" or line[0]=="%" or line[0]=="#": 
                        new_file.write(in_line)
                        continue
                    # split line by "=" and remove spaces again from string
                    # [0] gives you the parameter (before "="-sign)
                    file_parameter=line.split('=',1)[0].strip()
                    # [1] gives you the value (after "="-sign); remove trailing % or # and everything after in this case
                    file_value=line.split('=',1)[1].split("%")[0].split("#")[0].strip()
                    if file_parameter == parameter:
                        new_file.write("%s = %s \n" %(file_parameter,value))
                        parameter_set = True
                    else:
                        new_file.write(in_line)
                if not parameter_set:
                    new_file.write("%s = %s \n" %(parameter,value))
        os.remove(file_path)
        shutil.move(file_path+".replace",file_path)
#}}}
#{{{ def: input_add_entry(self,file_path,parameter,value)
    def input_add_entry(self,file_path,parameter,value):
# function to add a single parameter at the end of a MUNICH input file
        with open(file_path,'a') as in_file:
            if parameter == "" and value == "":
                in_file.write("\n")
            else:
                in_file.write("%s = %s \n" %(parameter,value))
#}}}
#{{{ def: input_remove_entry(self,file_path,parameter)
    def input_remove_entry(self,file_path,parameter):
# function to remove a single parameter (all occurences) from MUNICH input files
# if paramater does not exists nothing happens
        with open(file_path+".replace",'w') as new_file:
            with open(file_path, 'r') as in_file:
                for in_line in in_file:
                    line=in_line.strip() # strip removes all spaces (including tabs and newlines)
                    # if any line starts with %, # or is an emtpy line (disregarding spaces) it is a comment line and should be skipped
                    if line=="" or line[0]=="%" or line[0]=="#": 
                        new_file.write(in_line)
                        continue
                    # split line by "=" and remove spaces again from string
                    # [0] gives you the parameter (before "="-sign)
                    file_parameter=line.split('=',1)[0].strip()
                    # [1] gives you the value (after "="-sign); remove trailing % or # and everything after in this case
                    file_value=line.split('=',1)[1].split("%")[0].split("#")[0].strip()
                    if file_parameter == parameter:
                        continue
                    else:
                        new_file.write(in_line)
        os.remove(file_path)
        shutil.move(file_path+".replace",file_path)
#}}}
#{{{ def: input_search_keyword_set_entry_below(self,file_path,parameter,value)
    def input_search_keyword_set_entry_below(self,file_path,parameter,value):
# function to set single parameter (first occurence) of MUNICH input files
# prop 2do: depending on parameter one could allow only for certain value types
        # initial condition: keyword_needs to be found, and parameter needs to be set
        keyword_found = False
        parameter_set = False
        with open(file_path+".replace",'w') as new_file:
            with open(file_path, 'r') as in_file:
                for in_line in in_file:
                    line=in_line.strip() # strip removes all spaces (including tabs and newlines)
                    # if any line starts with %, # or is an emtpy line (disregarding spaces) it is a comment line and should be skipped
                    # or once the parameter was set, stop searching for more occurences and simply write out the rest of the file
                    if line=="" or line[0]=="%" or line[0]=="#" or parameter_set: 
                        new_file.write(in_line)
                        continue
                    # split line by "=" and remove spaces again from string
                    # [0] gives you the parameter (before "="-sign)
                    file_parameter=line.split('=',1)[0].strip()
                    # [1] gives you the value (after "="-sign); remove trailing % or # and everything after in this case
                    file_value=line.split('=',1)[1].split("%")[0].split("#")[0].strip()
                    # check parameter names until you find the one of the key with the right value
                    if file_parameter == ord_params_keyword[parameter][0] and file_value == ord_params_keyword[parameter][1]:
                        keyword_found = True
                    # when keyword found search below the parameter you want to set
                    if keyword_found == True and file_parameter == ord_params_keyword[parameter][2]:
                        new_file.write(file_parameter+" = "+value+" \n")
                        # once the parameter was set, stop searching for more occurences
                        parameter_set = True
                    else:
                        new_file.write(in_line)
        os.remove(file_path)
        shutil.move(file_path+".replace",file_path)
        if not keyword_found:
            out.print_warning("Keyword \"%s\" with value \"%s\" in file_parameter.dat was not found, could not set parameter \"%s\" below to value \"%s\", continuing..."%(ord_params_keyword[parameter][0],ord_params_keyword[parameter][1],parameter,value))
        if not parameter_set:
            out.print_warning("Keyword \"%s\" with value \"%s\" in file_parameter.dat was found, but could not find parameter \"%s\" below and set it to value \"%s\", continuing..."%(ord_params_keyword[parameter][0],ord_params_keyword[parameter][1],ord_params_keyword[parameter][2],value))

#}}}
#{{{ def: input_read_and_check_distribution_dat(self,file_path,distribution_list)
    def input_read_and_check_distribution_dat(self,file_path,distribution_list):
# function to read all parameters from the MATRIX input file (file_path, "parameter.dat")
# and write it into a dictionary (parameter_list)
# works also for MATRIX configuration file
        if not os.path.isfile(file_path):
            out.print_error("file "+file_path+" in function input_read_and_check_distribution_dat does not exist!")
        with open(file_path, 'r') as distribution_file:
            for in_line in distribution_file:
                line=in_line.strip() # strip removes all spaces (including tabs and newlines)
                # if any line starts with %, # or is an emtpy line (disregarding spaces) it is a comment line and should be skipped
                if line=="" or line[0]=="%" or line[0]=="#":
                    continue
                # split line by "=" and remove spaces again from string
                # [0] gives you the parameter (before "="-sign)
                parameter=line.split('=',1)[0].strip()
                # [1] gives you the value (after "="-sign); remove trailing % or # and everything after in this case
                value=line.split('=',1)[1].strip()
                if parameter == "distributionname":
                    if value in distribution_list:
                        out.print_error("Same distributionname \"%s\" in distribution.dat used twice. Use unique identifiers and restart..." % distribution_name)
                    else:
                        distribution_name = value
                else:
                    distribution_list[distribution_name][parameter]=value
#}}}
#{{{ def: input_read_parameter_dat(self,file_path,parameter_list)
    def input_read_parameter_dat(self,file_path,parameter_list):
# function to read all parameters from the MATRIX input file (file_path, "parameter.dat")
# and write it into a dictionary (parameter_list)
# works also for MATRIX configuration file
        if not os.path.isfile(file_path):
            out.print_error("file "+file_path+" in function input_read_parameter_dat does not exist!")
        with open(file_path, 'r') as param_file:
            for in_line in param_file:
                line=in_line.strip() # strip removes all spaces (including tabs and newlines)
                # if any line starts with %, # or is an emtpy line (disregarding spaces) it is a comment line and should be skipped
                if line=="" or line[0]=="%" or line[0]=="#":
                    continue
                # split line by "=" and remove spaces again from string
                # [0] gives you the parameter (before "="-sign)
                file_parameter=line.split('=',1)[0].strip()
                # [1] gives you the value (after "="-sign); remove trailing % or # and everything after in this case
                try:
                    file_value=line.split('=',1)[1]
                    if file_value.strip().startswith("\""):
                        file_value=file_value.split("\"")[1]
                    else:
                        file_value=file_value.split("%")[0].split("#")[0].strip()
                except:
                    out.print_error_no_stop("Something wrong in parameter.dat file. The following line:")
                    print "\033[91m" + "                 "+line + "\033[0m"
                    out.print_error_no_stop("can not be understood. The structure must be:")
                    print "\033[91m" + "                 $parameter = $value" + "\033[0m"
                    out.print_error("and comments must be started by \"#\" or \"%%\". Exiting...")
                parameter_list[file_parameter]=file_value
#}}}
#{{{ def: input_set_file_parameter_from_list(self,file_path,parameter_list)
    def input_set_file_parameter_from_list(self,file_path,parameter_list):
# function to set all parameters in MUNICH input ("file_parameter.dat") from parameter_list 
# read from the MATRIX input file
        parameter_list_copy = copy.copy(parameter_list)
        for parameter in parameter_list_copy:
            value = parameter_list[parameter] 
            if parameter in unique_parameters:
                # all parameters uniquely defined can be directly set as MUNICH input
                self.input_set_entry(file_path,parameter,value)
            elif parameter in ordered_parameters:
                # these parameters require a certain order, the following function sets the dependend on some keyword
                # the keywords for specific parameters are defined as a dictionary in the very beginning
                self.input_search_keyword_set_entry_below(file_path,parameter,value)
                #if parameter == "LHAPDF_NNLO":
                #    self.input_search_keyword_set_entry_below(file_path,"LHAPDF_NNNLO",value)
                #if parameter == "PDFsubset_NNLO":
                #    self.input_search_keyword_set_entry_below(file_path,"PDFsubset_NNNLO",value)
            elif parameter in renamed_parameters:
                parameter = renamed_parameter_mappings[parameter]
                self.input_set_entry(file_path,parameter,value)
            elif parameter in MATRIX_parameters:
                # backwards compatibility:
                if parameter == "run_NLO":
                    parameter_list["run_NLO_QCD"] = value
                if parameter == "precision_NLO":
                    parameter_list["precision_NLO_QCD"] = value
                if parameter == "run_NNLO":
                    parameter_list["run_NNLO_QCD"] = value
                if parameter == "precision_NNLO":
                    parameter_list["precision_NNLO_QCD"] = value
                if parameter == "accuracy_LO":
                    parameter_list["precision_LO"] = value
                if parameter == "accuracy_NLO":
                    parameter_list["precision_NLO_QCD"] = value
                if parameter == "accuracy_NNLO":
                    parameter_list["precision_NNLO_QCD"] = value
                # nothing else to do in that case
                continue
            elif parameter in special_parameters:
                # these are special cases for the parameter inputs
                if parameter == "switch_off_shell" and value == "1":
                    self.input_add_entry(file_path,"type_perturbative_order","all")
                    self.input_add_entry(file_path,"type_contribution","all")
                    self.input_add_entry(file_path,"type_correction","all")
                    self.input_add_entry(file_path,"n_tau_events","5000")
                    self.input_add_entry(file_path,"n_x1x2_events","5000")
                    self.input_add_entry(file_path,"n_IS_events","50000")
                    self.input_add_entry(file_path,"n_IS_events_factor","1")
                    self.input_add_entry(file_path,"n_alpha_events","5000")
                    self.input_add_entry(file_path,"n_step","5000")
                    self.input_add_entry(file_path,"","")
                    self.input_add_entry(file_path,"type_perturbative_order","all")
                    self.input_add_entry(file_path,"type_contribution","loop")
                    self.input_add_entry(file_path,"type_correction","all")
                    self.input_add_entry(file_path,"n_tau_events","500")
                    self.input_add_entry(file_path,"n_x1x2_events","500")
                    self.input_add_entry(file_path,"n_IS_events","5000")
                    self.input_add_entry(file_path,"n_IS_events_factor","1")
                    self.input_add_entry(file_path,"n_alpha_events","500")
                    self.input_add_entry(file_path,"n_step","500")
                    self.input_add_entry(file_path,"","")
                elif parameter == "switch_distribution": 
                    self.input_change_entry(file_path,parameter,value)
                    parameter = "switch_output_distribution"
                    self.input_change_entry(file_path,parameter,value)
                elif parameter == "dynamic_scale":
                    self.input_change_entry(file_path,parameter,value)
                    parameter = "dynamic_scale_CV" 
                    self.input_change_entry(file_path,parameter,value)
                elif parameter == "scale_variation":
                    if value == "0": # no variation
                        self.input_change_entry(file_path,"switch_CV","5")
                        self.input_change_entry(file_path,"n_scales_CV","7")
                        parameter_list["variation_factor"] = "1"
                        self.input_change_entry(file_path,"variation_factor_CV","1")
                    if value == "1": # 7-point variation
                        self.input_change_entry(file_path,"switch_CV","5")
                        self.input_change_entry(file_path,"n_scales_CV","7")
                    if value == "2": # 9-point variation
                        self.input_change_entry(file_path,"switch_CV","6")
                        self.input_change_entry(file_path,"n_scales_CV","9")
                elif parameter == "frixione_fixed_ET_max":
                    parameter = "frixione_epsilon" 
                    value_new = float(value)/10
                    self.input_change_entry(file_path,parameter,value_new)
                elif parameter == "flavor_scheme":
                    if value == "1": # 4FS
                        self.input_change_entry(file_path,"N_f","4")
                        self.input_change_entry(file_path,"N_f_active","4")
                    elif value == "2": # 5FS
                        self.input_change_entry(file_path,"N_f","5")
                        self.input_change_entry(file_path,"N_f_active","5")
                elif parameter == "reduce_workload":
                    if value == "0":
                        pass
                    elif value == "1" or value == "2":
                        self.input_set_entry(file_path,"switch_output_execution","1")   # needed for script to check wether job is finished
                        self.input_set_entry(file_path,"switch_output_result","1")      # needed for result output
                        self.input_set_entry(file_path,"switch_output_integration","0") # interesting for debugging, usually turned on, but turn off to reduce workload
                        self.input_set_entry(file_path,"switch_output_maxevent","0")    # usually turned off anyways
                        self.input_set_entry(file_path,"switch_output_comparison","0")  # usually turned off anyways
                        self.input_set_entry(file_path,"switch_output_gnuplot","0")     # usually turned off anyways
                        self.input_set_entry(file_path,"switch_output_proceeding","0")  # usually turned off anyways
                        self.input_set_entry(file_path,"switch_output_weights","0")     # usually turned off anyways
                    else:
                        out.print_error("Value \"%s\" for parameter \"reduce_workload\" not implemented. Remove switch or choose different value (0,1 or 2). Stopping the code..." % value)
# not a general solution which allows to run NNLO + NLO gg at the same time!
# >>>>
                elif parameter == "loop_induced":
                    if int(value) < 0:
                        self.input_change_entry(file_path,"min_qTcut","0.8")
                        self.input_change_entry(file_path,"n_qTcut","81")
                        self.input_change_entry(file_path,"step_qTcut","0.04")
                elif parameter == "switch_qT_accuracy":
                    if int(parameter_list.get("loop_induced")) < 0:
                        self.input_change_entry(file_path,"min_qTcut","0.8")
                        self.input_change_entry(file_path,"n_qTcut","81")
                        self.input_change_entry(file_path,"step_qTcut","0.04")
# <<<<
# not a general solution which allows to run NNLO + NLO gg at the same time!
                    elif value == "-99": # default: lower accuracy, faster convergence
                        self.input_change_entry(file_path,"min_qTcut","0.15")
                        self.input_change_entry(file_path,"n_qTcut","86")
                        self.input_change_entry(file_path,"step_qTcut","0.01")
                    elif value == "-3": # default: lower accuracy, faster convergence
                        continue # don't do anything in that mode
                    elif value == "-2": # default: lower accuracy, faster convergence
                        self.input_change_entry(file_path,"min_qTcut","0.15")
                        self.input_change_entry(file_path,"n_qTcut","35")
                        self.input_change_entry(file_path,"step_qTcut","0.025")
                    elif value == "-1": # default: lower accuracy, faster convergence
                        self.input_change_entry(file_path,"min_qTcut","0.01")
                        self.input_change_entry(file_path,"n_qTcut","100")
                        self.input_change_entry(file_path,"step_qTcut","0.01")
                    elif value == "0": # default: lower accuracy, faster convergence
                        self.input_change_entry(file_path,"min_qTcut","0.15")
                        self.input_change_entry(file_path,"n_qTcut","86")
                        self.input_change_entry(file_path,"step_qTcut","0.01")
                    elif value == "1": # increased accuracy, slower convergence
                        self.input_change_entry(file_path,"min_qTcut","0.05")
                        self.input_change_entry(file_path,"n_qTcut","96")
                        self.input_change_entry(file_path,"step_qTcut","0.01")
                    elif value == "1337" or value == "-1": # don't use that if you need proper distributions !!!
                        self.input_change_entry(file_path,"min_qTcut","0.01")
                        self.input_change_entry(file_path,"n_qTcut","100")
                        self.input_change_entry(file_path,"step_qTcut","0.01")
                    # elif value == "2": # increased accuracy, slower convergence
                    #     self.input_change_entry(file_path,"min_qTcut","0.01")
                    #     self.input_change_entry(file_path,"step_qTcut","0.01")
                    #     self.input_change_entry(file_path,"n_qTcut","100")
                    else:
                        out.print_error("Value \"%s\" for parameter \"switch_qT_accuracy\" not implemented. Remove switch or choose different value (0 or 1). Stopping the code..." % value)
                # REMOVE BELOW ??? not needed as settings are made with higher file parameter
                # elif parameter == "add_NLL" or parameter == "add_NNLL":
                #     if parameter_list["add_NNLL"] == "1" or parameter_list["add_NNLL"] == "1":
                #         self.input_change_entry(file_path,"do_resummation","1")
                #     else:
                #         self.input_change_entry(file_path,"do_resummation","0")
                continue
            elif parameter.startswith("user_"): # use this to include all possibly newly user-defined parameters
                # user defined parameters are process dependent and case dependent
                if prc.process_name in ["ppeeexex04"] and parameter in ["user_switch M_leplep","user_cut min_M_leplep","user_cut max_M_leplep","user_cut min_M_leplep_IR"]: # very special case to choose either ATLAS or CMS pairing
                    if not parameter == "user_switch M_leplep":
                        self.input_set_entry(file_path,parameter,value)                        
                    elif value == "0":
                        self.input_set_entry(file_path,parameter,value)                        
                    elif value == "1": # this case set ATLAS paring
                        self.input_set_entry(file_path,parameter,value)                        
                        self.input_set_entry(file_path,"user_switch M_leplep_ATLAS","1")
                        self.input_set_entry(file_path,"user_switch M_leplep_CMS","0")
                    elif value == "2": # this case set CMS paring
                        value = "1"
                        self.input_set_entry(file_path,parameter,value)                        
                        self.input_set_entry(file_path,"user_switch M_leplep_ATLAS","0")
                        self.input_set_entry(file_path,"user_switch M_leplep_CMS","1")
                else: # standard behavior
                    self.input_set_entry(file_path,parameter,value)
                    # not needed anymore since switches are now explicit in input file
#                     # and its switch
#                     if parameter in user_parameter_switches: # pre-defined hard-coded switch mappings
#                         switch = user_parameter_switches[parameter]
#                     else: # default behaviour
#                         if parameter.split()[1].startswith("max_"):
#                             switch = "user_switch "+parameter.split()[1][parameter.split()[1].startswith("max_") and len("max_"):]
#                         elif parameter.split()[1].startswith("min_"):
#                             switch = "user_switch "+parameter.split()[1][parameter.split()[1].startswith("min_") and len("min_"):]
#                         else:
#                             switch = "user_switch "+parameter.split()[1]

#                     switch_value = "1"
# #                    print "switch   ",switch,switch_value
#                     self.input_set_entry(file_path,switch,switch_value)
                continue
            elif parameter.startswith("hidden "): # use this to include all possible settings one may wanna add to the file_paramter.dat
                # a hidden parameter is used like a unique parameter, but can be any not just the pre-defined parameters
                parameter = parameter[7:]
                self.input_set_entry(file_path,parameter,value)
                continue
            elif parameter.startswith("OL "): # allow OL parameters to be overwritten or added
                self.input_set_entry(file_path,parameter,value)
                continue
            elif parameter.startswith("RCL "): # allow RCL parameters to be overwritten or added
                self.input_set_entry(file_path,parameter,value)
                continue
            elif parameter.startswith("switch_OL"): # allow OL switch parameter to be overwritten or added
                self.input_set_entry(file_path,parameter,value)
                continue
            elif parameter.startswith("switch_yuk"): # allow RCL switch parameter to be overwritten or added
                self.input_set_entry(file_path,parameter,value)
                continue
            elif parameter.startswith("order_y"): # allow RCL switch parameter to be overwritten or added
                self.input_set_entry(file_path,parameter,value)
                continue
         #   elif  parameter.startswith("N_quarks"):
         #       self.input_set_entry(file_path,parameter,value)
         #       continue
            else:
                out.print_error("Parameter \"%s\" in parameter.dat is not listed as proper input parameter." % parameter)
#}}}
#{{{ def: input_user_cuts_default_and_consistency(self,file_path,parameter_list)
# 2do: do we need this again, to check user inputs, or can we use the normal check_parameter_consistencies routine ?

#     def input_user_cuts_default_and_consistency(self,file_path,parameter_list): # incomplete (not crucial)
#         # this function sets the default parameters (all switches off) for the user cuts
#         # and checks wether there are inconsistencies user inputs
#         out.print_info("Checking parameter input in parameter.dat file...")
#         # first check if all mandatory parameter are set
#         # for parameter in mandatory_parameters:
#         #     if not parameter in parameter_list:
#         #         out.print_error("Parameter \"%s\" is mandatory, but not defined in parameter.dat file." % parameter)
#         # turn of switches for parameters not defined in parameter.dat
#         for parameter in user_parameters[prc.process_name]:
#             if not parameter in parameter_list:
#                 switch_need_for_other_parameter = False
#                 if parameter in user_parameter_switches: # pre-defined hard-coded mappings
#                     # for other_parameter in user_parameters[prc.process_name]:
#                     #     if other_parameter in parameter_list and user_parameter_switches[other_parameter] == user_parameter_switches[parameter]:
#                     #         switch_need_for_other_parameter = True
#                     switch = user_parameter_switches[parameter]
#                 else: # default behaviour
#                     if parameter.split()[1].startswith("max_"):
#                         switch = "user_switch "+parameter.split()[1][parameter.split()[1].startswith("max_") and len("max_"):]
#                     elif parameter.split()[1].startswith("min_"):
#                         switch = "user_switch "+parameter.split()[1][parameter.split()[1].startswith("min_") and len("min_"):]
#                     else:
#                         switch = "user_switch "+parameter.split()[1]
#                     # for other_parameter in user_parameters[prc.process_name]:
#                     #     if other_parameter in parameter_list and not other_parameter in user_parameter_switches:
#                     #         if parameter.split()[1].startswith("max_"):
#                     #             other_switch = "user_switch "+parameter.split()[1][parameter.split()[1].startswith("max_") and len("max_"):]
#                     #         elif parameter.split()[1].startswith("min_"):
#                     #             other_switch = "user_switch "+parameter.split()[1][parameter.split()[1].startswith("min_") and len("min_"):]
#                     #         else:
#                     #             other_switch = "user_switch "+parameter.split()[1]
#                     #         if other_switch == switch:
#                     #             switch_need_for_other_parameter = True
#                 switch_value = "0"
# #                print "switch   ",switch,switch_value,switch_need_for_other_parameter
# #                if not switch_need_for_other_parameter: # not needed because inputs are set and overwritten afterwards anyway
#                 self.input_set_entry(file_path,switch,switch_value)
#}}}
#{{{ def: input_check_parameter_consistencies_from_list(self,file_path,parameter_list)
    def input_check_parameter_consistencies_from_list(self,file_path,parameter_list): # incomplete (not crucial)
        # this function checks wether there are inconsistencies in the input of the parameter.dat file
        # this will be a long file with if clauses; it will also set defaults to parameters that are not mandatory
        # first check if all mandatory parameter are set
        for parameter in mandatory_parameters:
            if not parameter in parameter_list:
                out.print_error("Parameter \"%s\" is mandatory, but not set in parameter.dat file." % parameter)
        # handle all pre-defined user defined parameters as mandatory; otherwise unexpected behavior could occur when commenting 
        # a parameter, which is actually set in the default file_parameter.dat
        for parameter in user_parameters[prc.process_name]: # these are the !pre-defined! user parameters
            if not parameter in parameter_list:
                out.print_error("Parameter \"%s\" is a mandatory user parameter, but not set in parameter.dat file." % parameter)
        # then set default values for all parameters that are not given
        for parameter in default_parameters:
            if not parameter in parameter_list:
                parameter_list[parameter] = default_parameters[parameter]
                if not parameter == "check_interval" and not parameter == "print_out_interval" and (not parameter == "loop_induced" or prc.process_name in processes_with_loop_induced):
                    out.print_info("Parameter \"%s\" not set in parameter.dat file. Setting it to default value: %s." % (parameter,default_parameters[parameter]))
        # then check wether the given values meet the requirements
        # 2do

        # catch specific cases
        if parameter_list.get("frixione_isolation") in ["1","2"]: # frixione isolation turned on 
            frixione_list = ["frixione_n","frixione_delta_0"] # required parameters for frixione isolation
            for parameter in frixione_list:
                if not parameter in parameter_list: # catch if these parameters are not given
                    out.print_error("\"frixione_isolation\" requires input of \"%s\". Please specify in parameter.dat file and restart." % parameter)            
        if parameter_list.get("frixione_isolation") == "1": # frixione isolation ATLAS setup
            if not "frixione_epsilon" in parameter_list:
                out.print_error("\"frixione_isolation\" set to \"1\" (ATLAS setup) requires input of \"frixione_epsilon\". Please specify in parameter.dat file and restart.")
            if "frixione_fixed_ET_max" in parameter_list:
                out.print_error("\"frixione_isolation\" set to \"1\" (ATLAS setup) does not allow for input of \"frixione_fixed_ET_max\". Please remove (comment) from parameter.dat file and restart.")
        elif parameter_list.get("frixione_isolation") == "2": # frixione isolation CMS setup
            if not "frixione_fixed_ET_max" in parameter_list:
                out.print_error("\"frixione_isolation\" set to \"2\" (CMS setup) requires input of \"frixione_fixed_ET_max\". Please specify in parameter.dat file and restart.")
            if "frixione_epsilon" in parameter_list:
                out.print_error("\"frixione_isolation\" set to \"2\" (CMS setup) does not allow for input of \"frixione_epsilon\". Please remove (comment) from parameter.dat file and restart.")
        if float(parameter_list.get("max_time_per_job")) < 1: # max_time_per_job become unreliable below 1h
            out.print_warning("Parameter max_time_per_job (set to %s hours) chosen below 1 hour. This is fine to tune the degree of parallelization, but does not constitute a realistic maximal run time of the jobs." % parameter_list["max_time_per_job"])
        if not parameter_list.get("flavor_scheme","not-set") in ["1","2","not-set"]:
            out.print_error("Parameter flavor_scheme in parameter.dat (set to %s) chosen different from \"1\" (four-flavor scheme) and \"2\" (five-flavor scheme). Please choose either of these two values and restart..." % parameter_list["flavor_scheme"])
#        if prc.process_name in ["ppemxnmnex04"] and parameter_list.get("user_switch pT_W","-1") == "0": # technical cut on pT of W's needed at O(as^2) in WW to avoid instabilities.
#            out.print_warning("Technical lower cut on pT of W-bosons turned off (user_switch pT_W = %s). A cut of min_pT_W < 0.5 GeV may lead to instability issues of the matrix elements at O(as^2). Please stop the code and restart with min_pT_W cut if that is unwanted..." % parameter_list["user_switch pT_W"])
        if prc.process_name in ["ppeexexne04","ppeeexnex04"]: # technical cut on pT of W's needed at O(as^2) in WW to avoid instabilities.
            if parameter_list.get("user_switch lepton_identification") in ["1","2"]:
                pass
            elif not parameter_list.get("user_switch lepton_identification") == "0":
                for switch in ["user_switch lepW_cuts","user_switch M_Z","user_switch delta_M_Z","user_switch R_lepZlepZ","user_switch R_lepZlepW","user_switch MT_W"]:
                    if parameter_list.get(switch) == "1":
                        out.print_error("Applying cut \"%s\" (set to \"1\"), but no lepton identifaction chosen in parameter.dat (\"user_switch lepton_identification = %s\"). Please set \"user_switch lepton_isolation\" either to \"1\" (ATLAS prescription) and \"2\" (CMS prescription) and restart..." % (switch,parameter_list["user_switch lepton_identifiaction"]))
            else:
                out.print_error("Parameter \"user_switch lepton_identification\" in parameter.dat (set to \"%s\") chosen different from \"0\" (off), \"1\" (ATLAS prescription) and \"2\" (CMS prescription). Please choose either of these three values and restart..." % parameter_list["user_switch lepton_identifiaction"])
#        if prc.process_name in ["ppemxnmnex04"] and float(parameter_list.get("user_cut min_pT_W",1000)) < 0.5: # technical cut on pT of W's needed at O(as^2) in WW to avoid instabilities.
#            out.print_warning("Technical lower cut on pT of W-bosons (user_cut min_pT_W) set to %s GeV. A cut of min_pT_W < 0.5 GeV may lead to instability issues of the matrix elements at O(as^2). Please stop the code and restart with larger min_pT_W cut if that is unwanted..." % parameter_list["user_cut min_pT_W"])
# mandatory: technical cut on the pT of the W's to avoid stability problems                                                                                                                                                                   # WARNING: do not turn off lower cut or chose value below min_pT_W = 0.5 (GeV)                                                                                                                                                                user_switch pT_W = 1           #  switch to turn on cut on transverse momentum of the W-bosons (0) off;                                                                                                                                                                      #  (1) cut on all contributions;                                                                                                                                                                                                              #  (2) cut only on O(as^2) (non-loop induced) contributions (default)                                                                                                                                          user_cut min_pT_W = 1          #  requirement on pT of the W-bosons (lower cut)                                                                                                                                                               user_cut max_pT_W = 1.e99      #  requirement on pT of the W-bosons (upper cut)  
        if parameter_list.get("add_NLL") == "1":
            if not parameter_list.get("NLO_subtraction_method") == "2":
                out.print_warning("Resummation at NLL turned on (add_NLL=1), but not QT subtraction scheme chosen at NLO (NLO_subtraction_method!=2). Setting NLO_subtraction_method=2 (QT subtraction) and continuing...")
                parameter_list["NLO_subtraction_method"] = "2"
            if not parameter_list.get("run_NLO_QCD") == "1":
                out.print_error("Resummation at NLL turned on (add_NLL=1), must be matched to NLO cross section, but NLO turned off (run_NLO_QCD=0). Adjust parameter.dat file accordingly, ie, set run_NLO_QCD=1. Exiting...") # cannot run NLL resummation without NLO fixed order part
            if not parameter_list.get("scale_variation") == "0":
                out.print_warning("Resummation at NLL turned on (add_NLL=1), cannot do automatic scale variation (scale_variation=%s). Setting scale_variation=0 (no scale variation) and continuing..." % parameter_list.get("scale_variation") )
                parameter_list["scale_variation"] = "0"
            if not parameter_list.get("dynamic_scale") == "0":
                out.print_warning("Resummation at NLL turned on (add_NLL=1), cannot use dynamical scales for muR and muF (dynamic_scale!=0). Setting dynamic_scale=1 (fixed scale as specified in parameter.dat: scale_ren=%s and scale_fact=%s) and continuing..." % (parameter_list.get("scale_ren"),parameter_list.get("scale_ren")) )
                parameter_list["dynamic_scale"] = "0"

        # REMOVE BELOW ??? running FO and resummed in parallel for different runs should be possible by using higher file parameter!
            # if parameter_list.get("run_NNLO_QCD") == "1" and not parameter_list.get("run_NNLL") == "1":
            #     out.print_error("Resummation at NLO+NLL turned on (run_NLO_QCD=1, add_NLL=1), cannot run only fixed order at NNLOmust be matched to NLO cross section, but NLO turned off (run_NLO_QCD=0). Adjust parameter.dat file accordingly, ie, set run_NLO_QCD=1. Exiting...") # cannot run NLL resummation without NLO fixed order part
            # if parameter_list.get("run_LO") == "1":
            #     out.print_error("Resummation at NLO+NLL turned on (run_NLO_QCD=1, add_NLL=1), fixed order run (run_LO=1) not possible in that case. Adjust parameter.dat file accordingly, eg, set run_LO=0. Exiting...") # cannot run resummation and fixed order at the same time
        if parameter_list.get("add_NNLL") == "1":
            if not parameter_list.get("NLO_subtraction_method") == "2":
                out.print_warning("Resummation at NNLL turned on (add_NNLL=1), but not QT subtraction scheme chosen at NLO (NLO_subtraction_method!=2). Setting NLO_subtraction_method=2 (QT subtraction) and continuing...")
                parameter_list["NLO_subtraction_method"] = "2"
            if parameter_list.get("loop_induced") != "0":
                out.print_warning("Resummation at NNLL turned on (add_NNLL=1). Cannot include loop-induced contribution for resummation. Setting loop_induced=0 and continuing...")
                parameter_list["loop_induced"] = "0"
            if not parameter_list.get("run_NNLO_QCD") == "1":
                out.print_error("Resummation at NNLL turned on (add_NNLL=1), must be matched to NNLO cross section, but NNLO turned off (run_NNLO_QCD=0). Adjust parameter.dat file accordingly, ie, set run_NNLO_QCD=1. Exiting...") # cannot run NLL resummation without NLO fixed order part
            if not parameter_list.get("scale_variation") == "0":
                out.print_warning("Resummation at NNLL turned on (add_NNLL=1), cannot do automatic scale variation (scale_variation=%s). Setting scale_variation=0 (no scale variation) and continuing..." % parameter_list.get("scale_variation") )
                parameter_list["scale_variation"] = "0"
            if not parameter_list.get("dynamic_scale") == "0":
                out.print_warning("Resummation at NNLL turned on (add_NNLL=1), cannot use dynamical scales for muR and muF (dynamic_scale!=0). Setting dynamic_scale=1 (fixed scale as specified in parameter.dat: scale_ren=%s and scale_fact=%s) and continuing..." % (parameter_list.get("scale_ren"),parameter_list.get("scale_ren")) )
                parameter_list["dynamic_scale"] = "0"
        # REMOVE BELOW ??? running FO and resummed in parallel for different runs should be possible by using higher file parameter!
            # if parameter_list.get("run_NLO_QCD") == "1" and not parameter_list.get("run_NLL") == "1":
            # if parameter_list.get("run_LO") == "1":
            #     out.print_error("Resummation at NNLO+NNLL turned on (run_NNLO_QCD=1, add_NNLL=1), fixed order run (run_LO=1) not possible in that case. Adjust parameter.dat file accordingly, eg, set run_LO=0. Exiting...") # cannot run resummation and fixed order at the same time
        if parameter_list.get("run_NLO_EW") == "1" or parameter_list.get("add_NLO_EW") == "1":
            if parameter_list.get("frixione_isolation") in ["1","2"] and parameter_list.get("photon_recombination") == "1": # frixione isolation turned on 
                out.print_error("EW run turned on (run_NLO_EW=1 or add_NLO_EW=1); cannot do photon recombination and frixione frixione isolation at the same time. If you want to do an EW run please turn off frixione isolation. Exiting...")
        if parameter_list.get("add_NLO_EW") == "1" and not parameter_list.get("run_NNLO_QCD") == "1":
            out.print_error("If you want to add_NLO_EW at NNLO you also have to turn on run_NNLO_QCD. Exiting...")

        if parameter_list.get("run_NNLO_QCD") == "1" and int(parameter_list.get("loop_induced")) > 1:
            out.print_error("If you want to compute NNLO QCD and NLO loop-induced gg contribution together, you must used Catani-Seymour as NLO subtraction method. Either switch to Catani-Seymour (NLO_subtraction_method=1) or disable NLO gg contribution (loop_induced<2), and restart the code. Exiting...")
#}}}
#{{{ def: input_read_SLHA(self,file_path,SLHA_list)
    def input_read_SLHA(self,file_path,SLHA_list):
# function to read a file in the SLHA format, returning a dictionary with:
# BLOCK[number]: [value, commet]
        if not os.path.isfile(file_path):
            out.print_error("File "+file_path+" in function input_read_SLHA does not exist!")
        with open(file_path, 'r') as model_file:
            for in_line in model_file:
                line=in_line.strip() # strip removes all spaces (including tabs and newlines)
                # if any line starts with %, # or is an emtpy line (disregarding spaces) it is a comment line and should be skipped
                if line=="" or line[0]=="%" or line[0]=="#": 
                    continue
                # split line by spaces and remove otherspaces again from string
                # [0] gives you the number (before the first space)
                file_number=line.split()[0].strip() # 2do multiple spaces
                file_value=line.split()[1].strip()
                try:
                    file_comment=line.split('#')[1].strip()
                except: 
                    file_comment=""
                    pass
                if file_number.upper() == "DECAY":
                    Block=file_number
                    file_number=line.split()[1].strip()
                    file_value=line.split()[2].strip()
                    SLHA_list[Block][int(file_number)]=file_value
                    SLHA_list[Block+" comment"][int(file_number)]=file_comment
                    continue
 
                if file_number.upper() == "BLOCK":
                    Block=file_value
                elif file_number.upper() == "EW": 
                    ew_par = line.split()[1].strip() 
                    file_value=line.split()[2].strip()
                    SLHA_list["EW"][ew_par]= file_value #electroweak scheme
                else:
                    SLHA_list[Block][int(file_number)]=file_value
                    SLHA_list[Block+" comment"][int(file_number)]=file_comment
#}}}
#{{{ def: input_set_file_model_from_SLHA(self,file_path,SLHA_list)
    def input_set_file_model_from_SLHA(self,file_path,SLHA_list):
# function to set all model paramaters in MUNICH input ("file_model.dat") from SLHA_list 
# read from the MATRIX input file
        # do some concistency checks for Processes with CKM Matrix
        if ("CKM" in SLHA_list or "VCKMIN" in SLHA_list) and not prc.process_name in ckm_processes:
            out.print_error("CKM matrix not supported for process %s. Remove CKM and VCKMIN Blocks from model.dat file and restart. Exiting..." % prc.process_name)
        elif "CKM" in SLHA_list and "VCKMIN" in SLHA_list:
            out.print_error("Both blocks CKM (full CKM) and VCKMIN (Cabibbo) exist in model.dat file. Remove one of the two and restart. Exiting...")
        elif "CKM" in SLHA_list:
            self.input_set_entry(file_path,"CKM_matrix","individual")
        elif "VCKMIN" in SLHA_list:
            self.input_set_entry(file_path,"CKM_matrix","Cabibbo")
        elif prc.process_name in ckm_processes:
            self.input_set_entry(file_path,"CKM_matrix","individual")
            self.input_set_entry(file_path,"V_du","1")
            self.input_set_entry(file_path,"V_su","0")
            self.input_set_entry(file_path,"V_bu","0")
            self.input_set_entry(file_path,"V_dc","0")
            self.input_set_entry(file_path,"V_sc","1")
            self.input_set_entry(file_path,"V_bc","0")
            self.input_set_entry(file_path,"V_dt","0")
            self.input_set_entry(file_path,"V_st","0")
            self.input_set_entry(file_path,"V_bt","1")
        else:
            self.input_set_entry(file_path,"CKM_matrix","trivial")
        for Block in SLHA_list:
            try:
                split=Block.split()[1].strip()
                if not split=="comment":
                    out.print_error("Block in input_set_file_model_from_SLHA contains spaces.")
            except:
                for number in SLHA_list[Block]:              
                    parameter = model_mappings_to_MUNICH[Block][number]
                    value = SLHA_list[Block][number]
                    # add some sepecial case consistency checks
                    if parameter_list.get("flavor_scheme") == "1" and parameter == "M_b" and float(value) == 0.: # 4FS
                        out.print_warning("Bottom mass (block %s entry %s) in model.dat set to M_b=0, but four-flavor scheme (flavor_scheme=1) with massive bottom quarks chosen. Using default value for bottom mass (M_b=4.75 GeV)." %(Block,number))
                        value = 4.75
                    elif parameter_list.get("flavor_scheme") == "2" and parameter == "M_b" and float(value) != 0.: # 5FS
                        out.print_warning("Bottom mass (block %s entry %s) in model.dat set to M_b=%s GeV, but five-flavor scheme (flavor_scheme=1) with massless bottom quarks chosen. Using M_b=0 for bottom mass." %(Block,number,str(float(value))))
                        value = 0.
                    if Block != "EW":
                       self.input_set_entry(file_path,parameter,value)
            if Block == "EW": #electroweak scheme
                for i in SLHA_list[Block]:      
                    parameter = i
                    value =  SLHA_list[Block][i]
                    self.input_set_entry(file_path,parameter,value)
#}}}
#{{{ def: add_directories_to_result_files(self,file_path)
    def add_directories_to_result_files(self,file_path,runtime_table,phase):
# function to set add the directories that have to be taken into account
        keyword_found = False
        with open(file_path+".replace",'w') as new_file:
            with open(file_path, 'r') as in_file:
                for in_line in in_file:
                    line=in_line.strip() # strip removes all spaces (including tabs and newlines)
                    # if any line starts with %, # or is an emtpy line (disregarding spaces) it is a comment line and should be skipped
                    # or once the parameter was set, stop searching for more occurences and simply write out the rest of the file
                    if line=="" or line[0]=="%" or line[0]=="#": 
                        new_file.write(in_line)
                        continue
                    # split line by "=" and remove spaces again from string
                    # [0] gives you the parameter (before "="-sign)
                    file_parameter=line.split('=')[0].strip()
                    # [1] gives you the value (after "="-sign); remove trailing % or # and everything after in this case
                    file_value=line.split('=')[1].split("%")[0].split("#")[0].strip()
                    # check parameter names until you find the one of the key with the right value
                    if file_parameter == "directory" and not keyword_found:
                        continue
                    # when keyword found search below the parameter you want to set
                    if (keyword_found and file_parameter == "directory") or (file_parameter == "type_contribution" and keyword_found):
                        # remove from right to "/" (remove "/run.0")
                        # alternative to determine directory directly:
                        # get directory from the value in the file
                        # directory = file_value.rsplit('/', 1)[0]
                        # print directory
                        # exit(0)
#                        directory = pjoin(file_path.rsplit('/',1)[1].rsplit('.',2)[0],file_path.rsplit('/',1)[1].rsplit('.',2)[1],type_contribution+"%s" % (".QCD" if not type_contribution=="born" and not type_contribution=="loop" else ""))
#                        path = pjoin(fold.run_folder_path,directory)
                        path = glob.glob(pjoin(fold.run_folder_path,file_path.rsplit('/',1)[1].rsplit('.',2)[0],file_path.rsplit('/',1)[1].rsplit('.',2)[1],type_contribution+"*"))[0]
                        directory = os.path.relpath(path,fold.run_folder_path)
                        run_dirs_in_path = [ f for f in os.listdir(path) if f.startswith("run.") and os.path.isdir(pjoin(path,f)) ]
                        run_dir_numbers = []
                        for run_dir in run_dirs_in_path:
                            try:
                                number = int(run_dir.split(".")[1])
                                run_dir_numbers.append(number)
                            except:
                                pass
                        max_parallel_runs = max(run_dir_numbers or [-1])+1 # result combination now uses always all folders which are there
                        # determine wether to include the extrapolation runs or not
                        # default is to include them
                        start_folder_index = 0
                        # in case the number of events of all extrapolation events is 100 times smaller than the events in a single main run folder exclude them
                        run_dir_0 = pjoin(path,"run.0")
                        pre_events   = run.get_pre_run_min_events_for_contribution(run_dir_0)
                        parameter_files = []
                        main_dirs = fold.get_dirs_with_identifier(path,"main")
                        for main_dir in main_dirs:
                            parameter_files += glob.iglob(pjoin(main_dir,"log","file_parameter*"))
                        event_sum_main = 0
                        for parameter_file in list(parameter_files):
                            parameter = {}
                            self.input_read_parameter_dat(parameter_file,parameter)
                            event_sum_main += float(parameter.get("n_events_min",0))
                        # since we computed the sum of events for this contribution here, save it to the log for the final summary
                        if event_sum_main > 0: # make sure to only include contributions where events were run (otherwise we will end up with logs of contributions that were not run)
                            log.summary_add_events(event_sum_main,path)
                        if event_sum_main/pre_events > 500:
                            start_folder_index = max_parallel_runs - len(main_dirs)
                        if start_folder_index >= max_parallel_runs:
                            start_folder_index = 0 # to avoid that the directory gets removed from the file
                            max_parallel_runs = 1 # to avoid that the directory gets removed from the file
                        # use pre-defined switch if set
                        if "include_pre_in_results" in parameter_list and len(main_dirs)>0:
                            if parameter_list["include_pre_in_results"] == "0": # take only main runs into account (not in extrapolation phase=-1)
                                start_folder_index = max_parallel_runs - len(main_dirs)
                            elif parameter_list["include_pre_in_results"] == "1": # take also all pre runs into account
                                start_folder_index = 0
                            else:
                                out.print_error("Parameter \"include_pre_in_results\" in parameter.dat can only have values \"0\" and \"1\", give value: %s." % parameter_list["include_pre_in_results"])
                        if phase == -1:
                            start_folder_index = 0
                            max_parallel_runs = run.get_parallel_pre_runs_for_contribution(run_dir_0)
                        for i in range(start_folder_index,max_parallel_runs): # add all folders of the parallel runs
                            new_file.write("directory"+" = "+pjoin(directory,"run.%s"%i)+" \n")
                        # once we know the directory add it as often as needed
                    if file_parameter == "type_contribution":
                        new_file.write(in_line)
                        type_contribution = file_value
                        keyword_found = True
                    elif file_parameter == "directory":
                        keyword_found = False
                    else:
                        new_file.write(in_line)
                    if file_parameter == "type_contribution" and not keyword_found:
                        keyword_found = True
                        type_contribution = file_value
                else:
                    if keyword_found:
                        # remove from right to "/" (remove "/run.0")
                        # alternative to determine directory directly:
                        directory = pjoin(file_path.rsplit('/',1)[1].rsplit('.',2)[0],file_path.rsplit('/',1)[1].rsplit('.',2)[1],type_contribution+"%s" % (".QCD" if not type_contribution=="born" and not type_contribution=="loop" else ""))
                        # get directory from the value in the file
                        # directory = file_value.rsplit('/', 1)[0]
                        # print directory
                        # exit(0)
                        path = pjoin(fold.run_folder_path,directory)
                        run_dirs_in_path = [ f for f in os.listdir(path) if f.startswith("run.") and os.path.isdir(pjoin(path,f)) ]
                        max_parallel_runs = len(run_dirs_in_path) # result combination now uses always all folders which are there
                        # determine wether to include the extrapolation runs or not
                        # default is to include them
                        start_folder_index = 0
                        # in case the number of events of all extrapolation events is 100 times smaller than the events in a single main run folder exclude them
                        run_dir_0 = pjoin(path,"run.0")
                        pre_events   = run.get_pre_run_min_events_for_contribution(run_dir_0)
                        parameter_files = []
                        main_dirs = fold.get_dirs_with_identifier(path,"main")
                        for main_dir in main_dirs:
                            parameter_files += glob.iglob(pjoin(main_dir,"log","file_parameter*"))
                        event_sum_main = 0
                        for parameter_file in list(parameter_files):
                            parameter = {}
                            self.input_read_parameter_dat(parameter_file,parameter)
                            event_sum_main += float(parameter.get("n_events_min",0))
                        # since we computed the sum of events for this contribution here, save it to the log for the final summary
                        if event_sum_main > 0: # make sure to only include contributions where events were run (otherwise we will end up with logs of contributions that were not run)
                            log.summary_add_events(event_sum_main,path)
                        if event_sum_main/pre_events > 500:
                            start_folder_index = int(sorted(main_dirs)[0].rsplit('.',1)[1])
                        if start_folder_index >= max_parallel_runs:
                            start_folder_index = 0 # to avoid that the directory gets removed from the file
                            max_parallel_runs = 1 # to avoid that the directory gets removed from the file
                        # use pre-defined switch if set
                        if "include_pre_in_results" in parameter_list:
                            if parameter_list["include_pre_in_results"] == 0: # take only main runs into account (not in extrapolation phase=-1)
                                start_folder_index = int(sorted(main_dirs)[0].rsplit('.',1)[1])
                            elif parameter_list["include_pre_in_results"] == 1: # take also all pre runs into account
                                start_folder_index = 0
                            else:
                                out.print_error("Parameter \"include_pre_in_results\" in parameter.dat can only have values \"0\" and \"1\", give value: %s." % parameter_list["include_pre_in_results"])
                        if phase == -1:
                            start_folder_index = 0
                            max_parallel_runs = run.get_parallel_pre_runs_for_contribution(run_dir_0)
                        for i in range(start_folder_index,max_parallel_runs): # add all folders of the parallel runs
                            new_file.write("directory"+" = "+pjoin(directory,"run.%s"%i)+" \n")
                        # once we know the directory add it as often as needed
        os.remove(file_path)
        shutil.move(file_path+".replace",file_path)
#}}}
#{{{ def: get_cross_sections_from_file(self,file_path)
    def get_cross_sections_from_file(self,file_path):
# read total rates (possibly within cuts) from MUNICH output
        if not os.path.isfile(file_path):
            out.print_error("file "+file_path+" in function get_cross_sections_from_file does not exist!")
        variation = []
        with open(file_path, 'r') as param_file:
            for in_line in param_file:
                line=in_line.strip() # strip removes all spaces (including tabs and newlines)
                # if any line starts with %, # or is an emtpy line (disregarding spaces) it is a comment line and should be skipped
                if line=="" or line[0]=="%" or line[0]=="#": 
                    continue
                muRmuF=float(line.split()[0].strip())
                cross_section=float(line.split()[1].strip())
                err=float(line.split()[2].strip())
                if muRmuF==1.:
                    central     = cross_section
                    central_err = err
                variation.append(cross_section)
        up   = max(variation)
        down = min(variation)
        return central, central_err, up, down
#}}}
#{{{
# # this function reads in the qT cut dependence of the cross section and provides an extrapolation to rcut=0
#     def get_qT_extrapolation_error_from_file(self,file_path,order):
#       def power_suppressed_cut_dep_nlo(rcut,c,d,e,f):
#         return f + rcut*rcut*(c*np.power(np.log(rcut),2)+d*np.power(np.log(rcut),1)+e)
#       def power_suppressed_cut_dep_nnlo(rcut,a,b,c,d,e,f):
#         return f + rcut*rcut*(a*np.power(np.log(rcut),4)+b*np.power(np.log(rcut),3)+c*np.power(np.log(rcut),2)+d*np.power(np.log(rcut),1)+e)

#       cut_dep = np.loadtxt(file_path)
#       n_scales=(len(cut_dep[0,:])-1)/2
#       central_vals = cut_dep[:,(n_scales-1)+1]
#       x = cut_dep[:,0]
#       y = central_vals
#       sigmas = cut_dep[:,(n_scales-1)+2]

#       try:
#         from scipy.optimize import curve_fit
#       except:
#         # FIXME: need to check for version of numpy
#         return 0,0

#         out.print_warning("scipy module not avalaible, using polynomial fit for qT extrapolation")
#         deg=4
# # note: polyfit assumes weights to be 1/sqrt(var). check!
# # TODO: maybe a simpler (linear?) fit is more reliable here..
# # TODO: the w parameter is only supported in newer versions of numpy
# #        popt, pcov = np.polyfit(x,y,deg,w=1.0/sigmas,cov=True)
#         popt, pcov = np.polyfit(x,y,deg,cov=True)
#         perr = np.sqrt(np.diag(pcov))
# #        print(popt)
# #        print(perr)
#         return popt[deg],perr[deg]
#       else:
#         if (order==2):
#           p0 = np.array([0,0,0,0,0,y[0]])
#           popt, pcov = curve_fit(power_suppressed_cut_dep_nnlo,x,y,None,sigmas)
# #          print(popt)
# #          print(pcov)
#           deg=4
#         else:
#           p0 = np.array([0,0,0,y[0]])
#           popt, pcov = curve_fit(power_suppressed_cut_dep_nlo,x,y,None,sigmas)
#           deg=2

#         if not np.all(np.isfinite(pcov)):
#           return 0,0

#         perr = np.sqrt(np.diag(pcov))
# #      print(popt)
# #      print(perr)
#         fitted_curve = popt[deg+1]*np.ones(len(central_vals))
#         for n in range(deg+1):
#           fitted_curve += popt[n]*np.square(x)*np.power(np.log(x),deg-n)

# #      print(fitted_curve)
#         return popt[deg+1],perr[deg+1]
#}}}
#{{{ def: input_read_distribution_dat(self,file_path,content)
    def input_read_distribution_dat(self,file_path):
# this function reads in the whole file and returns it
        with open(file_path, "r") as f:
            content = f.read()
        return content
#}}}
#{{{ def: input_set_file_distribution_dat(self,file_path,content)
    def input_set_file_distribution_dat(self,file_path,content):
# this function appends content to the file file_path
        with open(file_path, "a") as f:
            f.write(content)
#}}}
#{{{ def: write_infile_result(self,order_in,file_path,coupling_order_LO,coupling_order_NLO,coupling_order_NNLO,loop_induced)
    def write_infile_result(self,order_in,file_path,NLO_subtraction,coupling_order_LO,coupling_order_NLO,coupling_order_NNLO,coupling_order_NNNLO,loop_induced):

# function to write the infile for the result/distribution collection
        with open(file_path, 'w') as new_file:
            # set combination mode hybrid with average_factor=1
            new_file.write("average_factor = 1  # number of samples for the hybrid combination\n")
            new_file.write("deviation_tolerance_factor = 50  # tollerance how much bigger the uncertainty of a part in the combination can be\n")
            new_file.write("\n")
            # determine minimum value for extrapolation range and error
            min_max_value = 0.25
            if int(parameter_list.get("loop_induced")) < 0: 
                min_qTcut = 0.8
                min_max_value = 2
            elif int(parameter_list.get("switch_qT_accuracy",-99)) == 0: 
                min_qTcut = 0.15
            elif int(parameter_list.get("switch_qT_accuracy",-99)) == 1: 
                min_qTcut = 0.05
                min_max_value = 0.15
            elif int(parameter_list.get("switch_qT_accuracy",-99)) == 1337: # don't use that if you need proper distributions !!!
                min_qTcut = 0.15
            elif int(parameter_list.get("switch_qT_accuracy",-99)) == -1: # don't use that if you need proper distributions !!!
                min_qTcut = 0.01
            elif int(parameter_list.get("switch_qT_accuracy",-99)) == -2:
                min_qTcut = 0.15
            elif int(parameter_list.get("switch_qT_accuracy",-99)) == -3:
                min_qTcut = 0.15
            elif prc.process_name == "ppttx20":
                min_qTcut = 0.05
                min_max_value = 0.25
            else:
                min_qTcut = 0.15
            new_file.write("min_qTcut_extrapolation = %s  # lowest value used for extrapolation (if lowest produced value is higher, that one is used)\n" % min_qTcut)
            new_file.write("max_qTcut_extrapolation = 1     # maximal value used for extrapolation (if highest produced value is lower, that one is used)\n")
            new_file.write("min_value_extrapolation_range = 0.1  # minimal interval used for extrapolation\n")
            new_file.write("error_extrapolation_range_chi2 = 4.  # biggest chi deviation\n")
            if "a" in prc.process_name or prc.process_name == "ppeex02" or prc.process_name == "ppttx20": # use smaller range for photon processes and ttx
                new_file.write("min_max_value_extrapolation_range = %s  # minimal value of the upper bound of the interval used for extrapolation\n" % min_max_value)
                new_file.write("min_n_qTcut_extrapolation_range = 10  # minimal number of values used for extrapolation\n")
            else:
                new_file.write("min_max_value_extrapolation_range = 0.5  # minimal value of the upper bound of the interval used for extrapolation\n")
                new_file.write("min_n_qTcut_extrapolation_range = 30  # minimal number of values used for extrapolation\n")
            new_file.write("\n")
            new_file.write("switch_output_plot = 4  # output option for plot.* results\n")
            new_file.write("switch_output_result = 0  # output option for result.* results\n")
            new_file.write("switch_output_overview = 3  # output option for overview.* results\n")
            new_file.write("\n")
            new_file.write("average_factor = 1  # number of samples for the hybrid combination\n")
            new_file.write("deviation_tolerance_factor = 50  # tollerance how much bigger the uncertainty of a part in the combination can be\n")
            new_file.write("\n")
            # determine minimum value for extrapolation range and error
            min_max_value = 0.25
            if int(parameter_list.get("loop_induced")) < 0: 
                min_qTcut = 0.8
                min_max_value = 2
            elif int(parameter_list.get("switch_qT_accuracy",-99)) == 0: 
                min_qTcut = 0.15
            elif int(parameter_list.get("switch_qT_accuracy",-99)) == 1: 
                min_qTcut = 0.05
                min_max_value = 0.15
            elif int(parameter_list.get("switch_qT_accuracy",-99)) == 1337: # don't use that if you need proper distributions !!!
                min_qTcut = 0.15
            elif int(parameter_list.get("switch_qT_accuracy",-99)) == -1: # don't use that if you need proper distributions !!!
                min_qTcut = 0.01
            elif int(parameter_list.get("switch_qT_accuracy",-99)) == -2:
                min_qTcut = 0.15
            elif int(parameter_list.get("switch_qT_accuracy",-99)) == -3:
                min_qTcut = 0.15
            elif prc.process_name == "ppttx20":
                min_qTcut = 0.05
                min_max_value = 0.25
            else:
                min_qTcut = 0.15
            new_file.write("min_qTcut_extrapolation = %s  # lowest value used for extrapolation (if lowest produced value is higher, that one is used)\n" % min_qTcut)
            new_file.write("max_qTcut_extrapolation = 1     # maximal value used for extrapolation (if highest produced value is lower, that one is used)\n")
            new_file.write("min_value_extrapolation_range = 0.1  # minimal interval used for extrapolation\n")
            new_file.write("error_extrapolation_range_chi2 = 4.  # biggest chi deviation\n")
            if "a" in prc.process_name or prc.process_name == "ppeex02" or prc.process_name == "ppttx20": # use smaller range for photon processes and ttx
                new_file.write("min_max_value_extrapolation_range = %s  # minimal value of the upper bound of the interval used for extrapolation\n" % min_max_value)
                new_file.write("min_n_qTcut_extrapolation_range = 10  # minimal number of values used for extrapolation\n")
            else:
                new_file.write("min_max_value_extrapolation_range = 0.5  # minimal value of the upper bound of the interval used for extrapolation\n")
                new_file.write("min_n_qTcut_extrapolation_range = 30  # minimal number of values used for extrapolation\n")
            new_file.write("\n")
            new_file.write("switch_output_plot = 4  # output option for plot.* results\n")
            new_file.write("switch_output_result = 0  # output option for result.* results\n")
            new_file.write("switch_output_overview = 3  # output option for overview.* results\n")
            new_file.write("\n")
            if not "LO" in order_in and not "NLO" in order_in and not "NNLO" in order_in:
                out.print_error("Unknown order=%s specified in write_infile_result. Exiting..." % order_in)
            if "LO" in order_in:
                new_file.write("final_resultdirectory = result..NNLO.QT-CS.NLO."+NLO_subtraction+".LO\n")
                new_file.write("\n")
                new_file.write("resultdirectory = LO\n")
                new_file.write("contribution_file = infile.result/LO."+coupling_order_LO+".dat\n")
                new_file.write("\n")
            if "NLO" in order_in:
                new_file.write("resultdirectory = NLO.QCD\n")
                new_file.write("contribution_file = infile.result/NLO."+coupling_order_LO+".dat\n")
                new_file.write("contribution_file = infile.result/NLO."+NLO_subtraction+"."+coupling_order_NLO+".dat\n")
                new_file.write("\n")
            if "NNLO" in order_in:
                new_file.write("resultdirectory = NNLO.QCD\n")
                new_file.write("contribution_file = infile.result/NNLO."+coupling_order_LO+".dat\n")
                new_file.write("contribution_file = infile.result/NNLO."+NLO_subtraction+"."+coupling_order_NLO+".dat\n")
                if abs(int(parameter_list["loop_induced"])) > 0:
                    new_file.write("contribution_file = infile.result/NNLO."+coupling_order_NNLO+".dat\n")
                new_file.write("contribution_file = infile.result/NNLO.QT-CS."+coupling_order_NNLO+".dat\n")
                if abs(int(parameter_list["loop_induced"])) > 1:
                    new_file.write("\n")
                    new_file.write("resultdirectory = NNNLO.QCD\n")
                    new_file.write("contribution_file = infile.result/NNLO."+coupling_order_NNLO+".dat\n")
                    new_file.write("contribution_file = infile.result/NNNLO.QT."+coupling_order_NNNLO+".dat\n")
                new_file.write("\n")
#}}}
#{{{ def: write_infile_result_MATRIX(self,file_path,order_in,paramter_list,NLO_subtraction,coupling_order_LO,coupling_order_NLO,coupling_order_NLO_EW,coupling_order_NNLO,loop_induced)
    def write_infile_result_MATRIX(self,file_path,order_in,paramter_list,NLO_subtraction,coupling_order_LO,coupling_order_NLO,coupling_order_NLO_EW,coupling_order_NNLO,coupling_order_NNNLO,loop_induced):
# function to write the infile for the result/distribution collection
        with open(file_path, 'w') as new_file:
            # set combination mode hybrid with average_factor=4
            new_file.write("average_factor = 1  # number of samples for the hybrid combination\n")
            new_file.write("deviation_tolerance_factor = 50  # tollerance how much bigger the uncertainty of a part in the combination can be\n")
            new_file.write("\n")
            # determine minimum value for extrapolation range and error
            min_max_value = 0.25
            max_value = 1
            if int(parameter_list.get("loop_induced")) < 0: 
                min_qTcut = 0.8
                min_max_value = 2
                max_value = 4
            elif int(parameter_list.get("switch_qT_accuracy",-99)) == 0: 
                min_qTcut = 0.15
            elif int(parameter_list.get("switch_qT_accuracy",-99)) == 1: 
                min_qTcut = 0.05
                min_max_value = 0.15
            elif int(parameter_list.get("switch_qT_accuracy",-99)) == 1337: # don't use that if you need proper distributions !!!
                min_qTcut = 0.15
            elif int(parameter_list.get("switch_qT_accuracy",-99)) == -1: # don't use that if you need proper distributions !!!
                min_qTcut = 0.01
            elif int(parameter_list.get("switch_qT_accuracy",-99)) == -2:
                min_qTcut = 0.15
            elif int(parameter_list.get("switch_qT_accuracy",-99)) == -3:
                min_qTcut = 0.15
            elif prc.process_name == "ppttx20":
                min_qTcut = 0.05
                min_max_value = 0.25
            else:
                min_qTcut = 0.15
            new_file.write("min_qTcut_extrapolation = %s  # lowest value used for extrapolation (if lowest produced value is higher, that one is used)\n" % min_qTcut)
            new_file.write("max_qTcut_extrapolation = %s  # maximal value used for extrapolation (if highest produced value is lower, that one is used)\n" % max_value)
            new_file.write("min_value_extrapolation_range = 0.1  # minimal interval used for extrapolation\n")
            new_file.write("error_extrapolation_range_chi2 = 4.  # biggest chi deviation\n")
            if "a" in prc.process_name or prc.process_name == "ppeex02" or prc.process_name == "ppttx20": # use smaller range for photon processes and ttx
                new_file.write("min_max_value_extrapolation_range = %s  # minimal value of the upper bound of the interval used for extrapolation\n" % min_max_value)
                new_file.write("min_n_qTcut_extrapolation_range = 10  # minimal number of values used for extrapolation\n")
            else:
                new_file.write("min_max_value_extrapolation_range = 0.5  # minimal value of the upper bound of the interval used for extrapolation\n")
                new_file.write("min_n_qTcut_extrapolation_range = 30  # minimal number of values used for extrapolation\n")
            new_file.write("\n")
            new_file.write("switch_output_plot = 4  # output option for plot.* results\n")
            new_file.write("switch_output_result = 0  # output option for result.* results\n")
            new_file.write("switch_output_overview = 3  # output option for overview.* results\n")
            new_file.write("\n")
            if not "LO" in order_in and not "NLO" in order_in and not "NNLO" in order_in:
                out.print_error("Unknown order=%s specified in write_infile_result. Exiting..." % order_in)
            result_folder = "result.."+os.path.basename(file_path).lstrip("infile.").replace(".dat","")
            new_file.write("final_resultdirectory = %s\n" % result_folder)
            new_file.write("\n")
            if "LO" in order_in:
                new_file.write("resultdirectory = LO\n")
                new_file.write("contribution_file = infile.result/LO."+coupling_order_LO+".dat\n")
                if parameter_list.get("photon_induced","0") == "1":
                    new_file.write("\n")
                    new_file.write("resultdirectory = aLO\n")
                    new_file.write("contribution_file = infile.result/LO.a"+coupling_order_LO+".dat\n")
                    new_file.write("\n")
                    new_file.write("resultdirectory = LO+aLO\n")
                    new_file.write("contribution_file = infile.result/LO."+coupling_order_LO+".dat\n")
                    new_file.write("contribution_file = infile.result/LO.a"+coupling_order_LO+".dat\n")
            if "NLO" in order_in:
                if parameter_list.get("run_NLO_QCD","0") == "1":
                    new_file.write("\n")
                    new_file.write("resultdirectory = NLO.QCD\n")
                    new_file.write("contribution_file = infile.result/NLO."+coupling_order_LO+".dat\n")
                    new_file.write("contribution_file = infile.result/NLO."+NLO_subtraction+"."+coupling_order_NLO+".dat\n")
                if parameter_list.get("run_NLO_EW","0") == "1":
                    new_file.write("\n")
                    new_file.write("resultdirectory = NLO.EW\n")
                    new_file.write("contribution_file = infile.result/NLO."+coupling_order_LO+".dat\n")
                    new_file.write("contribution_file = infile.result/NLO.CS."+coupling_order_NLO_EW+".dat\n")
                    if parameter_list.get("photon_induced","0") == "1":
                        new_file.write("\n")
                        new_file.write("resultdirectory = NLO.aEW\n")
                        new_file.write("contribution_file = infile.result/NLO.a"+coupling_order_LO+".dat\n")
                        new_file.write("contribution_file = infile.result/NLO.CS.a"+coupling_order_NLO_EW+".dat\n")
                        new_file.write("\n")
                        new_file.write("resultdirectory = NLO.EW+aEW\n")
                        new_file.write("contribution_file = infile.result/NLO."+coupling_order_LO+".dat\n")
                        new_file.write("contribution_file = infile.result/NLO.a"+coupling_order_LO+".dat\n")
                        new_file.write("contribution_file = infile.result/NLO.CS."+coupling_order_NLO_EW+".dat\n")
                        new_file.write("contribution_file = infile.result/NLO.CS.a"+coupling_order_NLO_EW+".dat\n")
                if parameter_list.get("run_NLO_QCD","0") == "1" and parameter_list.get("run_NLO_EW","0") == "1":
                    new_file.write("\n")
                    new_file.write("resultdirectory = NLO.QCD+EW\n")
                    new_file.write("contribution_file = infile.result/NLO."+coupling_order_LO+".dat\n")
                    new_file.write("contribution_file = infile.result/NLO."+NLO_subtraction+"."+coupling_order_NLO+".dat\n")
                    new_file.write("contribution_file = infile.result/NLO.CS."+coupling_order_NLO_EW+".dat\n")
                    new_file.write("\n")
                    new_file.write("resultdirectory = NLO.QCDxEW\n")
                    new_file.write("contribution_file = infile.result/NLO."+coupling_order_LO+".dat\n")
                    new_file.write("combination       = x")
                    new_file.write("contribution_file = infile.result/NLO."+NLO_subtraction+"."+coupling_order_NLO+".dat\n")
                    new_file.write("combination       = x")
                    new_file.write("contribution_file = infile.result/NLO.CS."+coupling_order_NLO_EW+".dat\n")                    
                    if parameter_list.get("photon_induced","0") == "1":
                        new_file.write("\n")
                        new_file.write("resultdirectory = NLO.QCD+EW+aEW\n")
                        new_file.write("contribution_file = infile.result/NLO."+coupling_order_LO+".dat\n")
                        new_file.write("contribution_file = infile.result/NLO."+NLO_subtraction+"."+coupling_order_NLO+".dat\n")
                        new_file.write("contribution_file = infile.result/NLO.CS."+coupling_order_NLO_EW+".dat\n")
                        new_file.write("contribution_file = infile.result/NLO.a"+coupling_order_LO+".dat\n")
                        new_file.write("contribution_file = infile.result/NLO.CS.a"+coupling_order_NLO_EW+".dat\n")
            if "NNLO" in order_in:
                new_file.write("\n")
                new_file.write("resultdirectory = NNLO.QCD\n")
                new_file.write("contribution_file = infile.result/NNLO."+coupling_order_LO+".dat\n")
                new_file.write("contribution_file = infile.result/NNLO."+NLO_subtraction+"."+coupling_order_NLO+".dat\n")
                new_file.write("contribution_file = infile.result/NNLO.QT-CS."+coupling_order_NNLO+".dat\n")
                if int(parameter_list["loop_induced"]) > 0:
                    

                    if parameter_list.get("add_NLO_EW","0") == "1":
                        new_file.write("\n")
                        new_file.write("resultdirectory = NNLO.EW\n")
                        new_file.write("contribution_file = infile.result/NNLO."+coupling_order_LO+".dat\n")
                        new_file.write("contribution_file = infile.result/NNLO.CS."+coupling_order_NLO_EW+".dat\n")
                        if parameter_list.get("photon_induced","0") == "1":
                            new_file.write("\n")
                            new_file.write("resultdirectory = NNLO.aEW\n")
                            new_file.write("contribution_file = infile.result/NNLO.a"+coupling_order_LO+".dat\n")
                            new_file.write("contribution_file = infile.result/NNLO.CS.a"+coupling_order_NLO_EW+".dat\n")
                            new_file.write("\n")
                            new_file.write("resultdirectory = NNLO.EW+aEW\n")
                            new_file.write("contribution_file = infile.result/NNLO."+coupling_order_LO+".dat\n")
                            new_file.write("contribution_file = infile.result/NNLO.a"+coupling_order_LO+".dat\n")
                            new_file.write("contribution_file = infile.result/NNLO.CS."+coupling_order_NLO_EW+".dat\n")
                            new_file.write("contribution_file = infile.result/NNLO.CS.a"+coupling_order_NLO_EW+".dat\n")
                        new_file.write("\n")
                        new_file.write("resultdirectory = NNLO.QCD+EW\n")
                        new_file.write("contribution_file = infile.result/NNLO."+coupling_order_LO+".dat\n")
                        new_file.write("contribution_file = infile.result/NNLO."+NLO_subtraction+"."+coupling_order_NLO+".dat\n")
                        if abs(int(parameter_list["loop_induced"])) > 0:
                            new_file.write("contribution_file = infile.result/NNLO."+coupling_order_NNLO+".dat\n")
                        if abs(int(parameter_list["loop_induced"])) > 1:
                            new_file.write("contribution_file = infile.result/NNNLO."+NLO_subtraction+"."+coupling_order_NNNLO+".dat\n")
                        new_file.write("contribution_file = infile.result/NNLO.QT-CS."+coupling_order_NNLO+".dat\n")
                        new_file.write("contribution_file = infile.result/NNLO.CS."+coupling_order_NLO_EW+".dat\n")
                        new_file.write("\n")
                        new_file.write("resultdirectory = NNLO.QCDxEW\n")
                        new_file.write("contribution_file = infile.result/NNLO."+coupling_order_LO+".dat\n")
                        new_file.write("combination       = x")
                        new_file.write("contribution_file = infile.result/NNLO."+NLO_subtraction+"."+coupling_order_NLO+".dat\n")
                        if abs(int(parameter_list["loop_induced"])) > 0:
                            new_file.write("contribution_file = infile.result/NNLO."+coupling_order_NNLO+".dat\n")
                        if abs(int(parameter_list["loop_induced"])) > 1:
                            new_file.write("contribution_file = infile.result/NNNLO."+NLO_subtraction+"."+coupling_order_NNNLO+".dat\n")
                        new_file.write("contribution_file = infile.result/NNLO.QT-CS."+coupling_order_NNLO+".dat\n")
                        new_file.write("contribution_file = infile.result/NNLO.CS."+coupling_order_NLO_EW+".dat\n")
                        new_file.write("combination       = x")
                        new_file.write("contribution_file = infile.result/NLO.CS."+coupling_order_NLO_EW+".dat\n")                    
                        if parameter_list.get("photon_induced","0") == "1":
                            new_file.write("\n")
                            new_file.write("resultdirectory = NNLO.QCD+EW+aEW\n")
                            new_file.write("contribution_file = infile.result/NNLO."+coupling_order_LO+".dat\n")
                            new_file.write("contribution_file = infile.result/NNLO."+NLO_subtraction+"."+coupling_order_NLO+".dat\n")
                            if abs(int(parameter_list["loop_induced"])) > 0:
                                new_file.write("contribution_file = infile.result/NNLO."+coupling_order_NNLO+".dat\n")
                            if abs(int(parameter_list["loop_induced"])) > 1:
                                new_file.write("contribution_file = infile.result/NNNLO."+NLO_subtraction+"."+coupling_order_NNNLO+".dat\n")
                            new_file.write("contribution_file = infile.result/NNLO.QT-CS."+coupling_order_NNLO+".dat\n")
                            new_file.write("contribution_file = infile.result/NNLO.CS."+coupling_order_NLO_EW+".dat\n")
                            new_file.write("contribution_file = infile.result/NNLO.a"+coupling_order_LO+".dat\n")
                            new_file.write("contribution_file = infile.result/NNLO.CS.a"+coupling_order_NLO_EW+".dat\n")
                if abs(int(parameter_list["loop_induced"])) > 0:
                    new_file.write("\n")
                    new_file.write("resultdirectory = loopLOgg.QCD\n")
                    new_file.write("contribution_file = infile.result/NNLO."+coupling_order_NNLO+".dat\n")
                if abs(int(parameter_list["loop_induced"])) > 1:
                    new_file.write("\n")
                    new_file.write("resultdirectory = loopNLOgg.QCD\n")
                    new_file.write("contribution_file = infile.result/NNLO."+coupling_order_NNLO+".dat\n")
                    new_file.write("contribution_file = infile.result/NNNLO."+NLO_subtraction+"."+coupling_order_NNNLO+".dat\n")
                if (int(parameter_list["loop_induced"]) > 0):
                    new_file.write("\n")
                    new_file.write("resultdirectory = NNLO_LOgg.QCD\n")
                    new_file.write("contribution_file = infile.result/NNLO."+coupling_order_LO+".dat\n")
                    new_file.write("contribution_file = infile.result/NNLO."+NLO_subtraction+"."+coupling_order_NLO+".dat\n")
                    new_file.write("contribution_file = infile.result/NNLO.QT-CS."+coupling_order_NNLO+".dat\n")
                    new_file.write("contribution_file = infile.result/NNLO."+coupling_order_NNLO+".dat\n")
                
                if (int(parameter_list["loop_induced"]) == 2):
                    new_file.write("\n")
                    new_file.write("resultdirectory = nNNLO.QCD\n")
                    new_file.write("contribution_file = infile.result/NNLO."+coupling_order_LO+".dat\n")
                    new_file.write("contribution_file = infile.result/NNLO."+NLO_subtraction+"."+coupling_order_NLO+".dat\n")
                    new_file.write("contribution_file = infile.result/NNLO.QT-CS."+coupling_order_NNLO+".dat\n")
                    new_file.write("contribution_file = infile.result/NNLO."+coupling_order_NNLO+".dat\n")
                    new_file.write("contribution_file = infile.result/NNNLO."+NLO_subtraction+"."+coupling_order_NNNLO+".dat\n")
                new_file.write("\n")
#}}}
#}}}
#{{{ class: Dummyopen(object)
class Dummyopen(object):
# dummy class to be called def: write(self, data) to do nothing
    def write(self, data):
        pass # ignore the data
    def __enter__(self): return self
    def __exit__(*x): pass
#}}}
#{{{ def: line_prepender(filename, line)
def line_prepender(filename, line):
    # adds line at beginning of file
    with open(filename, 'r+') as f:
        content = f.read()
        f.seek(0, 0)
        f.write(line.rstrip('\r\n') + '\n' + content)
#}}}
#{{{ def: get_nr_of_lines(filename)
def get_nr_of_lines(filename):
# returns the number of lines of a file (filename)
    """ Return number of lines of a file
    """  
    nr_of_lines = 0
    with open(filename) as f:
        for nr_of_lines, l in enumerate(f):
            pass
    return nr_of_lines + 1
#}}}
#{{{ class: run_class()
class run_class(): # class to run C++ executable of MUNICH in different modes (multicore, cluster)
#{{{ def: __init__(self,mode,grid_folder,main_run_folder,NLO_subtraction,order,set_parallel_runs,grid_assignment,include_loop_induced,config_list)
    def __init__(self,mode,grid_folder,main_run_folder,NLO_subtraction,order,set_parallel_runs,grid_assignment,include_loop_induced,config_list):
        if not run_mode in ["run_results","run_gnuplot"]:
            out.print_info("Now it's time for running...")
            out.print_info("Running in %s mode..." % mode)
        self.errors_flag = False
        self.grid_dirs = []
        self.loop_grid_dirs = []
        self.phase = 0
        # get grids to be run for warmup (separated in normal process folders and loop-induced ones)
        # these are class variables for later usage
        if new_MATRIX_run:
            self.grid_dirs, self.loop_grid_dirs = self.get_dirs("grid",NLO_subtraction,order)
            # get the channels associated to those grid dirs
            self.channels = self.get_dir_channels(self.grid_dirs) # these are class variables for later usage
            self.loop_channels = self.get_dir_channels(self.loop_grid_dirs) # these are class variables for later usage
            # get run dirs for the main run (separated in normal process folders and loop-induced ones)
            self.run_dirs, self.loop_run_dirs = self.get_dirs(main_run_folder,NLO_subtraction,order)
        else:
            self.grid_dirs, self.loop_grid_dirs = self.get_dirs_old(grid_folder,NLO_subtraction,order,1)
            # get the channels associated to those grid dirs
            self.channels = self.get_dir_channels_old(self.grid_dirs) # these are class variables for later usage
            self.loop_channels = self.get_dir_channels_old(self.loop_grid_dirs) # these are class variables for later usage
            self.run_dirs, self.loop_run_dirs = self.get_dirs_old(main_run_folder,NLO_subtraction,order,2)
        if not self.grid_dirs and not self.loop_grid_dirs and run_mode in ["run","run_grid", "run_pre", "run_main", "run_pre_and_main","run_without_pre","run_main_without_pre"]:
# should be improved to make sure that *all* required grid folders have been found
            out.print_error("No grid folders found. Stopping the code...")
        # make all external variables class variables, so that they can be changed later on
        # important particularly for runmode
        self.runmode=mode
        self.grid_folder=grid_folder
        self.main_run_folder=main_run_folder
        self.NLO_subtraction=NLO_subtraction[0] 
        self.order=order
# not used anymore:
#        self.combine_distributions_path = pjoin(munich_dir,"bin","modules","combine_distributions.py")
#        self.set_parallel_runs = set_parallel_runs
        # manual dictionary which connects level 3 folders with required grids, needs to be changed 
        # if, eg, K+P terms should get different phase-space, or when QED is considered as well
        self.grid_assignment=grid_assignment
        self.include_loop_induced=include_loop_induced
        self.grid_dirs_for_run_dirs = {}
        self.grid_dirs_for_run_dirs = self.get_grid_dirs_for_run_dirs() # assign the grid dirs to the runs
        self.config_list = config_list
        self.runtime_table = multidim_dict(3)
        # hard-coded parameters for extrapolation that might be adjusted with experience:
        # minimal number of events per channel
        self.min_events_per_channel = 50000
        # minimal number of parallelizations per channel (at least 2 in case one goes wrong)
        self.min_parallel_pre_run = 1 # in pre run parallelize at least with two instances
        self.min_parallel_per_channel = 1 # in main run (that means one additional; make sense because that is why we extrapolate the runtimes)
        run_class.max_nr_of_tries = 3
        # this is to make sure that if you abort the script the termination of jobs works as expected (or is not done because no jobs started)
        self.jobs_started = False
        self.pre_parallel_printed = [] # empty list, add folders for which warning about restriction of parallel runs was printed
        self.failed_run_list = [] # to print out all failed runs in the end
        self.top_order = max(self.order, key=len) # often needed: highest order of run
        # set common infiles for extrapolation/result combination which also uniquely determin result folder name of MUNICH
        self.infile_extrapolation   = pjoin(fold.run_folder_path,"result","infile.MATRIX."+self.top_order+".extrapolation.dat")
        self.infile_result          = pjoin(fold.run_folder_path,"result","infile.MATRIX."+self.top_order+".result.dat")
        self.infile_distribution    = pjoin(fold.run_folder_path,"result","infile.MATRIX."+self.top_order+".distribution.dat")
        self.folder_extrapolation   = pjoin(fold.run_folder_path,"result","result.."+os.path.basename(self.infile_extrapolation).lstrip("infile.").replace(".dat",""))
        self.folder_result          = pjoin(fold.run_folder_path,"result","result.."+os.path.basename(self.infile_result).lstrip("infile.").replace(".dat",""))
        self.folder_distribution    = pjoin(fold.run_folder_path,"result","result.."+os.path.basename(self.infile_distribution).lstrip("infile.").replace(".dat",""))
#}}}
#{{{ def: get_grid_dirs_for_run_dirs(self)
    def get_grid_dirs_for_run_dirs(self):
        # returns a dictionary that determines the grid folder of each run folder
        grid_dirs_for_run_dirs = {}
        for run_dir in self.run_dirs:
            dir_matched = False
            for grid_dir in self.grid_dirs:
                # check if contribution is the same and to distinguish photon-initiated also if coupling XX or aXX is the same
                contribution_match = self.get_contribution(run_dir) == self.get_contribution(grid_dir) and self.get_coupling(run_dir) == self.get_coupling(grid_dir)
                both_photon_induced = (self.get_coupling(run_dir).startswith("a") and self.get_coupling(grid_dir).startswith("a"))
                both_not_photon_induced = (not self.get_coupling(run_dir).startswith("a") and not self.get_coupling(grid_dir).startswith("a"))
                assignment_match = self.grid_assignment.get(self.get_contribution(run_dir)) == self.get_contribution(grid_dir) and (both_photon_induced or both_not_photon_induced)
                if contribution_match or assignment_match:
                    if dir_matched: out.print_error("There seems to be a double assignment of grids for run dir %s" % run_dir)
                    grid_dirs_for_run_dirs[run_dir]=grid_dir
                    dir_matched = True
            if not dir_matched:
                out.print_error("Could not find correct grid dir for run dir %s, check grid assignments" % run_dir)
        for run_dir in self.loop_run_dirs:
            dir_matched = False
            for grid_dir in self.loop_grid_dirs:
                contribution_match = self.get_contribution(run_dir) == self.get_contribution(grid_dir)
                assignment_match = self.grid_assignment.get(self.get_contribution(run_dir)) == self.get_contribution(grid_dir)
                if contribution_match or assignment_match:
                    grid_dirs_for_run_dirs[run_dir]=grid_dir
                    dir_matched = True
            if not dir_matched:
                out.print_error("Could not find correct grid dir for loop run dir %s, check grid assignments" % run_dir)
        return grid_dirs_for_run_dirs
#}}}
#{{{ def: warmup(self,wp_phase)
    def warmup(self,wp_phase): # pre-run to set up a required grids for run
        # wp_phase: switch allows to redo the warmup run if some runs have not correctly finished
        self.phase = wp_phase
        # (1) normal warmup, (2) redo warmup for failed runs
        if self.phase == 1: out.print_info("Starting grid setup (warmup)...")
        # Create a list of jobs and then iterate through
        # the number of processes appending each process to
        # the job list 
        job_list = []
        # add required jobs for warmup to job list split into:
        for grid_dir in self.grid_dirs: # normal dirs
            # create grid dir with channels in log folder
            if new_MATRIX_run:
                fold.create_dir(grid_dir,self.channels[grid_dir])
            # add grid identifier to these folders
            fold.add_dir_identifier(grid_dir,"grid")
            self.set_inputs_grid_run(grid_dir)
            for channel in self.channels[grid_dir]:
                if self.phase == 2 or continue_run:
                    # if we are in phase 2 (restarted runs) or we want an old run to contine; we ommit all jobs which have finished
                    if self.job_correctly_finished(grid_dir,channel):
                        continue
                job_list.append([grid_dir,channel])
        # switch later to turn off loop-induced sub-processes 
        if self.include_loop_induced:
            for grid_dir in self.loop_grid_dirs: # loop dirs
                # create grid dir with channels in log folder
                if new_MATRIX_run:
                    fold.create_dir(grid_dir,self.loop_channels[grid_dir])
                # add grid identifier to these folders
                fold.add_dir_identifier(grid_dir,"grid")
                self.set_inputs_grid_run(grid_dir)
                for channel in self.loop_channels[grid_dir]:
                    if self.phase == 2 or continue_run:
                        # if we are in phase 2 (restarted runs) or we want an old run to contine; we ommit all jobs which have finished
                        if self.job_correctly_finished(grid_dir,channel):
                            continue
                    job_list.append([grid_dir,channel])
        if job_list and self.phase == 2:
            out.print_info("Re-starting grid setup (warmup) for runs that failed...")

        self.jobs_started = True
        # reverse order of list so that NNLO runs start first
        job_list = list(reversed(sorted(job_list)))
        if job_list: self.submit_jobs(job_list)
#}}}
#{{{ def: clear_warmup(self)
    def clear_warmup(self):
        # always clean everything which would come after
        self.clear_pre_run()
        self.clear_main_run()
        # no cleaning if we continue a run; 2do: maybe still try cleaning for each folder AND channel that did NOT correctly finished?
        # self.job_correctly_finished(grid_dir,channel)
        if continue_run:
            return
        # remove unnecessary content of grid dirs
        something_cleaned = False
        for grid_dir in self.grid_dirs: # normal grid dirs
            needed_cleaning = fold.clean_run_dir(grid_dir) # returns true when there was something to clean
            if needed_cleaning:
                something_cleaned = True
        if self.include_loop_induced:
            for grid_dir in self.loop_grid_dirs: # loop grid dirs
                needed_cleaning = fold.clean_run_dir(grid_dir) # returns true when there was something to clean
                if needed_cleaning:
                    something_cleaned = True
        if something_cleaned:
            out.print_info("Cleaning previous grid runs (warmup)...")
#}}}
#{{{ def: clear_pre_run(self)
    def clear_pre_run(self):
        # always clean everything which would come after
        self.clear_main_run()
        self.clear_pre_results()
        # no cleaning if we continue a run; 2do: maybe still try cleaning for each folder AND channel that did NOT correctly finish?
        # self.job_correctly_finished(grid_dir,channel)
        if continue_run:
            return
        # remove all unnecessary content of run.0 dirs
        something_cleaned = False
        for run_dir in self.run_dirs: # normal grid dirs
            needed_cleaning = fold.clean_run_dir(run_dir) # returns true when there was something to clean
            if needed_cleaning:
                something_cleaned = True
        if self.include_loop_induced:
            for run_dir in self.loop_run_dirs: # loop grid dirs
                needed_cleaning = fold.clean_run_dir(run_dir) # returns true when there was something to clean
                if needed_cleaning:
                    something_cleaned = True
        # remove all run.1-XX dirs
        for run_dir_0 in self.run_dirs: # normal run dirs
            run_dir_up = run_dir_0.rsplit('/', 1)[0]
            run_dirs_in_folder = [ d for d in os.listdir(run_dir_up) if os.path.isdir(pjoin(run_dir_up,d)) and d.startswith("run.") and not d == "run.0"]
            for run_dir in run_dirs_in_folder:
                shutil.rmtree(pjoin(run_dir_up,run_dir))
                something_cleaned = True
        if self.include_loop_induced:
            for run_dir in self.loop_run_dirs: # loop run dirs
                run_dir_up = run_dir.rsplit('/', 1)[0]
                run_dirs_in_folder = [ d for d in os.listdir(run_dir_up) if os.path.isdir(pjoin(run_dir_up,d)) and d.startswith("run.") and not d == "run.0"]
                for run_dir in run_dirs_in_folder:
                    shutil.rmtree(pjoin(run_dir_up,run_dir))
                    something_cleaned = True
        if something_cleaned:
            out.print_info("Cleaning previous extrapolation runs (pre run)...")
#}}}
#{{{ def: main_run(self,mn_phase)
    def main_run(self,mn_phase): # main run to start cross section computation in all folders
        # mn_phase: switch allows to redo the main run if some runs have not correctly finished
        self.phase = mn_phase
        if self.phase == -1: out.print_info("Starting runs to determine runtimes (pre run)...")
        if self.phase == 1: out.print_info("Starting cross section computation (main run)...")
        # Create a list of jobs and then iterate through
        # the number of processes appending each process to
        # the job list 
        job_list = []
        # add required jobs for main run to job list split into:
        for run_dir in self.run_dirs: # normal dirs
            # create run.0 dir for each run dir with channels in log folder
            if new_MATRIX_run:
                fold.create_dir(run_dir,self.channels[self.grid_dirs_for_run_dirs[run_dir]])
            for channel in self.channels[self.grid_dirs_for_run_dirs[run_dir]]: # loop through channels
                if self.phase < 0: # for pre run use predefine number of parallel runs, depending on contribution
                    parallel_runs = self.get_parallel_pre_runs_for_contribution(run_dir)
                else: # for main run use computed number of parallel runs from runtime extrapolation
                    parallel_runs = self.runtime_table[run_dir][channel]["parallel_jobs"]
                for k in range(0,parallel_runs): # for parallel running of same contributions
                    # change here the required inputs (in file_parameter.dat) for the pre run (in run.0-parallel_runs_of_pre folder)
                    if self.phase < 0:
                        i=k # this is to use run.0-parallel_runs_of_pre for the extrapolation run
                    else:
                        i=k+self.get_parallel_pre_runs_for_contribution(run_dir) # this is to use run.parallel_runs_of_pre and onwards for the main runs
                    run_dir_i = run_dir.replace(main_run_folder,"run.%s" % i)
                    # if we are in phase 2 (restarted runs) or we want an old run to contine; we ommit all jobs which have finished
                    # CAREFULL: if we continue a run we HAVE TO make sure we use the same inputs as before; 2do: implement cross check of inputs; 2do: remove cleaning
                    if abs(self.phase) == 2 or continue_run:
                        if self.job_correctly_finished(run_dir.replace(main_run_folder,"run.%s" % i),channel):
                            continue
                    # for certain processes exclude VT2 in prerun here
                    if self.phase < 0 and "VT2.QCD" in run_dir and prc.process_name in VT2_use_default_runtime:
                        out.print_info("VT2 job (dir: %s, channel: %s) excluded from extrapolation run, because amplitude to slow. Will assume a default setting for the main run." % (run_dir_i,channel))
                        continue
                    # set inputs the same for all channels in pre runs
                    if self.phase < 0:
                        self.set_inputs_pre_run(run_dir_i,i)
                        # add pre run identifier to these folders
                        fold.add_dir_identifier(run_dir_i,"pre")
                    # change here the required inputs (in file_parameter.dat) inside the run.X folders separately for the different channels
                    if self.phase > 0:
                        self.set_inputs_main_run(run_dir_i,channel,i)
                        # add main run identifier to these folders
                        fold.add_dir_identifier(run_dir_i,"main")
                    job_list.append([run_dir_i,channel])
        # switch later to turn off loop-induced sub-processes 
        if self.include_loop_induced:
            for run_dir in self.loop_run_dirs: # loop dirs
              # create run.0 dir for each run dir with channels in log folder
              if new_MATRIX_run:
                  fold.create_dir(run_dir,self.loop_channels[self.grid_dirs_for_run_dirs[run_dir]])
              for channel in self.loop_channels[self.grid_dirs_for_run_dirs[run_dir]]: # loop through channels
                if self.phase < 0:
                    parallel_runs = self.get_parallel_pre_runs_for_contribution(run_dir)
                else:
                    parallel_runs = self.runtime_table[run_dir][channel]["parallel_jobs"]
                for k in range(0,parallel_runs): # for parallel running of same contributions
                    # change here the required inputs (in file_parameter.dat) for the pre run (in run.0-parallel_runs_of_pre folder)
                    if self.phase < 0:
                        i=k # this is to use run.0-parallel_runs_of_pre for the extrapolation run
                    else:
                        i=k+self.get_parallel_pre_runs_for_contribution(run_dir) # this is to use run.parallel_runs_of_pre and onwards for the main runs
                    run_dir_i = run_dir.replace(main_run_folder,"run.%s" % i)
                    # if we are in phase 2 (restarted runs) or we want an old run to contine; we ommit all jobs which have finished
                    if abs(self.phase) == 2 or continue_run:
                        if self.job_correctly_finished(run_dir.replace(main_run_folder,"run.%s" % i),channel):
                            continue
                    # set inputs the same for all channels in pre runs
                    if self.phase < 0:
                        self.set_inputs_pre_run(run_dir_i,i)
                        # add pre run identifier to these folders
                        fold.add_dir_identifier(run_dir_i,"pre")
                    # change here the required inputs (in file_parameter.dat) inside the run.X folders separately for the different channels
                    if self.phase > 0:
                        self.set_inputs_main_run(run_dir_i,channel,i)
                        # add main run identifier to these folders
                        fold.add_dir_identifier(run_dir_i,"main")
                    job_list.append([run_dir_i,channel])
        if job_list:
            if self.phase == 2:
                out.print_info("Re-starting cross section computation (main run) for runs that failed...")
            elif self.phase == -2:
                out.print_info("Re-starting extrapolation runs (pre run) for runs that failed...")
        # reverse order of list so that NNLO runs start first, in particular VT2 very early (~ third position)
        job_list = list(reversed(sorted(job_list)))
        # for job in job_list:
        #     print job
        # exit(0)
        # this is to make sure that if you abort the script the termination of jobs works as expected (or is not done because no jobs started)
        self.jobs_started = True
        # run the jobs from the list (only if there are jobs in job_list)
        if job_list: self.submit_jobs(job_list)
#}}}
#{{{ def: clear_main_run(self)
    def clear_main_run(self):
        # no cleaning if we continue a run; 2do: maybe still try cleaning for each folder AND channel that did NOT correctly finish?
        # self.job_correctly_finished(grid_dir,channel)
        if continue_run:
            return
        # remove all run.1-XX dirs
        something_cleaned = False
        for run_dir_0 in self.run_dirs: # normal run dirs
            run_dir_up = run_dir_0.rsplit('/', 1)[0]
            run_dirs_in_folder = fold.get_dirs_with_identifier(run_dir_up,"main")
            for run_dir in run_dirs_in_folder:
                shutil.rmtree(pjoin(run_dir_up,run_dir))
                something_cleaned = True
        if self.include_loop_induced:
            for run_dir in self.loop_run_dirs: # loop run dirs
                run_dir_up = run_dir.rsplit('/', 1)[0]
                run_dirs_in_folder = fold.get_dirs_with_identifier(run_dir_up,"main")
                for run_dir in run_dirs_in_folder:
                    shutil.rmtree(pjoin(run_dir_up,run_dir))
                    something_cleaned = True
        if something_cleaned:
            out.print_info("Cleaning previous cross section runs (main run)...")
#}}}
#{{{ def: collect_results(self)
    def collect_results(self): # collect results after the main run has finished
        out.print_info("Collecting and combining results...")
        # Create a list of jobs and then iterate through
        # the number of processes appending each process to
        # the job list 
        path = pjoin(fold.run_folder_path,"result","infile.result")
#        files_in_path = [ f for f in os.listdir(path) if f.endswith(".dat") and f.split('.')[0] in self.order and os.path.isfile(pjoin(path,f)) ]
# 2do: make sure this causes no problems somewhere:
        files_in_path = [ f for f in os.listdir(path) if f.endswith(".dat") and os.path.isfile(pjoin(path,f)) ]
        for file_name in files_in_path:
            inp.add_directories_to_result_files(pjoin(path,file_name),self.runtime_table,1)
 
        # for the distributions we only need one collection run
        distributions_computed = False
        folder = {}
        job_list = []
        if "NNLO" in self.order:
            folder["NNLO"] = pjoin(fold.run_folder_path,"result","result.NNLO.QT-CS.NLO."+self.NLO_subtraction+".LO")
            infile_combination_NNLO = pjoin(fold.run_folder_path,"result","infile.result.NNLO.QT-CS.NLO."+self.NLO_subtraction+".LO.dat")
            inp.write_infile_result(["LO","NLO","NNLO"],infile_combination_NNLO,self.NLO_subtraction,self.get_coupling_order("LO"),self.get_coupling_order("NLO"),self.get_coupling_order("NNLO"),self.get_coupling_order("NNNLO"),self.include_loop_induced)
            inp.input_set_entry(infile_combination_NNLO,"final_resultdirectory","result.NNLO.QT-CS.NLO."+self.NLO_subtraction+".LO")
            job_list.append(["infile.result.NNLO.QT-CS.NLO."+self.NLO_subtraction+".LO.dat","result"]) # "result" for combining total rates
            # create/overwrite infile for NNLO combination
            if int(parameter_list["switch_distribution"]) == 1: 
                infile_distribution_NNLO = pjoin(fold.run_folder_path,"result","infile.distribution.NNLO.QT-CS.NLO."+self.NLO_subtraction+".LO.dat")
                inp.write_infile_result(self.order,infile_distribution_NNLO,self.NLO_subtraction,self.get_coupling_order("LO"),self.get_coupling_order("NLO"),self.get_coupling_order("NNLO"),self.get_coupling_order("NNNLO"),self.include_loop_induced)
                inp.input_set_entry(infile_distribution_NNLO,"final_resultdirectory","result.NNLO.QT-CS.NLO."+self.NLO_subtraction+".LO_dist")
                job_list.append(["infile.distribution.NNLO.QT-CS.NLO."+self.NLO_subtraction+".LO.dat","distribution"]) # "distribution" for combining the distributions
                distributions_computed = True
                folder["distribution"] = pjoin(fold.run_folder_path,"result","result.NNLO.QT-CS.NLO."+self.NLO_subtraction+".LO_dist")
        if "NLO" in self.order:
            folder["NLO"] = pjoin(fold.run_folder_path,"result","result.NLO."+self.NLO_subtraction+".LO")
            infile_combination_NLO = pjoin(fold.run_folder_path,"result","infile.result.NLO."+self.NLO_subtraction+".LO.dat")
            inp.write_infile_result(["LO","NLO"],infile_combination_NLO,self.NLO_subtraction,self.get_coupling_order("LO"),self.get_coupling_order("NLO"),self.get_coupling_order("NNLO"),self.get_coupling_order("NNNLO"),self.include_loop_induced)
            inp.input_set_entry(infile_combination_NLO,"final_resultdirectory","result.NLO."+self.NLO_subtraction+".LO")
            job_list.append(["infile.result.NLO."+self.NLO_subtraction+".LO.dat","result"]) # "result" for combining total rates
            if int(parameter_list["switch_distribution"]) == 1 and not distributions_computed: 
                infile_distribution_NLO = pjoin(fold.run_folder_path,"result","infile.distribution.NLO."+self.NLO_subtraction+".LO.dat")
                if "LO" in self.order:
                    new_order = ["LO","NLO"]
                else:
                    new_order = ["NLO"]
                inp.write_infile_result(new_order,infile_distribution_NLO,self.NLO_subtraction,self.get_coupling_order("LO"),self.get_coupling_order("NLO"),self.get_coupling_order("NNLO"),self.get_coupling_order("NNNLO"),self.include_loop_induced)
                inp.input_set_entry(infile_distribution_NLO,"final_resultdirectory","result.NLO."+self.NLO_subtraction+".LO_dist")
                job_list.append(["infile.distribution.NLO."+self.NLO_subtraction+".LO.dat","distribution"]) # "distribution" for combining the distributions
                distributions_computed = True
                folder["distribution"] = pjoin(fold.run_folder_path,"result","result.NLO."+self.NLO_subtraction+".LO_dist")
        if "LO" in self.order:
            folder["LO"] = pjoin(fold.run_folder_path,"result","result.LO")
            infile_combination_LO = pjoin(fold.run_folder_path,"result","infile.result.LO.dat")
            inp.write_infile_result(["LO"],infile_combination_LO,self.NLO_subtraction,self.get_coupling_order("LO"),self.get_coupling_order("NLO"),self.get_coupling_order("NNLO"),self.get_coupling_order("NNNLO"),self.include_loop_induced)
            inp.input_set_entry(infile_combination_LO,"final_resultdirectory","result.LO")
            job_list.append(["infile.result.LO.dat","result"]) # "result" for combining total rates
            if int(parameter_list["switch_distribution"]) == 1 and not distributions_computed: 
                infile_distribution_LO = pjoin(fold.run_folder_path,"result","infile.distribution.LO.dat")
                inp.write_infile_result(self.order,infile_distribution_LO,self.NLO_subtraction,self.get_coupling_order("LO"),self.get_coupling_order("NLO"),self.get_coupling_order("NNLO"),self.get_coupling_order("NNNLO"),self.include_loop_induced)
                inp.input_set_entry(infile_distribution_LO,"final_resultdirectory","result.LO_dist")
                job_list.append(["infile.distribution.LO.dat","distribution"]) # "distribution" for combining the distributions
                distributions_computed = True
                folder["distribution"] = pjoin(fold.run_folder_path,"result","result.LO_dist")

        # 2do: add some sanity checks to make sure all data is there

        # use local mode, but save original runmode
        runmode_sav = self.runmode
        run.runmode = "multicore" # the result combination is always done locally
        # run the jobs from the list (only if there are jobs in job_list)
        if job_list: self.submit_jobs(job_list)
        self.runmode = runmode_sav # change back to original runmode

        for key,folda in folder.iteritems():
            if not os.path.exists(folda):
                if "_dist" in folda:
                    logfile = pjoin(fold.run_folder_path,"result","infile."+folda.rsplit("/",1)[1].replace("_dist","").replace("result","distribution")+".dat.distribution.log")
                    errfile = pjoin(fold.run_folder_path,"result","infile."+folda.rsplit("/",1)[1].replace("_dist","").replace("result","distribution")+".dat.distribution.err")
                else:
                    logfile = pjoin(fold.run_folder_path,"result","infile."+folda.rsplit("/",1)[1]+".dat.result.log")
                    errfile = pjoin(fold.run_folder_path,"result","infile."+folda.rsplit("/",1)[1]+".dat.result.err")
                if os.path.exists(logfile) or os.path.exists(logfile):
                    out.print_error_no_stop("There was a problem in the result run. The result folder %s was not created (see log- and err-file):" % folda)
                    out.print_last_five_lines_of_file(logfile)
                    out.print_last_five_lines_of_file(errfile)
                else:
                    out.print_error_no_stop("There was a problem in the result run. The result folder %s was not created. It seems the result combination has not even been started since neither log- nor err-file exists." % folda)
                exit(0)

        # redirect stout to logfile
        # orig_stdout = sys.stdout
        # logfile = pjoin(fold.run_folder_path,"result","infile.result.LO.dat"+"."+"distribution"+".log")
        # f = file(logfile, 'w')
        # sys.stdout = f

        # combine_distributions.combine_distributions(pjoin(fold.run_folder_path,"result"),"infile.result.LO.dat")
        # sys.stdout = orig_stdout
        # f.close()

        result_method = "CV"

        # for new 7/9-point varation create folder that was called "equal" before and move rate files in there; 
        # this way we can use the result collection of the ordinary "CV" variation
        for order,folder_path in folder.iteritems():
            if order == "distribution":
                continue
            if res.seven_point_variation(pjoin(folder_path,result_method)): # 7-point variation
                new_folder = pjoin(folder_path,result_method,"7-point")
            elif res.nine_point_variation(pjoin(folder_path,result_method)): # 9-point variation
                new_folder = pjoin(folder_path,result_method,"9-point")
            else:
                out.print_error("Trying whoot to collect results from a folder (path: %s) that contains neither 7-point nor 9-point variation." % pjoin(folder_path,result_method))
            try:
                os.makedirs(new_folder)
            except:
                pass
            files_and_folders_to_move = glob.iglob(pjoin(folder_path,result_method,"*"))
            for file_and_folder in files_and_folders_to_move:
                if not "scale." in file_and_folder and not "equal" in file_and_folder and not "7-point" in file_and_folder and not "9-point" in file_and_folder:
                    shutil.move(file_and_folder,pjoin(new_folder,file_and_folder.rsplit('/',1)[1]))

        # copy relevant results to MATRIX results folder
        self.cross_section = multidim_dict(3) # multidimensonal dictionary
        self.MATRIX_rates_folder = {}
        for this_order in self.order:
            result_folder = pjoin(process_dir,"result",run_folder,this_order+"-run")
            try:
                os.makedirs(result_folder)
            except:
                pass
            files_to_copy = self.get_named_files(folder[this_order],"."+result_method+".",2)
            files_to_copy = [f for f in files_to_copy if "LO" in f.rsplit('/',1)[1] or "NLO" in f.rsplit('/',1)[1] or "NNLO" in f.rsplit('/',1)[1]]
            files_to_copy = [f for f in files_to_copy if not ("extrapolated" in f.rsplit('/',1)[1] and (".LO." in f.rsplit('/',1)[1] or (".NLO." in f.rsplit('/',1)[1] and self.NLO_subtraction == "CS")))]
            if self.include_loop_induced and this_order == "NNLO": # add separate contribution for loop-induced process
                files_to_copy += self.get_named_files(folder[this_order],result_method+".NNLO."+self.get_coupling_order("NNLO")+".dat",3)
            # these file do not contain the wanted information since LO has always LO PDFs
            # so copy the right files first into MATRIX_rates
            for file_src in files_to_copy: # some special treatment for loop-induced contribution
                file_src_cutvar = file_src.replace("plot.CV","plot.qTcut")
#                print("file_src_cutvar="+file_src_cutvar)
#                if self.cross_section[this_order][file_src]["central"] != 0:
#                     if 'plot.qTcut.NNLO.QCD.dat' in file_src_cutvar:
#                         qTextr,qTerr = inp.get_qT_extrapolation_error_from_file(file_src_cutvar,2)
# #                        print("extrapolated cross section is "+np.str(qTextr)+" +/- "+np.str(qTerr))
#                     elif 'plot.qTcut.NLO.QCD.dat' in file_src_cutvar:
#                         qTextr,qTerr = inp.get_qT_extrapolation_error_from_file(file_src_cutvar,1)
# #                        print("extrapolated cross section is "+np.str(qTextr)+" +/- "+np.str(qTerr))


                if result_method+".NNLO."+self.get_coupling_order("NNLO")+".dat" in file_src:
                    loop_induced = True
                else:
                    loop_induced = False
                if not loop_induced:
                    self.MATRIX_rates_folder[this_order] = pjoin(file_src.rsplit('/', 1)[0],"MATRIX_rates")
                    try:
                        os.makedirs(self.MATRIX_rates_folder[this_order])
                    except:
                        pass
                file_name = file_src.rsplit('/', 1)[1]
                if loop_induced:
                    file_name = file_name.replace(".dat",".loop.dat")
                file_to_copy = file_src
                file_dest = pjoin(self.MATRIX_rates_folder[this_order],file_name)
                # change the files to be copied if PDF order does not correspond to this_order
                if ".LO." in file_name and not this_order == "LO":
                    files_right_PDF_order = self.get_named_files(folder[this_order],"."+result_method+"."+this_order+"."+self.get_coupling_order("LO")+".dat",3)
                    for file_right_PDF_order in files_right_PDF_order:
                        file_to_copy = file_right_PDF_order
                    shutil.copy(file_to_copy,file_dest)  
                elif ".NLO." in file_name and not this_order == "NLO":
                    all_files_right_PDF_order =  self.get_named_files(folder[this_order],"."+result_method+"."+this_order+"."+self.get_coupling_order("LO")+".dat",3)
                    all_files_right_PDF_order += self.get_named_files(folder[this_order],"."+result_method+"."+this_order+"."+self.NLO_subtraction+"."+self.get_coupling_order("NLO")+".dat",3)
                    files_right_PDF_order = [f for f in all_files_right_PDF_order if not f.endswith("~")] #remove the files that end with "~"
                    # add up these two files and write it into MATRIX_rates folder
                    res.add_result_files(files_right_PDF_order,file_dest)  
                else:
                    shutil.copy(file_to_copy,file_dest) 
            src_files = os.listdir(self.MATRIX_rates_folder[this_order])
            # now copy all files from MATRIX_rates_folder into MATRIX result folder
            # renaming done here


            for file_name in src_files:
        
                MATRIX_result_folder = pjoin(process_dir,"result",run_folder,this_order+"-run")
                file_name_MATRIX = "rate_"+file_name.replace(".","_").replace("plot_","").replace("CV_","").replace("_dat",".dat").replace("NNLO_"+self.get_coupling_order("NNLO")+"_loop","loop-induced_QCD").replace("_"+self.get_coupling_order("LO"),"").replace("_"+self.get_coupling_order("NLO"),"").replace("_"+self.get_coupling_order("NNLO"),"")
                file_dest = pjoin(MATRIX_result_folder,file_name_MATRIX)
                file_src = pjoin(self.MATRIX_rates_folder[this_order], file_name)
                if (os.path.isfile(file_src)):
                    # save cross section information
                    central, err, up, down = inp.get_cross_sections_from_file(file_src)
                    self.cross_section[this_order][file_src]["central"] = central
                    self.cross_section[this_order][file_src]["err"]     = err
                    self.cross_section[this_order][file_src]["up"]      = up
                    self.cross_section[this_order][file_src]["down"]    = down
                    if central == 0.:
#                        out.print_error("Combined cross section appears to be zero when trying to collect %s results. Exiting..." % this_order)
                        out.print_warning("Combined cross section appears to be zero when trying to collect %s results." % this_order)
                    # change the first column into two columns with muR and muF
                    res.convert_to_independent_scales(file_src)

                    shutil.copy(file_src,file_dest)    
                    line_prepender(file_dest,"#      muR       muF  cross_section        num_err")

        # copy distributions 
        if distributions_computed:
            files = {}
            if res.seven_point_variation_distribution(pjoin(folder["distribution"],result_method)): # 7-point variation
                central_dir = pjoin(folder["distribution"],result_method,"scale.3.3")
            elif res.nine_point_variation_distribution(pjoin(folder["distribution"],result_method)): # 9-point variation
                central_dir = pjoin(folder["distribution"],result_method,"scale.4.4")
            else:
                out.print_error("Trying to combine distributions from a folder that contains neither 7-point nor 9-point variation.")
            MATRIX_distributions_folder = pjoin(central_dir,"MATRIX_distributions")
            try:
                os.makedirs(MATRIX_distributions_folder)
            except:
                pass
            variation_dirs = [x for x in os.listdir(pjoin(folder["distribution"],result_method)) if x.startswith("scale.") and not x == central_dir.rsplit("/",1)[1]] # variation
            files["all"] = os.listdir(central_dir)

            for current_order in self.order:
                files[current_order] = [x for x in files["all"] if "."+current_order+"." in x and os.path.isfile(pjoin(central_dir,x)) and not "NLO.plus.loop.QCD.dat" in x]
                for file_name in files[current_order]:
                    file_list = []
                    file_list.append(pjoin(central_dir,file_name))
                    for dir_name in variation_dirs:
                        full_name = pjoin(folder["distribution"],result_method,dir_name,file_name)
                        file_list.append(full_name)
                    out_file = pjoin(MATRIX_distributions_folder,file_name)
                    res.get_central_min_max_from_distribution_files(file_list,out_file)

            # have distributions of loop-induced seperately in run-NNLO folder
            if self.include_loop_induced and "NNLO" in self.order:
                files["loop"] = glob.iglob(pjoin(central_dir,"NNLO."+self.get_coupling_order("NNLO"),"loop","plot.*.loop.dat"))
                for loop_file in files["loop"]:
                    file_name = loop_file.rsplit("/",1)[1]
                    file_list = []
                    file_list.append(loop_file)
                    for dir_name in variation_dirs:
                        full_name = pjoin(folder["distribution"],result_method,dir_name,"NNLO."+self.get_coupling_order("NNLO"),"loop",file_name)
                        file_list.append(full_name)
                    out_file = pjoin(MATRIX_distributions_folder,file_name.replace(".NNLO","..NNLO"))
                    res.get_central_min_max_from_distribution_files(file_list,out_file)

            # have distributions of NLO + loop-induced separately in run-NLO folder
            if NLO_plus_loop_induced and "NNLO" in self.order and "NLO" in self.order:
                files["loop"] = glob.iglob(pjoin(central_dir,"NNLO."+self.get_coupling_order("NNLO"),"loop","plot.*.loop.dat"))
                for loop_file in files["loop"]:
                    plot_name_full = loop_file.rsplit("/",1)[1]
                    plot_name_suffix = ".NNLO."+self.get_coupling_order("NNLO")+".loop.dat"
                    if plot_name_full.endswith(plot_name_suffix):
                        plot_name = plot_name_full[0:-len(plot_name_suffix)]
                    else:
                        assert(False)
                    NLO_file = pjoin(central_dir,plot_name+"..NLO.QCD.dat")
                    files_to_add = [loop_file,NLO_file]
                    file_dest = NLO_file.replace("..NLO.QCD.dat","..NLO.plus.loop.QCD.dat")
                    res.add_result_files(files_to_add,file_dest)
                    file_list = []
                    file_list.append(file_dest)
                    for dir_name in variation_dirs:
                        NLO_file = pjoin(folder["distribution"],result_method,dir_name,plot_name+"..NLO.QCD.dat")
                        files_to_add = [loop_file,NLO_file]
                        file_dest = NLO_file.replace("..NLO.QCD.dat","..NLO.plus.loop.QCD.dat")
                        res.add_result_files(files_to_add,file_dest)
                        file_list.append(file_dest)
                    out_file = pjoin(MATRIX_distributions_folder,file_dest.rsplit("/",1)[1])
                    res.get_central_min_max_from_distribution_files(file_list,out_file)

            # have distributions of NLO (with NNLO PDFs) + loop-induced separately in run-NNLO folder
            if NLO_plus_loop_induced and "NNLO" in self.order:
                files["loop"] = glob.iglob(pjoin(central_dir,"NNLO."+self.get_coupling_order("NNLO"),"loop","plot.*.loop.dat"))
                for loop_file in files["loop"]:
                    plot_name_full = loop_file.rsplit("/",1)[1]
                    plot_name_suffix = ".NNLO."+self.get_coupling_order("NNLO")+".loop.dat"
                    if plot_name_full.endswith(plot_name_suffix):
                        plot_name = plot_name_full[0:-len(plot_name_suffix)]
                    else:
                        assert(False)
                    born_file = pjoin(central_dir,"NNLO."+self.get_coupling_order("LO"),"born",plot_name+".NNLO."+self.get_coupling_order("LO")+".born.dat")
                    if self.NLO_subtraction == "CS":
                        CA_file = pjoin(central_dir,"NNLO.CS."+self.get_coupling_order("NLO"),"CA.QCD",plot_name+".NNLO.CS."+self.get_coupling_order("NLO")+".CA.QCD.dat")
                        RA_file = pjoin(central_dir,"NNLO.CS."+self.get_coupling_order("NLO"),"RA.QCD",plot_name+".NNLO.CS."+self.get_coupling_order("NLO")+".RA.QCD.dat")
                        VA_file = pjoin(central_dir,"NNLO.CS."+self.get_coupling_order("NLO"),"VA.QCD",plot_name+".NNLO.CS."+self.get_coupling_order("NLO")+".VA.QCD.dat")
                    elif self.NLO_subtraction == "QT":
                        CA_file = pjoin(central_dir,"NNLO.QT."+self.get_coupling_order("NLO"),"CT.QCD",plot_name+".NNLO.QT."+self.get_coupling_order("NLO")+".CT.QCD.dat")
                        RA_file = pjoin(central_dir,"NNLO.QT."+self.get_coupling_order("NLO"),"RT.QCD",plot_name+".NNLO.QT."+self.get_coupling_order("NLO")+".RT.QCD.dat")
                        VA_file = pjoin(central_dir,"NNLO.QT."+self.get_coupling_order("NLO"),"VT.QCD",plot_name+".NNLO.QT."+self.get_coupling_order("NLO")+".VT.QCD.dat")
                    else:
                        out.print_error("NLO_subtraction neither \"CS\" nor \"QT\" in collect_results. Exiting...")
                    files_to_add = [loop_file,born_file,CA_file,RA_file,VA_file]
                    file_dest = pjoin(central_dir,plot_name+"..NLO.prime.plus.loop.QCD.dat")
                    res.add_result_files(files_to_add,file_dest)
                    file_list = []
                    file_list.append(file_dest)
                    for dir_name in variation_dirs:
                        variation_dir = pjoin(folder["distribution"],result_method,dir_name)
                        loop_file = pjoin(variation_dir,"NNLO."+self.get_coupling_order("NNLO"),"loop",plot_name+".NNLO."+self.get_coupling_order("NNLO")+".loop.dat")
                        born_file = pjoin(variation_dir,"NNLO."+self.get_coupling_order("LO"),"born",plot_name+".NNLO."+self.get_coupling_order("LO")+".born.dat")
                        if self.NLO_subtraction == "CS":
                            CA_file = pjoin(variation_dir,"NNLO.CS."+self.get_coupling_order("NLO"),"CA.QCD",plot_name+".NNLO.CS."+self.get_coupling_order("NLO")+".CA.QCD.dat")
                            RA_file = pjoin(variation_dir,"NNLO.CS."+self.get_coupling_order("NLO"),"RA.QCD",plot_name+".NNLO.CS."+self.get_coupling_order("NLO")+".RA.QCD.dat")
                            VA_file = pjoin(variation_dir,"NNLO.CS."+self.get_coupling_order("NLO"),"VA.QCD",plot_name+".NNLO.CS."+self.get_coupling_order("NLO")+".VA.QCD.dat")
                        elif self.NLO_subtraction == "QT":
                            CA_file = pjoin(variation_dir,"NNLO.QT."+self.get_coupling_order("NLO"),"CT.QCD",plot_name+".NNLO.QT."+self.get_coupling_order("NLO")+".CT.QCD.dat")
                            RA_file = pjoin(variation_dir,"NNLO.QT."+self.get_coupling_order("NLO"),"RT.QCD",plot_name+".NNLO.QT."+self.get_coupling_order("NLO")+".RT.QCD.dat")
                            VA_file = pjoin(variation_dir,"NNLO.QT."+self.get_coupling_order("NLO"),"VT.QCD",plot_name+".NNLO.QT."+self.get_coupling_order("NLO")+".VT.QCD.dat")
                        else:
                            out.print_error("NLO_subtraction neither \"CS\" nor \"QT\" in collect_results. Exiting...")
                        NLO_file = pjoin(folder["distribution"],result_method,dir_name,plot_name+"..NLO.QCD.dat")
                        files_to_add = [loop_file,born_file,CA_file,RA_file,VA_file]
                        file_dest = pjoin(variation_dir,plot_name+"..NLO.prime.plus.loop.QCD.dat")
                        res.add_result_files(files_to_add,file_dest)
                        file_list.append(file_dest)
                    out_file = pjoin(MATRIX_distributions_folder,file_dest.rsplit("/",1)[1])
                    res.get_central_min_max_from_distribution_files(file_list,out_file)

            src_files = os.listdir(MATRIX_distributions_folder)
            for file_name in src_files:
                if "..NNLO." in file_name or "..NLO.prime" in file_name:
                    this_order = "NNLO"
                elif "..NLO." in file_name:
                    this_order = "NLO"
                elif "..LO." in file_name:
                    this_order = "LO"
                if "..NNLO."+self.get_coupling_order("NNLO")+".loop." in file_name:
                    add_string = "_only_loop-induced"
                elif "NLO.plus.loop.QCD.dat" in file_name:
                    add_string = "_NLO_plus_loop-induced"
                elif "NLO.prime.plus.loop.QCD.dat" in file_name:
                    add_string = "_NLO_prime_plus_loop-induced"
                else:
                    add_string = ""

                MATRIX_result_dist_folder = pjoin(process_dir,"result",run_folder,this_order+"-run","distributions"+add_string)
                try:
                    os.makedirs(MATRIX_result_dist_folder)
                except:
                    pass
                file_name_MATRIX = file_name.split("..")[0].replace("plot.","")+"__"+file_name.split("..")[1].replace("..",".").replace(".","_").replace("plot_","").replace("CV_","").replace("_dat",".dat").replace("NNLO_"+self.get_coupling_order("NNLO")+"_loop","loop-induced_QCD")
                file_dest = pjoin(MATRIX_result_dist_folder,file_name_MATRIX)
                file_src = pjoin(MATRIX_distributions_folder, file_name)
                if (os.path.isfile(file_src)):
                    shutil.copy(file_src,file_dest)
                    length = 15
                    observable = file_name_MATRIX.split(this_order)[0].strip("_")
                    if "NLO.prime.plus.loop.QCD.dat" in file_name:
                        observable = file_name_MATRIX.split("__")[0]
                    observable2 = "" # usually not two line are needed
                    if len(observable)>14:
                        observable2 = observable[13:]
                        observable = observable[:13]
                    central = "central"
                    minimum = "min (scale)"
                    maximum = "max (scale)"
                    num_err = "num_err"
                    line = "#"+" "*(length-len(observable)-1)+observable # 1 column title: observable
                    line = line+" "*(length-len(central))+central # 2: central cross section
                    line = line+" "*(length-len(num_err))+num_err # 3: num_err
                    line = line+" "*(length-len(minimum))+minimum # 4: minimal cross section due to scale variation
                    line = line+" "*(length-len(num_err))+num_err # 5: num_err
                    line = line+" "*(length-len(maximum))+maximum # 6: maximal cross section due to scale variation
                    line = line+" "*(length-len(num_err))+num_err # 7: num_err
                    if observable2:
                        line_prepender(file_dest,"# "+observable2)
                    line_prepender(file_dest,line)
#}}}
#{{{ def: print_results_onscreen_and_to_summary_file(self)
    def print_results_onscreen_and_to_summary_file(self):
        # screen output at end of run
        banner.size = 50
        banner.intend = 20
        intend = " "*16
        if parameter_list["coll_choice"]=="1":
            collider = "LHC"
        elif parameter_list["coll_choice"]=="2":
            collider = "Tevatron"
        else:
            out.print_error("collider_choice in parameter_list in routine collect_results is neither 1 nor 2.")
        energy = float(parameter_list["E"])*2/1000
        process = prc.get_nice_process_name()
        where = "@ %.4g TeV %s" % (energy,collider)

        # redirect output also to file in summary folder
        summary_folder = pjoin(process_dir,"result",run_folder,"summary")
        try:
            os.makedirs(summary_folder)
        except:
            pass
        f = open(pjoin(summary_folder,"result_summary.dat"), "w")
        original = sys.stdout
        sys.stdout = Tee(sys.stdout, f)

        print ""
        banner.initial_print()
        banner.print_center("Final result for:")
        banner.print_center(process+"  "+where)
        banner.final_print()
        nruns = len(self.order)
        print ""
#        print intend+"%s separate run(s) has been made" % nruns
        if(nruns > 1):
            out.print_result("%s separate runs were made" % nruns)
        else:
            out.print_result("%s separate run was made" % nruns)
        print ""
        for this_order in sorted(self.order):
            result_out = this_order+"-run"
            print intend+"#"+"-"*(len(result_out)+2)+"\\" 
            print intend+"# "+result_out+" |"
            print intend+"#"+"-"*(len(result_out)+2)+"/" 
            # out.print_result("#"+"-"*(len(result_out)+2)+"\\")
            # out.print_result("# "+result_out+" |")
            # out.print_result("#"+"-"*(len(result_out)+2)+"/")
            pdf_set = parameter_list["LHAPDF_%s" % this_order]
            # print intend+"PDF: "+pdf_set
            # print intend+"Total rate (possibly within cuts):"
            out.print_result("PDF: "+pdf_set)
            if int(parameter_list.get("switch_qT_accuracy","-99")) == 1:
                rcut_value = 0.0005
            else:
                rcut_value = 0.0015
            if this_order == "LO":
                out.print_result("Total rate (possibly within cuts):")
                out.print_result("----------------------------------------------------------------------")
            elif this_order == "NLO":
                if int(parameter_list["NLO_subtraction_method"]) == 2:  # using QT subtraction at NLO
                    out.print_result("Total rate (possibly within cuts):")
                    out.print_result("----------------------------------------------------------------------")
#                    out.print_result("Total rate (possibly within cuts, with r_cut=%s at NLO):" % (rcut_value))
                else:
                    out.print_result("Total rate (possibly within cuts):")
                    out.print_result("----------------------------------------------------------------------")
            elif this_order == "NNLO":
                if int(parameter_list["NLO_subtraction_method"]) == 2:  # using QT subtraction at NLO
                   out.print_result("Total rate (possibly within cuts):")
                   out.print_result("----------------------------------------------------------------------")
 #                    out.print_result("Total rate (possibly within cuts, with r_cut=%s at NLO/NNLO):" % (rcut_value))
                else:
                    out.print_result("Total rate (possibly within cuts):")
                    out.print_result("----------------------------------------------------------------------")
 #                   out.print_result("Total rate (possibly within cuts, with r_cut=%s at NNLO):" % (rcut_value))
            result_folder = pjoin(process_dir,"result",run_folder,this_order+"-run")
            try:
                os.makedirs(result_folder)
            except:
                pass

            
            # self.MATRIX_rates_folder = {}
            # for this_order in self.order:
            #     self.MATRIX_rates_folder[this_order] = pjoin(process_dir,run_folder,"result","result..MATRIX."+this_order+".result/CV/7-point/MATRIX_rates")


            src_files = os.listdir(self.MATRIX_rates_folder[this_order])
            sorted_list = sorted(src_files)
            extrapolated_list = []
            # put extrapolated results at right position
            copy_sorted_list = copy.copy(sorted_list)
            for item in copy_sorted_list:
                if "extrapolated" in item:
                    sorted_list.remove(item)
                    extrapolated_list.append(item)
            final_sorted_list = []
            #for item in sorted_list:
             #   final_sorted_list.append(item)
             #   for entry in extrapolated_list:
             #       if item.replace("plot.CV.","") == entry.replace("plot.CV.extrapolated.",""):
             #           final_sorted_list.append(entry)

            # loop through all files in ordered way
            for item in sorted_list:
                if ".LO.dat" in item:
                    final_sorted_list.append(item)
            for item in sorted_list:
                if (".NLO." in item) and ("loop" not in item):
                    final_sorted_list.append(item)
                    for entry in extrapolated_list:
                        if item.replace("plot.CV.","") == entry.replace("plot.CV.extrapolated.",""):
                            final_sorted_list.append(entry)
            for item in sorted_list:
                if (".NNLO." in item) and ("loop" not in item):
                    final_sorted_list.append(item)
                    for entry in extrapolated_list:
                        if item.replace("plot.CV.","") == entry.replace("plot.CV.extrapolated.",""):
                            final_sorted_list.append(entry)
            final_sorted_list.append(0)
            
            run_type = ""

	    if this_order == "NNLO":
                run_type = "qq"
            for file_name in final_sorted_list:
                if file_name == 0:
                    run_type = "gg"
                    out.print_result("----------------------------------------------------------------------")
                if file_name == 1:
                    run_type = ""
                    out.print_result("----------------------------------------------------------------------")
                if file_name == 2:
                    out.print_result("----------------------------------------------------------------------")
                if type(file_name)is not int:
                    file_src = pjoin(self.MATRIX_rates_folder[this_order], file_name)
                    central = self.cross_section[this_order][file_src]["central"]
                    err = self.cross_section[this_order][file_src]["err"]
                    up = self.cross_section[this_order][file_src]["up"]
                    down = self.cross_section[this_order][file_src]["down"]
                    file_name = "rate_"+file_src.rsplit('/', 1)[1]
                    size_central = len("0.0000001")
                    size_err     = len("0.00001")
                    if run_type == "qq" or run_type == "gg":
                        if(".LO." in file_name):
                            order_out = "LO     :    "
                        elif(".NLO." in file_name):
                            order_out = "NLO    :    "
                     #   elif(".loop." in file_name) and abs(int(parameter_list["loop_induced"])) > 0:
                      #      order_out = "LO gg  :    "
                        elif(".loopNLOgg." in file_name):
                            order_out = "NLO gg :    "
                        elif(".loopLOgg." in file_name):
                            order_out = "LO gg  :    "
                        elif(".NNLO." in file_name):
			    #order_out = "NNLO qq:    "
                            order_out = "NNLO   :    "
                    else:
                        if(".LO." in file_name):
                            order_out = "LO     :    "
                        elif(".NLO." in file_name):
                            order_out = "NLO    :    "
                        elif("NNLO_LOgg" in file_name):
                            order_out = "NNLO   :    "
                        elif("nNNLO" in file_name):
                            order_out = "nNNLO  :    "                       
                    central_string = "%#.4g" % central
                    err_string     = "%.2g" % err
                    central_spaces = " "*(size_central-len(central_string))
                    err_spaces     = " "*(size_err-len(err_string))
                    if central == 0 or abs((up/central-1)*100) >= 10:
                        up_space = ""
                    else:
                        up_space = " "
                    if central == 0 or abs((down/central-1)*100) >= 10:
                        down_space = ""
                    else:
                        down_space = " "
                            #                print intend+order_out+central_spaces+central_string+" fb +/- "+err_string+" fb"+err_spaces+" (muR, muF unc.: "+up_space+"+%.1f%% " % abs((up/central-1)*100)+down_space+"-%.1f%%)" % abs((down/central-1)*100)
                    if central != 0:
                        out.print_result(order_out+central_spaces+central_string+" fb +/- "+err_string+" fb"+err_spaces+" (muR, muF unc.: "+up_space+"+%.1f%% " % abs((up/central-1)*100)+down_space+"-%.1f%%)" % abs((down/central-1)*100))
                    else:
                        out.print_result(order_out+central_spaces+central_string+" fb +/- "+err_string+" fb"+err_spaces+" (muR, muF unc.: "+up_space+"+%.1f%% " % 0+down_space+"-%.1f%%)" % 0)
                    if ".NLO." in file_name and int(parameter_list["NLO_subtraction_method"]) == 2 and not ".extrapolated." in file_name and not ".loop." in file_name:
                        out.print_result("       (computed with finite qT-subtraction cut-off r_cut=%s)" % (rcut_value))
                    if ".NNLO." in file_name and not ".extrapolated." in file_name and not ".loop." in file_name:
                        out.print_result("       (computed with finite qT-subtraction cut-off r_cut=%s)" % (rcut_value))
                    if ".nNNLO." in file_name  and not ".extrapolated." in file_name and not ".loop." in file_name:
                        out.print_result("       (computed with finite qT-subtraction cut-off r_cut=%s)" % (rcut_value))
                    if "NNLO_LOgg" in file_name and not ".extrapolated." in file_name and not ".loop." in file_name:
                        out.print_result("       (computed with finite qT-subtraction cut-off r_cut=%s)" % (rcut_value))
                    if ".loopNLOgg." in file_name and int(parameter_list["NLO_subtraction_method"]) == 2 and not ".extrapolated." in file_name:
                        out.print_result("       (computed with finite qT-subtraction cut-off r_cut=%s)" % (rcut_value))
                                                   
                    if ".NLO." in file_name and int(parameter_list["NLO_subtraction_method"]) == 2 and ".extrapolated." in file_name and not ".loop." in file_name:
                        out.print_result("       (extrapolated to r_cut=0 -- final result with uncertainty)")
                    if ".NNLO." in file_name and ".extrapolated." in file_name and not ".loop." in file_name:
                        out.print_result("       (extrapolated to r_cut=0 -- final result with uncertainty)")
                    if ".nNNLO." in file_name and ".extrapolated." in file_name and not ".loop." in file_name:
                        out.print_result("       (extrapolated to r_cut=0 -- final result with uncertainty)")
                    if "NNLO_LOgg" in file_name  and ".extrapolated." in file_name and not ".loop." in file_name:
                        out.print_result("       (extrapolated to r_cut=0 -- final result with uncertainty)")
                    if ".loopNLOgg." in file_name and int(parameter_list["NLO_subtraction_method"]) == 2 and ".extrapolated." in file_name and not ".loop." in file_name:
                        out.print_result("       (extrapolated to r_cut=0 -- final result with uncertainty)")
        

# =======
#                 if ".NNLO." in file_name and not ".loop" in file_name:
#                     qT_dependence_file = pjoin(os.path.dirname(self.MATRIX_rates_folder["NNLO"]),"plot.qTcut.NNLO.QCD.dat")
#                     # use always only the lowerst values for the extrapolation
#                     relative_uncertainty = res.extrapolate_qT_dependence(qT_dependence_file,0.25)
# #                    if "a" in prc.process_name:
# #                    else:
# #                        relative_uncertainty = res.extrapolate_qT_dependence(qT_dependence_file)
#                     if abs(relative_uncertainty)*5 < abs(err/central)*100:
#                         out.print_result("r_cut-->0 indicates qT-subtraction uncertainty far below numerical one.")
#                     elif abs(relative_uncertainty) < 0.02:
#                         out.print_result("r_cut-->0 indicates qT-subtraction uncertainty of %.3f%% at NNLO." % relative_uncertainty)
#                     elif abs(relative_uncertainty) < 0.2:
#                         out.print_result("r_cut-->0 indicates qT-subtraction uncertainty of %.2f%% at NNLO." % relative_uncertainty)
#                     else:
#                         out.print_result("r_cut-->0 indicates qT-subtraction uncertainty of %.1f%% at NNLO." % relative_uncertainty)
#                     # extra statement if below numerical uncertainty
# >>>>>>> develop_EW
            print ""

        #use the original output
        sys.stdout = original
#}}}
#{{{ def: print_prerun(self)
    def print_pre_run(self):
        # combine collected results from pre-run/extrapolation and print first preliminary result

        pre_folder = self.folder_extrapolation
        this_order = self.top_order

        result_method = "CV"

        # for new 7/9-point varation create folder that was called "equal" before and move rate files in there; 
        # this way we can use the result collection of the ordinary "CV" variation
        if res.seven_point_variation(pjoin(pre_folder,result_method)): # 7-point variation
            new_folder = pjoin(pre_folder,result_method,"7-point")
        elif res.nine_point_variation(pjoin(pre_folder,result_method)): # 9-point variation
            new_folder = pjoin(pre_folder,result_method,"9-point")
        else:
            out.print_error("Trying whoot to collect results from a folder (path: %s) that contains neither 7-point nor 9-point variation." % pjoin(folder_path,result_method))
        try:
            os.makedirs(new_folder)
        except:
            pass
        files_and_folders_to_move = glob.iglob(pjoin(pre_folder,result_method,"*"))
        for file_and_folder in files_and_folders_to_move:
            if not "scale." in file_and_folder and not "equal" in file_and_folder and not "7-point" in file_and_folder and not "9-point" in file_and_folder:
                try:
                    shutil.move(file_and_folder,pjoin(new_folder,file_and_folder.rsplit('/',1)[1]))
                except:
                    pass
        # copy relevant results to MATRIX results folder
        self.pre_cross_section = multidim_dict(3) # multidimensonal dictionary
        self.MATRIX_rates_folder = {}
        files_to_copy = self.get_named_files(pre_folder,"."+result_method+".",2)
        files_to_copy = [f for f in files_to_copy if ("LO" in f.rsplit('/',1)[1] or "NLO" in f.rsplit('/',1)[1] or "NNLO" in f.rsplit('/',1)[1]) and not "extrapolated" in f.rsplit('/',1)[1]]
        if self.include_loop_induced and this_order == "NNLO": # add separate contribution for loop-induced process
            files_to_copy += self.get_named_files(pre_folder,result_method+".NNLO."+self.get_coupling_order("NNLO")+".dat",3)
            # these file do not contain the wanted information since LO has always LO PDFs
            # so copy the right files first into MATRIX_rates
        for file_src in files_to_copy: # some special treatment for loop-induced contribution
            if result_method+".NNLO."+self.get_coupling_order("NNLO")+".dat" in file_src:
                loop_induced = True
            else:
                loop_induced = False
            if not loop_induced:
                self.MATRIX_rates_folder[this_order] = pjoin(file_src.rsplit('/', 1)[0],"MATRIX_rates")
                try:
                    os.makedirs(self.MATRIX_rates_folder[this_order])
                except:
                    pass
            file_name = file_src.rsplit('/', 1)[1]

            if loop_induced:
                file_name = file_name.replace(".dat",".loop.dat")
            file_to_copy = file_src
            file_dest = pjoin(self.MATRIX_rates_folder[this_order],file_name)
            # change the files to be copied if PDF order does not correspond to this_order
            if ".LO." in file_name and not this_order == "LO":
                files_right_PDF_order = self.get_named_files(pre_folder,"."+result_method+"."+this_order+"."+self.get_coupling_order("LO")+".dat",3)
                for file_right_PDF_order in files_right_PDF_order:
                    file_to_copy = file_right_PDF_order
                shutil.copy(file_to_copy,file_dest)
            elif ".NLO." in file_name and not this_order == "NLO":
                all_files_right_PDF_order =  self.get_named_files(pre_folder,"."+result_method+"."+this_order+"."+self.get_coupling_order("LO")+".dat",3)
        
                all_files_right_PDF_order += self.get_named_files(pre_folder,"."+result_method+"."+this_order+"."+self.NLO_subtraction+"."+self.get_coupling_order("NLO")+".dat",3)
                files_right_PDF_order = [f for f in all_files_right_PDF_order if not f.endswith("~")] #remove the files that end with "~"
                # add up these two files and write it into MATRIX_rates folder
                res.add_result_files(files_right_PDF_order,file_dest)
            else:
                shutil.copy(file_to_copy,file_dest)
        src_files = os.listdir(self.MATRIX_rates_folder[this_order])
        # now copy all files from MATRIX_rates_folder into MATRIX result folder
        # renaming done here
        for file_name in src_files:
            file_name_MATRIX = "rate_"+file_name.replace(".","_").replace("plot_","").replace("CV_","").replace("_dat",".dat").replace("NNLO_"+self.get_coupling_order("NNLO")+"_loop","loop-induced_QCD").replace("_"+self.get_coupling_order("LO"),"").replace("_"+self.get_coupling_order("NLO"),"").replace("_"+self.get_coupling_order("NNLO"),"")
            file_src = pjoin(self.MATRIX_rates_folder[this_order], file_name)
            if (os.path.isfile(file_src)):
                # save cross section information
                central, err, up, down = inp.get_cross_sections_from_file(file_src)
                self.pre_cross_section[this_order][file_src]["central"] = central
                self.pre_cross_section[this_order][file_src]["err"]     = err
                self.pre_cross_section[this_order][file_src]["up"]      = up
                self.pre_cross_section[this_order][file_src]["down"]    = down
                if central == 0.:
                    out.print_warning("Combined cross section appears to be zero when trying to collect %s results." % this_order)


        banner.size = 50
        banner.intend = 20
        intend = " "*16
        if parameter_list["coll_choice"]=="1":
            collider = "LHC"
        elif parameter_list["coll_choice"]=="2":
            collider = "Tevatron"
        else:
            out.print_error("collider_choice in parameter_list in routine collect_results is neither 1 nor 2.")
        energy = float(parameter_list["E"])*2/1000
        process = prc.get_nice_process_name()
        where = "@ %.4g TeV %s" % (energy,collider)

        # redirect output also to file in summary folder
        summary_folder = pjoin(process_dir,"result",run_folder,"summary")
        try:
            os.makedirs(summary_folder)
        except:
            pass
        f = open(pjoin(summary_folder,"result_summary.dat"), "w")
        original = sys.stdout
        sys.stdout = Tee(sys.stdout, f)

        print ""
        banner.initial_print()
        banner.print_center("Preliminary (inaccurate) result for:")
        banner.print_center(process+"  "+where)
        banner.final_print()
        result_out = this_order+"-run"
        print intend+"#"+"-"*(len(result_out)+2)+"\\" 
        print intend+"# "+result_out+" |"
        print intend+"#"+"-"*(len(result_out)+2)+"/" 
        # out.print_result("#"+"-"*(len(result_out)+2)+"\\")
        # out.print_result("# "+result_out+" |")
        # out.print_result("#"+"-"*(len(result_out)+2)+"/")
        pdf_set = parameter_list["LHAPDF_%s" % this_order]
        # print intend+"PDF: "+pdf_set
        # print intend+"Total rate (possibly within cuts):"
        out.print_result("PDF: "+pdf_set)
        out.print_result("Total rate (possibly within cuts):")
        src_files = os.listdir(self.MATRIX_rates_folder[this_order])

        sort_list = [".LO.",".aLO.",".LO+aLO.",".NLO.QCD.",".NLO.EW.",".NLO.aEW.",".NLO.QCD+EW.",".NLO.QCDxEW.",".NLO.EW+aEW.",".NLO.QCD+EW+aEW.",".NNLO.QCD.", ".NNLO.EW.","NNLO.aEW","NNLO.EW+aEW","NNLO.QCD+EW","NNLO.QCDxEW","NNLO.QCD+EW+aEW",".loopNLOgg.",".loopLOgg."]

        output_list = {}            
        output_list[".LO."]              = "LO            :"
        output_list[".aLO."]             = "LO(a)         :"
        output_list[".LO+aLO."]          = "LO+LO(a)      :"
        output_list[".NLO.QCD."]         = "NLO(QCD)      :"
        output_list[".NLO.EW."]          = "NLO(EW)       :"
        output_list[".NLO.QCD+EW."]      = "NLO(QCD+EW)   :"
        output_list[".NLO.QCDxEW."]      = "NLO(QCDxEW)   :"
        output_list[".NLO.aEW."]         = "NLO(a)        :"
        output_list[".NLO.EW+aEW."]      = "NLO(EW+a)     :"
        output_list[".NLO.QCD+EW+aEW."]  = "NLO(QCD+EW+a) :"
        output_list[".NNLO.QCD."]        = "NNLO(QCD)     :"
        output_list[".NNLO.EW."]         = "NNLO(EW)      :"
        output_list[".NNLO.QCD+EW."]     = "NNLO(QCD+EW)  :"
        output_list[".NNLO.QCDxEW."]     = "NNLO(QCDxEW)  :"
        output_list[".NNLO.aEW."]        = "NNLO(a)       :"
        output_list[".NNLO.EW+aEW."]     = "NNLO(EW+a)    :"
        output_list[".NNLO.QCD+EW+aEW."] = "NNLO(QCD+EW+a):"
        output_list[".loopNLOgg."]       = "NLO gg        :"
        output_list[".loopLOgg."]        = "LO gg         :"
        sorted_src_files = []
        max_length_output = 0
        for part_of_file_name in sort_list:

            for file_name in src_files:
                if part_of_file_name in file_name:
                    sorted_src_files.append([file_name,output_list.get(part_of_file_name,"")])
                    max_length_output = max(max_length_output,len(output_list.get(part_of_file_name,"")))
        for [file_name,order_out] in sorted_src_files:
            file_src = pjoin(self.MATRIX_rates_folder[this_order], file_name)
            central = self.pre_cross_section[this_order][file_src]["central"]
            err = self.pre_cross_section[this_order][file_src]["err"]
            up = self.pre_cross_section[this_order][file_src]["up"]
            down = self.pre_cross_section[this_order][file_src]["down"]
            file_name = "rate_"+file_src.rsplit('/', 1)[1]
            size_central = len("0.0000001")
            size_err     = len("0.00001")
            # if(".LO." in file_name):
            #     order_out = "LO:  "
            # elif(".NLO." in file_name):
            #     order_out = "NLO: "
            # elif(".loop." in file_name):
            #     order_out = "loop:"
            # elif(".NNLO." in file_name):
            #     order_out = "NNLO:"
            central_string = "%#.4g" % central
            err_string     = "%.2g" % err
            central_spaces = " "*(size_central-len(central_string))
            err_spaces     = " "*(size_err-len(err_string))
            if central == 0 or abs((up/central-1)*100) >= 10:
                up_space = ""
            else:
                up_space = " "
            if central == 0 or abs((down/central-1)*100) >= 10:
                down_space = ""
            else:
                down_space = " "
                #                print intend+order_out+central_spaces+central_string+" fb +/- "+err_string+" fb"+err_spaces+" (muR, muF unc.: "+up_space+"+%.1f%% " % abs((up/central-1)*100)+down_space+"-%.1f%%)" % abs((down/central-1)*100)
            if central != 0:
                out.print_result(order_out+" " * (max_length_output-len(order_out))+central_spaces+central_string+" fb +/- "+err_string+" fb"+err_spaces+" (muR, muF unc.: "+up_space+"+%.1f%% " % abs((up/central-1)*100)+down_space+"-%.1f%%)" % abs((down/central-1)*100))
            else:
                out.print_result(order_out+" " * (max_length_output-len(order_out))+central_spaces+central_string+" fb +/- "+err_string+" fb"+err_spaces+" (muR, muF unc.: "+up_space+"+%.1f%% " % 0+down_space+"-%.1f%%)" % 0)
        out.print_result("This result is very inaccurate and only a rough estimate!")
        out.print_result("Wait until the main run finishes to get the final result!")
        print ""

        #use the original output
        sys.stdout = original
#}}}
#{{{ def: clear_pre_results(self)
    def clear_pre_results(self):
        # remove MUNICH extrapolation folder
        try:
            shutil.rmtree(self.folder_extrapolation)
        except:
            pass
#}}}
#{{{ def: clear_results(self)
    def clear_results(self):
        # remove MUNICH and MATRIX result folders

        # MUNICH folders:
        try:
            shutil.rmtree(self.folder_result)
        except:
            pass
        try:
            shutil.rmtree(self.folder_distribution)
        except:
            pass

        # MATRIX folder:
        # remove only the old *-run folders (if there are any)
        result_folder = pjoin(process_dir,"result",run_folder)
        result_sub_folder = glob.glob(pjoin(result_folder,"*-run"))
        # remove summary folder
        if os.path.isdir(pjoin(fold.result_folder_path,"summary")):
            result_sub_folder.append(pjoin(fold.result_folder_path,"summary"))
        if os.path.isdir(pjoin(fold.result_folder_path,"gnuplot")):
            result_sub_folder.append(pjoin(fold.result_folder_path,"gnuplot"))
        for folder in result_sub_folder:
            shutil.rmtree(folder)
        try:
            os.remove(pjoin(fold.result_folder_path,"CITATIONS.bib"))
        except:
            pass
        out.print_info("Cleaning previous results (result run)...")
#}}}
#{{{ def: combine_results(self)
    def combine_results(self): # collect results after the main run has finished
        out.print_info("Collecting and combining results...")
        # Create a list of jobs and then iterate through the number of processes appending each process to the job list 

        path = pjoin(fold.run_folder_path,"result","infile.result")
        files_in_path = [ f for f in os.listdir(path) if f.endswith(".dat") and os.path.isfile(pjoin(path,f)) ]
        for file_name in files_in_path:
            inp.add_directories_to_result_files(pjoin(path,file_name),self.runtime_table,1)
        job_list = []

        # 2do: add some sanity checks to make sure all data is there in the run.xx folders
        inp.write_infile_result_MATRIX(self.infile_result,self.order,parameter_list,self.NLO_subtraction,self.get_coupling_order("LO"),self.get_coupling_order("NLO") \
                                ,self.get_coupling_order("LO","NLO"),self.get_coupling_order("NNLO"),self.get_coupling_order("NNNLO"),self.include_loop_induced)
        job_list.append([os.path.basename(self.infile_result),"result"])

        if int(parameter_list["switch_distribution"]) == 1:
            inp.write_infile_result_MATRIX(self.infile_distribution,self.order,parameter_list,self.NLO_subtraction,self.get_coupling_order("LO"),self.get_coupling_order("NLO") \
                                           ,self.get_coupling_order("LO","NLO"),self.get_coupling_order("NNLO"),self.get_coupling_order("NNNLO"),self.include_loop_induced)
            job_list.append([os.path.basename(self.infile_distribution),"distribution"])

        # use local mode, but save original runmode
        runmode_sav = self.runmode
        run.runmode = "multicore" # the result combination is always done locally
        # run the jobs from the list (only if there are jobs in job_list)
        if job_list: self.submit_jobs(job_list)
        self.runmode = runmode_sav # change back to original runmode

        self.copy_rates("CV")
        if int(parameter_list["switch_distribution"]) == 1: 
            self.copy_distributions("CV")
#}}}
#{{{ def: copy_rates(self,folder_distribution,result_method)
    def copy_rates(self,result_method): # copy results needed for MATRIX from MUNICH output
        # for new 7/9-point varation create folder that was called "equal" before and move rate files in there; 
        # this way we can use the result collection of the ordinary "CV" variation
        folder = {}
        folder["NNLO"] = self.folder_result  #  ADAPT THE WHOLE ROUTINE
        folder["NLO"] = self.folder_result  #  ADAPT THE WHOLE ROUTINE
        folder["LO"] = self.folder_result  #  ADAPT THE WHOLE ROUTINE
        # for order,folder_path in folder.iteritems():
        #     if order == "distribution":
        #         continue
        #     if res.seven_point_variation(pjoin(folder_path,result_method)): # 7-point variation
        #         new_folder = pjoin(folder_path,result_method,"7-point")
        #     elif res.nine_point_variation(pjoin(folder_path,result_method)): # 9-point variation
        #         new_folder = pjoin(folder_path,result_method,"9-point")
        #     else:
        #         out.print_error("Trying whoot to collect results from a folder (path: %s) that contains neither 7-point nor 9-point variation." % pjoin(folder_path,result_method))
        #     try:
        #         os.makedirs(new_folder)
        #     except:
        #         pass
        #     files_and_folders_to_move = glob.iglob(pjoin(folder_path,result_method,"*"))
        #     for file_and_folder in files_and_folders_to_move:
        #         if not "scale." in file_and_folder and not "equal" in file_and_folder and not "7-point" in file_and_folder and not "9-point" in file_and_folder:
        #             shutil.move(file_and_folder,pjoin(new_folder,file_and_folder.rsplit('/',1)[1]))

        # copy relevant results to MATRIX results folder
        self.cross_section = multidim_dict(3) # multidimensonal dictionary
        self.MATRIX_rates_folder = {}
        for this_order in self.order:
            #if int(parameter_list.get("loop_induced")) <= 0:
            result_folder = pjoin(process_dir,"result",run_folder,this_order+"-run")
            try:
                os.makedirs(result_folder)
            except:
                pass
            if this_order == "NNLO":
                result_folder_qq = pjoin(process_dir,"result",run_folder,this_order+"-run" , "qq-rates")
                try:
                    os.makedirs(result_folder_qq)
                except:
                    pass
                result_folder_gg = pjoin(process_dir,"result",run_folder,this_order+"-run" , "gg-rates")
                try:
                    os.makedirs(result_folder_gg)
                except:
                    pass
           
            files_to_copy = self.get_named_files(folder[this_order],"."+result_method+".",2)
            files_to_copy = [f for f in files_to_copy if "LO" in f.rsplit('/',1)[1] or "NLO" in f.rsplit('/',1)[1] or "NNLO" in f.rsplit('/',1)[1]]
            files_to_copy = [f for f in files_to_copy if not ("extrapolated" in f.rsplit('/',1)[1] and (".LO." in f.rsplit('/',1)[1] or ".loopLOgg." in f.rsplit('/',1)[1] or (".NLO." in f.rsplit('/',1)[1] and self.NLO_subtraction == "CS") or (".loopNLOgg." in f.rsplit('/',1)[1] and self.NLO_subtraction == "CS")))]
            if not this_order == self.top_order:
                if this_order == "LO":
                    files_to_copy = [f for f in files_to_copy if (not "NLO" in f.rsplit('/',1)[1] and not "NNLO" in f.rsplit('/',1)[1])]
                if this_order == "NLO":
                    files_to_copy = [f for f in files_to_copy if not "NNLO" in f.rsplit('/',1)[1]]
            if int(parameter_list["loop_induced"]) != -1 and this_order == "NNLO": # add separate contribution for loop-induced process
                files_to_copy += self.get_named_files(folder[this_order],result_method+".NNLO."+self.get_coupling_order("NNLO")+".dat",3)
            # these file do not contain the wanted information since LO has always LO PDFs
            # so copy the right files first into MATRIX_rates
            for file_src in files_to_copy: # some special treatment for loop-induced contribution
                file_src_cutvar = file_src.replace("plot.CV","plot.qTcut")
#                print("file_src_cutvar="+file_src_cutvar)
#                if self.cross_section[this_order][file_src]["central"] != 0:
#                     if 'plot.qTcut.NNLO.QCD.dat' in file_src_cutvar:
#                         qTextr,qTerr = inp.get_qT_extrapolation_error_from_file(file_src_cutvar,2)
# #                        print("extrapolated cross section is "+np.str(qTextr)+" +/- "+np.str(qTerr))
#                     elif 'plot.qTcut.NLO.QCD.dat' in file_src_cutvar:
#                         qTextr,qTerr = inp.get_qT_extrapolation_error_from_file(file_src_cutvar,1)
# #                        print("extrapolated cross section is "+np.str(qTextr)+" +/- "+np.str(qTerr))


                if result_method+".NNLO."+self.get_coupling_order("NNLO")+".dat" in file_src:
                    loop_induced = True
                   
                else:
                    loop_induced = False
                    self.MATRIX_rates_folder[this_order] = pjoin(file_src.rsplit('/', 1)[0],"MATRIX_rates_"+this_order)
                
                    try:
                        os.makedirs(self.MATRIX_rates_folder[this_order])
                    except:
                        pass 
                file_name = file_src.rsplit('/', 1)[1]
                if loop_induced:
                    file_name = file_name.replace(".dat",".loop.dat")
                file_to_copy = file_src
                file_dest = pjoin(self.MATRIX_rates_folder[this_order],file_name)
                # change the files to be copied if PDF order does not correspond to this_order
                if ".LO." in file_name and not this_order == "LO":
                    files_right_PDF_order = self.get_named_files(folder[this_order],"."+result_method+"."+this_order+"."+self.get_coupling_order("LO")+".dat",3)
                    for file_right_PDF_order in files_right_PDF_order:
                        file_to_copy = file_right_PDF_order
                    shutil.copy(file_to_copy,file_dest)
                elif ".NLO." in file_name and not this_order == "NLO":
                    all_files_right_PDF_order =  self.get_named_files(folder[this_order],"."+result_method+"."+this_order+"."+self.get_coupling_order("LO")+".dat",3)
                    all_files_right_PDF_order += self.get_named_files(folder[this_order],"."+result_method+"."+this_order+"."+self.NLO_subtraction+"."+self.get_coupling_order("NLO")+".dat",3)
                    files_right_PDF_order = [f for f in all_files_right_PDF_order if not f.endswith("~")] #remove the files that end with "~"
                    # add up these two files and write it into MATRIX_rates folder
                    res.add_result_files(files_right_PDF_order,file_dest)
                else:
                    shutil.copy(file_to_copy,file_dest)

            src_files = os.listdir(self.MATRIX_rates_folder[this_order])
            # now copy all files from MATRIX_rates_folder into MATRIX result folder
            # renaming done here
    
            for file_name in src_files:

                MATRIX_result_folder = pjoin(process_dir,"result",run_folder,this_order+"-run")
                file_name_MATRIX = "rate_"+file_name.replace(".","_").replace("plot_","").replace("CV_","").replace("_dat",".dat").replace("NNLO_"+self.get_coupling_order("NNLO")+"_loop","loop-induced_QCD").replace("_"+self.get_coupling_order("LO"),"").replace("_"+self.get_coupling_order("NLO"),"").replace("_"+self.get_coupling_order("NNLO"),"")
                file_dest = ""
                if this_order == "LO"  and ("loopLOgg" not in file_name_MATRIX) :
                    file_dest = pjoin(MATRIX_result_folder,file_name_MATRIX)
                elif (this_order == "NLO" and ("loopLOgg" not in file_name_MATRIX)) and ("loopNLOgg" not in file_name_MATRIX):
                    if "_LO" not in file_name_MATRIX:
                        file_dest = pjoin(MATRIX_result_folder,file_name_MATRIX)
                elif this_order == "NNLO":
                    if ("_nNNLO_QCD" in  file_name_MATRIX) or  ("_NNLO_LOgg_QCD" in file_name_MATRIX):
                        file_dest = pjoin(MATRIX_result_folder,file_name_MATRIX)
                    elif ("_NNLO_QCD" in file_name_MATRIX):
                        file_dest = pjoin(MATRIX_result_folder,"qq-rates",file_name_MATRIX)
                    elif ("loop" in file_name_MATRIX) and ("loop-induced_QCD" not in file_name_MATRIX):
                        file_dest = pjoin(MATRIX_result_folder,"gg-rates",file_name_MATRIX)
                    
                file_src = pjoin(self.MATRIX_rates_folder[this_order], file_name)
                if (os.path.isfile(file_src)):
                    # save cross section information
                    central, err, up, down = inp.get_cross_sections_from_file(file_src)
                    self.cross_section[this_order][file_src]["central"] = central
                    self.cross_section[this_order][file_src]["err"]     = err
                    self.cross_section[this_order][file_src]["up"]      = up
                    self.cross_section[this_order][file_src]["down"]    = down
                    if central == 0.:
#                        out.print_error("Combined cross section appears to be zero when trying to collect %s results. Exiting..." % this_order)
                        out.print_warning("Combined cross section appears to be zero when trying to collect %s results." % this_order)
                    # change the first column into two columns with muR and muF
                    res.convert_to_independent_scales(file_src)
                    if file_dest:
                        shutil.copy(file_src,file_dest)      
                        line_prepender(file_dest,"#      muR       muF  cross_section        num_err")
                    
#}}}
#{{{ def: copy_distributions(self,folder_distribution,result_method)
    def copy_distributions(self,result_method): # copy distribution needed for MATRIX from MUNICH output
        files = {}
        if res.seven_point_variation_distribution(pjoin(self.folder_distribution,result_method)): # 7-point variation
            central_dir = pjoin(self.folder_distribution,result_method,"scale.3.3")
        elif res.nine_point_variation_distribution(pjoin(self.folder_distribution,result_method)): # 9-point variation
            central_dir = pjoin(self.folder_distribution,result_method,"scale.4.4")
        else:
            out.print_error("Trying to combine distributions from a folder that contains neither 7-point nor 9-point variation.")
        MATRIX_distributions_folder = pjoin(central_dir,"MATRIX_distributions")
        try:
            os.makedirs(MATRIX_distributions_folder)
        except:
            pass
        variation_dirs = [x for x in glob.glob(pjoin(self.folder_distribution,result_method,"scale*.*")) if not x == central_dir] # variation

        files["all"]        = glob.glob(pjoin(central_dir,"*.dat"))
        files["LO.NLO-run"] = [x for x in glob.glob(pjoin(central_dir,"NLO."+self.get_coupling_order("LO"),"*.dat")) if not os.path.basename(x).startswith("overview")]
#        files["NLO.EW"] = [x for x in all_files if "..NLO.EW." in x]
#        for contribution in files:

        # compute minimum und maximum of all distributions and save to MATRIX subfolder
        for file_path in files["all"]:
            file_list = [file_path]
            for variation_dir in variation_dirs:
                file_list.append(file_path.replace(central_dir,variation_dir))
            if "overview" not in file_path:
                res.get_central_min_max_from_distribution_files(file_list,pjoin(MATRIX_distributions_folder,os.path.basename(file_path)))

        for file_path in files["LO.NLO-run"]:
            file_list = [file_path]
            for variation_dir in variation_dirs:
                file_list.append(file_path.replace(central_dir,variation_dir))
            if "overview" not in file_path:
                res.get_central_min_max_from_distribution_files(file_list,pjoin(MATRIX_distributions_folder,os.path.basename(file_path).replace(".NLO."+self.get_coupling_order("LO")+".dat", "..NLO."+self.get_coupling_order("LO")+".dat")))


#{{{ simplify the following
        # have distributions of loop-induced seperately in run-NNLO folder
        if int(parameter_list["loop_induced"]) > 0 and "NNLO" in self.order:
            files["loop"] = glob.iglob(pjoin(central_dir,"NNLO."+self.get_coupling_order("NNLO"),"loop","*.loop.dat"))
            for loop_file in files["loop"]:
                file_name = os.path.basename(loop_file)
                file_list = []
                file_list.append(loop_file)
                for variation_dir in variation_dirs:
                    full_name = pjoin(variation_dir,"NNLO."+self.get_coupling_order("NNLO"),"loop",file_name)
                    file_list.append(full_name)
                out_file = pjoin(MATRIX_distributions_folder,file_name.replace(".NNLO","..NNLO"))
                if "overview" not in out_file:
                    res.get_central_min_max_from_distribution_files(file_list,out_file)
        
        # have distributions of NLO + loop-induced separately in run-NLO folder
        if int(parameter_list["loop_induced"]) > 0 and NLO_plus_loop_induced and "NNLO" in self.order and "NLO" in self.order:
            files["loop"] = glob.iglob(pjoin(central_dir,"NNLO."+self.get_coupling_order("NNLO"),"loop","*.loop.dat"))
            for loop_file in files["loop"]:
                plot_name_full = os.path.basename(loop_file)
                plot_name_suffix = ".NNLO."+self.get_coupling_order("NNLO")+".loop.dat"
                if plot_name_full.endswith(plot_name_suffix):
                    plot_name = plot_name_full[0:-len(plot_name_suffix)]
                else:
                    assert(False)
                NLO_file = pjoin(central_dir,plot_name+"..NLO.QCD.dat")
                files_to_add = [loop_file,NLO_file]
                file_dest = NLO_file.replace("..NLO.QCD.dat","..NLO.plus.loop.QCD.dat")
                if "overview" not in file_dest:
                    res.add_result_files(files_to_add,file_dest)
                file_list = []
                file_list.append(file_dest)
                for variation_dir in variation_dirs:
                    NLO_file = pjoin(variation_dir,plot_name+"..NLO.QCD.dat")
                    files_to_add = [loop_file,NLO_file]
                    file_dest = NLO_file.replace("..NLO.QCD.dat","..NLO.plus.loop.QCD.dat")
                    if "overview" not in file_dest:
                        res.add_result_files(files_to_add,file_dest)
                    file_list.append(file_dest)
                out_file = pjoin(MATRIX_distributions_folder,os.path.basename(file_dest))
                if "overview" not in out_file:
                    res.get_central_min_max_from_distribution_files(file_list,out_file)

        # have distributions of NLO (with NNLO PDFs) + loop-induced separately in run-NNLO folder
        if int(parameter_list["loop_induced"]) > 0 and NLO_plus_loop_induced and "NNLO" in self.order:
            files["loop"] = glob.iglob(pjoin(central_dir,"NNLO."+self.get_coupling_order("NNLO"),"loop","*.loop.dat"))
            for loop_file in files["loop"]:
                plot_name_full = os.path.basename(loop_file)
                plot_name_suffix = ".NNLO."+self.get_coupling_order("NNLO")+".loop.dat"
                if plot_name_full.endswith(plot_name_suffix):
                    plot_name = plot_name_full[0:-len(plot_name_suffix)]
                else:
                    assert(False)
                born_file = pjoin(central_dir,"NNLO."+self.get_coupling_order("LO"),"born",plot_name+".NNLO."+self.get_coupling_order("LO")+".born.dat")
                if self.NLO_subtraction == "CS":
                    CA_file = pjoin(central_dir,"NNLO.CS."+self.get_coupling_order("NLO"),"CA.QCD",plot_name+".NNLO.CS."+self.get_coupling_order("NLO")+".CA.QCD.dat")
                    RA_file = pjoin(central_dir,"NNLO.CS."+self.get_coupling_order("NLO"),"RA.QCD",plot_name+".NNLO.CS."+self.get_coupling_order("NLO")+".RA.QCD.dat")
                    VA_file = pjoin(central_dir,"NNLO.CS."+self.get_coupling_order("NLO"),"VA.QCD",plot_name+".NNLO.CS."+self.get_coupling_order("NLO")+".VA.QCD.dat")
                elif self.NLO_subtraction == "QT":
                    CA_file = pjoin(central_dir,"NNLO.QT."+self.get_coupling_order("NLO"),"CT.QCD",plot_name+".NNLO.QT."+self.get_coupling_order("NLO")+".CT.QCD.dat")
                    RA_file = pjoin(central_dir,"NNLO.QT."+self.get_coupling_order("NLO"),"RT.QCD",plot_name+".NNLO.QT."+self.get_coupling_order("NLO")+".RT.QCD.dat")
                    VA_file = pjoin(central_dir,"NNLO.QT."+self.get_coupling_order("NLO"),"VT.QCD",plot_name+".NNLO.QT."+self.get_coupling_order("NLO")+".VT.QCD.dat")
                else:
                    out.print_error("NLO_subtraction neither \"CS\" nor \"QT\" in collect_results_py. Exiting...")
                files_to_add = [loop_file,born_file,CA_file,RA_file,VA_file]
                file_dest = pjoin(central_dir,plot_name+"..NLO.prime.plus.loop.QCD.dat")
                if "overview" not in file_dest:
                    res.add_result_files(files_to_add,file_dest)
                file_list = []
                file_list.append(file_dest)
                for variation_dir in variation_dirs:
                    loop_file = pjoin(variation_dir,"NNLO."+self.get_coupling_order("NNLO"),"loop",plot_name+".NNLO."+self.get_coupling_order("NNLO")+".loop.dat")
                    born_file = pjoin(variation_dir,"NNLO."+self.get_coupling_order("LO"),"born",plot_name+".NNLO."+self.get_coupling_order("LO")+".born.dat")
                    if self.NLO_subtraction == "CS":
                        CA_file = pjoin(variation_dir,"NNLO.CS."+self.get_coupling_order("NLO"),"CA.QCD",plot_name+".NNLO.CS."+self.get_coupling_order("NLO")+".CA.QCD.dat")
                        RA_file = pjoin(variation_dir,"NNLO.CS."+self.get_coupling_order("NLO"),"RA.QCD",plot_name+".NNLO.CS."+self.get_coupling_order("NLO")+".RA.QCD.dat")
                        VA_file = pjoin(variation_dir,"NNLO.CS."+self.get_coupling_order("NLO"),"VA.QCD",plot_name+".NNLO.CS."+self.get_coupling_order("NLO")+".VA.QCD.dat")
                    elif self.NLO_subtraction == "QT":
                        CA_file = pjoin(variation_dir,"NNLO.QT."+self.get_coupling_order("NLO"),"CT.QCD",plot_name+".NNLO.QT."+self.get_coupling_order("NLO")+".CT.QCD.dat")
                        RA_file = pjoin(variation_dir,"NNLO.QT."+self.get_coupling_order("NLO"),"RT.QCD",plot_name+".NNLO.QT."+self.get_coupling_order("NLO")+".RT.QCD.dat")
                        VA_file = pjoin(variation_dir,"NNLO.QT."+self.get_coupling_order("NLO"),"VT.QCD",plot_name+".NNLO.QT."+self.get_coupling_order("NLO")+".VT.QCD.dat")
                    else:
                        out.print_error("NLO_subtraction neither \"CS\" nor \"QT\" in collect_results_py. Exiting...")
                    NLO_file = pjoin(variation_dir,plot_name+"..NLO.QCD.dat")
                    files_to_add = [loop_file,born_file,CA_file,RA_file,VA_file]
                    file_dest = pjoin(variation_dir,plot_name+"..NLO.prime.plus.loop.QCD.dat")
                    if "overview" not in file_dest:
                        res.add_result_files(files_to_add,file_dest)
                    file_list.append(file_dest)
                out_file = pjoin(MATRIX_distributions_folder,os.path.basename(file_dest))
                if "overview" not in out_file:
                    res.get_central_min_max_from_distribution_files(file_list,out_file)
#}}}

        match_distribution_to_order_and_identifier = {}
        match_distribution_to_order_and_identifier["NLO.EW"] = ["NLO-run","NLO_EW"]
        match_distribution_to_order_and_identifier["NLO."+self.get_coupling_order("LO")] = ["NLO-run","LO"]
        match_distribution_to_order_and_identifier["NLO.prime.plus.loop.QCD"] = ["NNLO-run","NLO_prime_plus_loop_QCD"]
        match_distribution_to_order_and_identifier["NLO.plus.loop.QCD"] = ["NNLO-run", "NLO_plus_loop_QCD"] 
        match_distribution_to_order_and_identifier["NNLO."+self.get_coupling_order("NNLO")+".loop"] = ["NNLO-run","loop_QCD"]
        match_distribution_to_order_and_identifier["LO"] = ["LO-run","LO"]
        match_distribution_to_order_and_identifier["NLO.QCD"] = ["NLO-run","NLO_QCD"]
        match_distribution_to_order_and_identifier["NNLO.QCD"] = ["NNLO-run","NNLO_QCD"]
        match_distribution_to_order_and_identifier["loopNLOgg.QCD"] = ["NNLO-run","loopNLOgg_QCD"]
        match_distribution_to_order_and_identifier["loopLOgg.QCD"] = ["NNLO-run","loopLOgg_QCD"]

        # 2do for NNLO:
        #     if "..NNLO." in file_name or "..NLO.prime" in file_name:
        #         this_order = "NNLO"
        #     elif "..NLO." in file_name:
        #         this_order = "NLO"
        #     elif "..LO." in file_name:
        #         this_order = "LO"
        #     if "..NNLO."+self.get_coupling_order("NNLO")+".loop." in file_name:
        #         add_string = "_only_loop-induced"
        #     elif "NLO.plus.loop.QCD.dat" in file_name:
        #         add_string = "_NLO_plus_loop-induced"
        #     elif "NLO.prime.plus.loop.QCD.dat" in file_name:
        #         add_string = "_NLO_prime_plus_loop-induced"
        #     else:
        #         add_string = ""


        # copy distributions with modified names to MATRIX result folders
        for file_name in os.listdir(MATRIX_distributions_folder):
            
            file_key = file_name.split("..")[1].replace(".dat","")
            if file_key in match_distribution_to_order_and_identifier:
                order_run  = match_distribution_to_order_and_identifier[file_key][0]
                identifier = match_distribution_to_order_and_identifier[file_key][1]
            elif "nNNLO" in file_key:
                order_run  = "NNLO-run"
                identifier = "nNNLO_QCD"
            elif "NNLO_LOgg" in file_key:
                order_run  = "NNLO-run"
                identifier = "NNLO_LOgg_QCD"                
            else:
                out.print_error("No information how to handle file key %s when copying distributions into MATRIX result folder. Exiting..." % file_key)
         #   if (order_run != "NLO-run" or identifier != "LO") and (identifier != "loop_QCD"):
            MATRIX_result_dist_folder = pjoin(process_dir,"result",run_folder,order_run,"distributions__"+identifier)
            try:
                os.makedirs(MATRIX_result_dist_folder)
            except:
                pass
              #  MATRIX_result_dist_folder = ""
            file_name_MATRIX = file_name.split("..")[0].replace("plot.","")+"__"+identifier+".dat"
#            file_name_MATRIX = file_name.split("..")[0].replace("plot.","")+"__"+file_name.split("..")[1].replace("..",".").replace(".","_").replace("plot_","").replace("CV_","").replace("_dat",".dat").replace("NNLO_"+self.get_coupling_order("NNLO")+"_loop","loop-induced_QCD")
            if MATRIX_result_dist_folder:
                file_dest = pjoin(MATRIX_result_dist_folder,file_name_MATRIX)
                file_src  = pjoin(MATRIX_distributions_folder,file_name)
            if (os.path.isfile(file_src) and (order_run != "NLO-run" or identifier != "LO")) and (file_dest):
                shutil.copy(file_src,file_dest)
                length = 15
                observable = file_name_MATRIX.split("__")[0]
                observable2 = "" # usually not two line are needed
                if len(observable)>14:
                    observable2 = observable[13:]
                    observable = observable[:13]
                central = "central"
                minimum = "min (scale)"
                maximum = "max (scale)"
                num_err = "num_err"
                line = "#"+" "*(length-len(observable)-1)+observable # 1 column title: observable
                line = line+" "*(length-len(central))+central # 2: central cross section
                line = line+" "*(length-len(num_err))+num_err # 3: num_err
                line = line+" "*(length-len(minimum))+minimum # 4: minimal cross section due to scale variation
                line = line+" "*(length-len(num_err))+num_err # 5: num_err
                line = line+" "*(length-len(maximum))+maximum # 6: maximal cross section due to scale variation
                line = line+" "*(length-len(num_err))+num_err # 7: num_err
                if observable2:
                    line_prepender(file_dest,"# "+observable2)
                line_prepender(file_dest,line)
#}}}
#{{{ def: extrapolate_runtimes(self)
    def extrapolate_runtimes(self): # extrapolate results after the pre run has finished
        out.print_info("Extrapolating runtimes...")

        path = pjoin(fold.run_folder_path,"result","infile.result")
        files_in_path = [ f for f in os.listdir(path) if f.endswith(".dat") and os.path.isfile(pjoin(path,f)) ]
        for file_name in files_in_path:
            inp.add_directories_to_result_files(pjoin(path,file_name),self.runtime_table,-1)

        # this is done by a single results job
        inp.write_infile_result_MATRIX(self.infile_extrapolation,self.order,parameter_list,self.NLO_subtraction,self.get_coupling_order("LO"),self.get_coupling_order("NLO") \
                                ,self.get_coupling_order("LO","NLO"),self.get_coupling_order("NNLO"),self.get_coupling_order("NNNLO"),self.include_loop_induced)
        job_list = []
# REMOVE if runs work !!!
# <<<<<<< HEAD 
#         if "NNLO" in self.order:
#             infile_combination_NNLO = pjoin(fold.run_folder_path,"result","infile.result.NNLO.QT-CS.NLO."+self.NLO_subtraction+".LO.dat")
#             inp.write_infile_result(["LO","NLO","NNLO"],infile_combination_NNLO,self.NLO_subtraction,self.get_coupling_order("LO"),self.get_coupling_order("NLO"),self.get_coupling_order("NNLO"),self.get_coupling_order("NNNLO"),self.include_loop_induced)
#             inp.input_set_entry(infile_combination_NNLO,"final_resultdirectory","extrapolation.NNLO.QT-CS.NLO."+self.NLO_subtraction+".LO")
#             job_list.append(["infile.result.NNLO.QT-CS.NLO."+self.NLO_subtraction+".LO.dat","result"])
#         elif "NLO" in self.order:
#             infile_combination_NLO = pjoin(fold.run_folder_path,"result","infile.result.NLO."+self.NLO_subtraction+".LO.dat")
#             inp.write_infile_result(["LO","NLO"],infile_combination_NLO,self.NLO_subtraction,self.get_coupling_order("LO"),self.get_coupling_order("NLO"),self.get_coupling_order("NNLO"),self.get_coupling_order("NNNLO"),self.include_loop_induced)
#             inp.input_set_entry(infile_combination_NLO,"final_resultdirectory","extrapolation.NLO."+self.NLO_subtraction+".LO")
#             job_list.append(["infile.result.NLO."+self.NLO_subtraction+".LO.dat","result"])
#         elif "LO" in self.order:
#             infile_combination_LO = pjoin(fold.run_folder_path,"result","infile.result.LO.dat")
#             inp.write_infile_result(["LO"],infile_combination_LO,self.NLO_subtraction,self.get_coupling_order("LO"),self.get_coupling_order("NLO"),self.get_coupling_order("NNLO"),self.get_coupling_order("NNNLO"),self.include_loop_induced)
#             inp.input_set_entry(infile_combination_LO,"final_resultdirectory","extrapolation.LO")
#             job_list.append(["infile.result.LO.dat","result"])
#         # 2do: add some sanity checks to make sure all data is there
# =======
# REMOVE if runs work !!!
        job_list.append([os.path.basename(self.infile_extrapolation),"result"])

        runmode_sav = self.runmode
        self.runmode = "multicore" # the extrapolation is always done locally
        if job_list: self.submit_jobs(job_list)
        self.runmode = runmode_sav # change back to original runmode

        runtime_file_path = pjoin(fold.run_folder_path,"result","runtime.dat")
        shutil.copyfile(runtime_file_path, runtime_file_path.replace(".dat","")+"_pre_run.dat")
#}}}
#{{{ def: submit_jobs(self,job_list)
    def submit_jobs(self,job_list):
        if self.runmode == "multicore":
            self.submit_jobs_local(job_list)
        elif self.runmode == "cluster":
#            self.submit_jobs_local(job_list)
            self.submit_jobs_cluster(job_list)
#}}}
#{{{ def: submit_jobs_local(self,job_list)
    def submit_jobs_local(self,job_list):
        # loop over jobs and add to process queue
        jobs = []
        prc_names = []

        for job in job_list:
            prc = multiprocessing.Process(target=self.job_process, args=(job,))
            jobs.append(prc)
            prc_names.append(prc.name)
        max_jobs = len(jobs)
        queued_jobs = max_jobs
        missing_jobs = max_jobs
        current_time  = str(datetime.datetime.now()).split('.')[0]
        # prints initial phase of jobs (all jobs queued)
        out.print_jobs("| %s | Queued: %s | Running: 0 | Finished: 0 |" % 
                       (current_time, queued_jobs))
        # Start the processes
        for prc in jobs:
            time_before = time.time()
            firsttime = True
            while (len(multiprocessing.active_children()) >= int(nr_cores) and int(nr_cores) != -1) or (self.runmode == "cluster" and cluster.get_jobs_in_cluster_queue() > int(config_list["max_jobs_in_cluster_queue"]) and int(config_list["max_jobs_in_cluster_queue"]) !=-1):
                time.sleep(1)
                time_now = time.time()
                # prints status every 5 minutes to show that script is alive
                # (important also for cluster mode, where not all jobs can always be started at once)
                if time_now - time_before >= parameter_list["print_out_interval"] or firsttime:
                    firsttime = False
                    if self.runmode == "cluster" and int(nr_cores) != -1 or (self.runmode == "cluster" and cluster.get_jobs_in_cluster_queue() > int(config_list["max_jobs_in_cluster_queue"]) and int(config_list["max_jobs_in_cluster_queue"]) !=-1):
                        queued_jobs   = missing_jobs+cluster.get_nr_of_cluster_jobs("pending")
                        running_jobs  = cluster.get_nr_of_cluster_jobs("running")
                        finished_jobs = max_jobs - queued_jobs - running_jobs
                        # print information why in this loop
                        if cluster.get_jobs_in_cluster_queue() > int(config_list["max_jobs_in_cluster_queue"]) and int(config_list["max_jobs_in_cluster_queue"]) !=-1:
                            out.print_info("Cluster queue full (%s jobs > max_jobs_in_cluster_queue=%s in MATRIX_configuration). Waiting for other jobs to finish..." % (cluster.get_jobs_in_cluster_queue(),config_list["max_jobs_in_cluster_queue"]))
                        elif len(multiprocessing.active_children()) >= nr_cores and int(nr_cores) != -1:
                            out.print_info("Submitted max_nr_parallel_jobs=%s (from MATRIX_configuration) jobs to cluster queue. Waiting for own jobs to finish..." % nr_cores)
                    current_time  = str(datetime.datetime.now()).split('.')[0]
                    out.print_jobs("| %s | Queued: %s | Running: %s | Finished: %s |" % 
                                   (current_time, queued_jobs, running_jobs, finished_jobs))
                    time_before = time_now
            # stop main program if one job produces an error
            prc.start() # start of each process
#            time.sleep(0.001)
            if self.runmode == "multicore": # only difference here is that not all submitted jobs are directly running
                queued_jobs   = queued_jobs - 1
                running_jobs  = len(multiprocessing.active_children())
                finished_jobs = max_jobs - queued_jobs - running_jobs
            elif self.runmode == "cluster":# and int(nr_cores) != -1 or (self.runmode == "cluster" and cluster.get_jobs_in_cluster_queue() > int(config_list["max_jobs_in_cluster_queue"]) and int(config_list["max_jobs_in_cluster_queue"]) !=-1):
#            elif self.runmode == "cluster":# and (int(nr_cores) != -1 or (cluster.get_jobs_in_cluster_queue() > int(config_list["max_jobs_in_cluster_queue"]) and int(config_list["max_jobs_in_cluster_queue"]) !=-1)):
                missing_jobs   = missing_jobs - 1
#                if queued_jobs+running_jobs != len(multiprocessing.active_children()): # this does not work always due to different timings I suppose, also not really important...
#                    out.print_warning("Sum of pending and running jobs does not add up to number of active jobs.")
                # if queued_jobs+running_jobs != cluster.get_nr_of_cluster_jobs("pending,running"): # this does not work always due to different timings I suppose, also not really important...
                #     out.print_warning("Sum of pending and running jobs does is not equal to all active jobs.")
            current_time  = str(datetime.datetime.now()).split('.')[0]
            # prints intermediate status, whenever jobs have been sent
            if len(multiprocessing.active_children()) >= nr_cores and int(nr_cores) != -1:
                if self.runmode == "cluster" and int(nr_cores) != -1:
                    queued_jobs   = missing_jobs+cluster.get_nr_of_cluster_jobs("pending")
                    running_jobs  = cluster.get_nr_of_cluster_jobs("running")
                    finished_jobs = max_jobs - queued_jobs - running_jobs
                out.print_jobs("| %s | Queued: %s | Running: %s | Finished: %s |" % 
                               (current_time, queued_jobs, running_jobs, finished_jobs))
        if self.runmode == "multicore": # only difference here is that not all submitted jobs are directly running
            running_jobs  = len(multiprocessing.active_children())
            finished_jobs = max_jobs - queued_jobs - running_jobs
        elif self.runmode == "cluster":
            queued_jobs   = missing_jobs+cluster.get_nr_of_cluster_jobs("pending")
            running_jobs  = cluster.get_nr_of_cluster_jobs("running")
            finished_jobs = max_jobs - queued_jobs - running_jobs
        running_jobs_before = running_jobs
        finished_jobs_before = finished_jobs
        time_before = time.time()
        firsttime = True
        # this whileloop waits that all jobs finish and prints out the number of queued/running/finished jobs from time to time
        while len(multiprocessing.active_children()) > 0:
            time.sleep(5)
            time_now = time.time()
            if self.runmode == "multicore": # only difference here is that not all submitted jobs are directly running
                running_jobs  = len(multiprocessing.active_children())
                finished_jobs = max_jobs - queued_jobs - running_jobs
            elif self.runmode == "cluster":
                queued_jobs   = cluster.get_nr_of_cluster_jobs("pending")
                running_jobs  = cluster.get_nr_of_cluster_jobs("running")
#                if queued_jobs+running_jobs != len(multiprocessing.active_children()): # this does not work always due to different timings I suppose, also not really important...
#                    out.print_warning("Sum of pending and running jobs does not add up to number of active jobs.")
#                if queued_jobs+running_jobs != cluster.get_nr_of_cluster_jobs("pending,running"): # this does not work always due to different timings I suppose, also not really important...
#                    out.print_warning("Sum of pending and running jobs does is not equal to all active jobs.")
            finished_jobs = max_jobs - queued_jobs - running_jobs
            if firsttime: # print out the status the first time in loop
            # simply assume that there are no finished jobs at the first printout (looks stupid otherwise if jobs are finished because of an error and then resubmitted
                if self.runmode == "cluster":
                    finished_jobs = 0
                    queued_jobs = max_jobs - running_jobs
                current_time  = str(datetime.datetime.now()).split('.')[0]
                out.print_jobs("| %s | Queued: %s | Running: %s | Finished: %s |" % 
                               (current_time, queued_jobs, running_jobs, finished_jobs))
                firsttime = False
            if running_jobs_before != running_jobs or finished_jobs_before != finished_jobs: # print out status whenever some jobs started or finished running
                # wait if these jobs failed and are resubmitted
#                print self.job_just_restarted()
#                if self.job_just_restarted():
                if self.runmode == "multicore": # only difference here is that not all submitted jobs are directly running
                    running_jobs  = len(multiprocessing.active_children())
                    finished_jobs = max_jobs - queued_jobs - running_jobs
                elif self.runmode == "cluster":
                    queued_jobs   = cluster.get_nr_of_cluster_jobs("pending")
                    running_jobs  = cluster.get_nr_of_cluster_jobs("running")
                    time.sleep(15)
                #    self.job_just_restarted_remove()
                if running_jobs_before != running_jobs or finished_jobs_before != finished_jobs: # print out status whenever some jobs started or finished running
                    current_time  = str(datetime.datetime.now()).split('.')[0]
                    out.print_jobs("| %s | Queued: %s | Running: %s | Finished: %s |" % 
                                   (current_time, queued_jobs, running_jobs, finished_jobs))
                    time_before = time_now
                    running_jobs_before = running_jobs
                    finished_jobs_before = finished_jobs
            if time_now - time_before >= parameter_list["print_out_interval"]: # print out status every time interval (hard coded at the beginning)
                current_time  = str(datetime.datetime.now()).split('.')[0]
                out.print_jobs("| %s | Queued: %s | Running: %s | Finished: %s |" % 
                               (current_time, queued_jobs, running_jobs, finished_jobs))
                time_before = time_now

        # Ensure all of the processes have finished
        for prc in jobs:
            while prc.exitcode is None:
                prc.join()
            if prc.exitcode > 0:
                self.errors_flag = True

        current_time  = str(datetime.datetime.now()).split('.')[0]
        # prints final phase of jobs (all jobs finished)
        out.print_jobs("| %s | Queued: 0 | Running: 0 | Finished: %s |" % 
                       (current_time, max_jobs))

        # check wether there are still active jobs
        if self.runmode == "cluster":
            folder_path = pjoin(fold.run_folder_path,"cluster","active_jobs")
            onlyfiles = [ f for f in os.listdir(folder_path) if os.path.isfile(pjoin(folder_path,f)) ]
            if onlyfiles:
                out.print_warning("Although run finished there appear to remain still active jobs, see folder: "+folder_path)
#}}}
#{{{ def: submit_jobs_cluster(self,job_list)
    def submit_jobs_cluster(self,job_list):
        # loop over jobs and add to process queue

        # create job dictionary to keep track of all jobs and its IDs to check which status they are and restart if necessary
        job_dict = {}
        should_not_be_running_and_not_correctly_finished = [] # list with job ids that should not be running anymore on cluster (either restarted or failed)

        max_jobs = len(job_list)
        queued_jobs = max_jobs
        missing_jobs = max_jobs
#{{{ initial printout
        current_time  = str(datetime.datetime.now()).split('.')[0]
        out.print_jobs("| %s | Queued: %s | Running: 0 | Finished: 0 |" % (current_time, queued_jobs))
#}}}
#{{{ initial submit of all jobs, including relevant printouts
        for job in job_list:
            time_before = time.time()
            firsttime = True
            while (self.runmode == "cluster" and cluster.get_jobs_in_cluster_queue() > int(config_list["max_jobs_in_cluster_queue"]) and int(config_list["max_jobs_in_cluster_queue"]) !=-1):
                time.sleep(1)
                time_now = time.time()
                # prints status every 5 minutes to show that script is alive
                # (important also for cluster mode, where not all jobs can always be started at once)
                if time_now - time_before >= parameter_list["print_out_interval"] or firsttime:
                    firsttime = False
                    if self.runmode == "cluster" and int(nr_cores) != -1 or (self.runmode == "cluster" and cluster.get_jobs_in_cluster_queue() > int(config_list["max_jobs_in_cluster_queue"]) and int(config_list["max_jobs_in_cluster_queue"]) !=-1):
                        queued_jobs   = missing_jobs+cluster.get_nr_of_cluster_jobs("pending")
                        running_jobs  = cluster.get_nr_of_cluster_jobs("running")
                        finished_jobs = max_jobs - queued_jobs - running_jobs
                        # print information while in this loop
                        if cluster.get_jobs_in_cluster_queue() > int(config_list["max_jobs_in_cluster_queue"]) and int(config_list["max_jobs_in_cluster_queue"]) !=-1:
                            out.print_info("Cluster queue full (%s jobs > max_jobs_in_cluster_queue=%s in MATRIX_configuration). Waiting for other jobs to finish..." % (cluster.get_jobs_in_cluster_queue(),config_list["max_jobs_in_cluster_queue"]))
                        elif len(multiprocessing.active_children()) >= nr_cores and int(nr_cores) != -1:
                            out.print_info("Submitted max_nr_parallel_jobs=%s (from MATRIX_configuration) jobs to cluster queue. Waiting for own jobs to finish..." % nr_cores)
                    current_time  = str(datetime.datetime.now()).split('.')[0]
                    out.print_jobs("| %s | Queued: %s | Running: %s | Finished: %s |" % 
                                   (current_time, queued_jobs, running_jobs, finished_jobs))
                    time_before = time_now
            nr_of_tries = 1 
            job_id, start_time = self.start_job_on_cluster(job,nr_of_tries)
            current_time  = str(datetime.datetime.now()).split('.')[0]
            self.write_job_log_start(job[0],job[1],"started at %s ..."%(current_time))
            job_dict[job_id] = job+[nr_of_tries,start_time]
            missing_jobs   = missing_jobs - 1
        queued_jobs_before   = missing_jobs+cluster.get_nr_of_cluster_jobs("pending")
        running_jobs_before  = cluster.get_nr_of_cluster_jobs("running")
        finished_jobs_before = max_jobs - queued_jobs_before - running_jobs_before
        time_before = time.time()
        firsttime = True
#}}}
#{{{ monitor all jobs, take action if they finished, failes, etc.
        time_before = time.time()
        while job_dict: # this whileloop waits until all jobs finish and prints out the number of queued/running/finished jobs from time to time
            # check all jobs every 2 minutes
            if time.time() - time_before >= parameter_list["check_interval"] or (queued_jobs == 0 and running_jobs == 0) or (running_jobs_before != cluster.get_nr_of_cluster_jobs("running")):
                time_before = time.time()
                for job_id in should_not_be_running_and_not_correctly_finished:
                    if not cluster.cluster_job_finished(job_id):
                        cluster.cluster_job_remove(job_id) # removes from active jobs and add to finished job list
                        out.print_warning("Killed ghost job (job_id: %s) which should not be running as it has already been restarted or finally failed." % job_id)
                # go through job list and check which job have finished, failed, etc. and need to be removed from the list, need to be restarted, etc.
                job_dict_copy = copy.copy(job_dict)
                for job_id, job in job_dict_copy.iteritems():
                    run_path = job[0]
                    process  = job[1]
                    cluster_job_finished = False
                    job_correctly_finished = False
                    if cluster.cluster_job_finished(job_id):
                        cluster_job_finished = True
                        if self.job_correctly_finished(run_path,process):
                            job_correctly_finished = True
                        else:
                            time.sleep(10)
                            if cluster.cluster_job_finished(job_id): # check again if this job really does not exist anymore
                              if self.job_correctly_finished(run_path,process): # make sure job has not correctly finished in meanwhile
                                job_correctly_finished = True
                              else:
                                time.sleep(10)
                                if cluster.cluster_job_finished(job_id): # check again if this job really does not exist anymore
                                  if self.job_correctly_finished(run_path,process): # make sure job has not correctly finished in meanwhile
                                    job_correctly_finished = True
                                  else:
                                    time.sleep(10)
                                    if cluster.cluster_job_finished(job_id): # check again if this job really does not exist anymore
                                      if self.job_correctly_finished(run_path,process): # make sure job has not correctly finished in meanwhile
                                        job_correctly_finished = True
                                    else:
                                      cluster_job_finished = False
                                else:
                                  cluster_job_finished = False
                            else:
                              cluster_job_finished = False
                    if cluster_job_finished or job_correctly_finished:
                        del job_dict[job_id]
                        run_path    = job[0]
                        process     = job[1]
                        nr_of_tries = job[2]
                        start_time  = job[3]
                        m, s = divmod(time.time()-start_time, 60)
                        h, m = divmod(m, 60)
                        if job_correctly_finished:
                            # remove logs
                            cluster.remove_job_from_folder(job_id,"active_jobs") # remove job from active jobs
                            cluster.add_job_id_to_list(job_id,"job_ids_finished.list") # add job to finished job list
                            # write that job successfully finished into log file (with tries and time) and final line
                            self.write_job_log(run_path,process,"try %s successfully finished after %dh:%02dm:%02ds"%(nr_of_tries, h, m, s))
                            current_time  = str(datetime.datetime.now()).split('.')[0]
                            self.write_job_log_finish(run_path,process,"...success %s"%(current_time))
                            # try to remove from list of failed jobs (can happen when job was restarted)
                            try:
                                self.remove_job_failed_log(run_path,process)
                            except: 
                                pass
                            # add job to log of list of succseffully finished jobs
                            self.add_job_success_log(run_path,process)
                        else:
                            job_id_old = job_id
                            should_not_be_running_and_not_correctly_finished.append(job_id)
                            # write that try failed
                            self.write_job_log(run_path,process,"try %s failed after %dh:%02dm:%02ds"%(nr_of_tries, h, m, s))
                            if nr_of_tries < self.max_nr_of_tries:
                                out.print_warning("Job%s failed; path: %s, channel: %s. Re-trying with different random seed..." % (" (ID %s)" % job_id if self.runmode == "cluster" else "",run_path,process))
                                # change random seed of run
                                try: # normal runs
                                    parallel_folder_i = int(run_path.rsplit(".",1)[1])
                                    run_dirs_in_path = [ f for f in os.listdir(run_path.rsplit('/', 1)[0]) if f.startswith("run.") and os.path.isdir(pjoin(run_path.rsplit('/', 1)[0],f)) ]
                                    max_parallel_runs = int(len(run_dirs_in_path)) # result combination now uses always all folders which are there
                                    zwahl = max_parallel_runs*(nr_of_tries)+parallel_folder_i
                                except: # grid runs
                                    zwahl = randint(10,5000)
                                # adding max_parallel_runs makes sure there is no overlapping in random seed with the other runs; 
                                # NOTE (2do:?) there could be a overlapping between extrapolation and main run, but what do you want?
                                param_file_path = pjoin(run_path,"log","file_parameter."+process+".dat")
                                inp.input_set_entry(param_file_path,"zwahl",str(zwahl))
                                # save jobs that were restarted to a file which is printed at the end:
                                log.add_to_list("restarted_list.log",pjoin(run_path,"log",process+"_try"+str(nr_of_tries)+".log"))
                                # restart job
                                job_id, start_time = self.start_job_on_cluster(job,nr_of_tries+1)
                                job_dict[job_id] = job
                                job_dict[job_id][2] = nr_of_tries+1
                                job_dict[job_id][3] = start_time
                            else:
                                current_time  = str(datetime.datetime.now()).split('.')[0]
                                self.write_job_log_finish(run_path,process,"... failed %s"%(current_time))
                                # try to remove from list of successful jobs (can happen when successful job was restarted)
                                try:
                                    self.remove_job_success_log(run_path,process)
                                except: 
                                    pass
                                self.add_job_failed_log(run_path,process)
                            cluster.cluster_job_remove(job_id_old) # removes from active jobs and add to finished job list
#}}}
#{{{ print out job info if at first time, if things change, or every time interval
            time.sleep(5)
            time_now = time.time()
            queued_jobs   = cluster.get_nr_of_cluster_jobs("pending")
            running_jobs  = cluster.get_nr_of_cluster_jobs("running")
            finished_jobs = max_jobs - queued_jobs - running_jobs
            if firsttime: # print out the status the first time in loop
                # simply assume that there are no finished jobs at the first printout (looks stupid otherwise if jobs are finished because of an error and then resubmitted
                finished_jobs = 0
                queued_jobs = max_jobs - running_jobs
                current_time  = str(datetime.datetime.now()).split('.')[0]
                out.print_jobs("| %s | Queued: %s | Running: %s | Finished: %s |" % 
                               (current_time, queued_jobs, running_jobs, finished_jobs))
                firsttime = False
            if running_jobs_before != running_jobs or finished_jobs_before != finished_jobs: # print out status whenever some jobs started or finished running
                # wait if these jobs failed and are resubmitted
                time.sleep(15)
                queued_jobs   = cluster.get_nr_of_cluster_jobs("pending")
                running_jobs  = cluster.get_nr_of_cluster_jobs("running")
                if running_jobs_before != running_jobs or finished_jobs_before != finished_jobs: # print out status whenever some jobs started or finished running
                    current_time  = str(datetime.datetime.now()).split('.')[0]
                    out.print_jobs("| %s | Queued: %s | Running: %s | Finished: %s |" % 
                                   (current_time, queued_jobs, running_jobs, finished_jobs))
                    time_before = time_now
                    running_jobs_before = running_jobs
                    finished_jobs_before = finished_jobs
            if time_now - time_before >= parameter_list["print_out_interval"]: # print out status every time interval (set by default or as input from parameter.dat)
                current_time  = str(datetime.datetime.now()).split('.')[0]
                out.print_jobs("| %s | Queued: %s | Running: %s | Finished: %s |" % 
                               (current_time, queued_jobs, running_jobs, finished_jobs))
                time_before = time_now
#}}}
#{{{ final printout
        current_time  = str(datetime.datetime.now()).split('.')[0]
        out.print_jobs("| %s | Queued: 0 | Running: 0 | Finished: %s |" % 
                       (current_time, max_jobs))
#}}}
#{{{ check if there appear to be active jobs
        folder_path = pjoin(fold.run_folder_path,"cluster","active_jobs")
        onlyfiles = [ f for f in os.listdir(folder_path) if os.path.isfile(pjoin(folder_path,f)) ]
        if onlyfiles:
            out.print_warning("Although run finished there appear to remain still active jobs, see folder: "+folder_path)
#}}}

#}}}
#{{{ def: job_process(self,run_info)
    def job_process(self,run_info):
    # job process specified by folder structure, to be run in specified mode
    # here the execution of the code is actually done        
    # determines how often job is restarted in order to correctly finish ("final result" in execution folder)
      max_nr_of_tries = run_class.max_nr_of_tries # set (hard-coded) in initilisation
      nr_of_tries = 1
      # this is for the final run where all results are collected and combined
      if run_info[0].startswith("infile.") or run_info[0].startswith("infile.distribution"):
#{{{
          results_combination_file = run_info[0]
          results_type = run_info[1] # "result" for total rates or "distribution" for distributions
          results_path = pjoin(process_dir,run_folder,"result")
          # switch directory to result_path
          os.chdir(results_path)
          # set path of ouput file (standard screen output)
          subprocess_out_path = pjoin(results_path,run_info[0]+"."+results_type+".log")
          # set path of error file (error screen output)
#          subprocess_err_path = pjoin(results_path,run_info[0]+"."+results_type+".err")
          with open(subprocess_out_path,'w+') as subprocess_out:
#              with open(subprocess_err_path,'w+') as subprocess_err:
                  # save start_time to compute difference to finishing time and print into log how long the jobs take
                  start_time = time.time()
                  # determine current_time in convenient format to print it as starting time in log
                  current_time  = str(datetime.datetime.now()).split('.')[0]

                  # also here: supplement with cluster mode
                  if self.runmode == "multicore":
                      env = copy.copy(os.environ)
                      env["LIBC_FATAL_STDERR_"] = "1"
#                      if results_type == "result":
                      p = subprocess.Popen([fold.exe_path,results_type,results_combination_file], stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)
#                      elif results_type == "distribution":
#                          p = subprocess.Popen([self.combine_distributions_path,results_combination_file], stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)
                      signal.signal(signal.SIGINT,self.control_c_handler_child)
                  elif self.runmode == "cluster":
                      # this submits the job to the cluster, returns the job_id, writes the job_id to a file and saves the batch file
                      dummy = ""
                      # use the same routine as for the usual run jobs so that the batchfile has the same structure
                      time_before_first_submit = time.time()
                      job_id = cluster.cluster_job_submit(results_combination_file,results_type,dummy,subprocess_out_path,subprocess_out_path)
                      signal.signal(signal.SIGINT,self.control_c_handler_child)
                      while job_id == "resubmit":
                          job_id = cluster.cluster_job_submit(results_combination_file,results_type,dummy,subprocess_out_path,subprocess_out_path)
                          signal.signal(signal.SIGINT,self.control_c_handler_child)
                          if time.time() - time_before_first_submit > 600:
                              out.print_error("There seems to be a problem with the submission to the cluster. Job could not be submitted within 10 minutes, while re-submitting every minute. Please stop the code and solve the issue...")
                  stdout = []
                  stderr = []
                  while True:
                      if self.runmode == "multicore":
                          reads = [p.stdout.fileno(), p.stderr.fileno()]
                          ret = select.select(reads, [], [])
                  
                          for fd in ret[0]:
                              if fd == p.stdout.fileno():
                                  read = p.stdout.readline().rstrip('\n')
                                  print >>subprocess_out, "%s" % read
                                  subprocess_out.flush()
                              if fd == p.stderr.fileno():
                                  read = p.stderr.readline().rstrip('\n')
                                  print >>subprocess_out, "%s" % read
                                  subprocess_out.flush()
                      # this somehow (poll) checks wether the job p.subprocess finished, as long as it does not
                      # the script remains in the while loop, once it finished the while loop "break"s
                      if self.runmode == "multicore" and p.poll() != None:
                          break # stops while loop because job finished
                      if self.runmode == "cluster" and cluster.cluster_job_finished(job_id):
                          time.sleep(1)
                          if cluster.cluster_job_finished(job_id):
                              time.sleep(1)
                              # if cluster.cluster_job_finished(job_id):
                              #     time.sleep(2)
                              if cluster.cluster_job_finished(job_id):
                                  cluster.remove_job_from_folder(job_id,"active_jobs") # remove job from active jobs
                                  cluster.add_job_id_to_list(job_id,"job_ids_finished.list") # add job to finished job list
                                  break
                      elif self.runmode == "cluster":
                          time.sleep(30)

#}}}
      # this is for the warmup and mainrun, when program runs in multicore or cluster mode
      elif self.runmode == "multicore" or self.runmode == "cluster":
#{{{
          run_path = run_info[0]
# hack until QCD works in EW executable
          fold.exe_path = fold.exe_pathEW
          if "RA.QCD" in self.get_contribution(run_path) or "RRA.QCD" in self.get_contribution(run_path):
              fold.exe_path = fold.exe_pathQCD
          process = run_info[1]
          # switch directory to run_path
          os.chdir(run_path)
          #subprocess.call(self.exe_path, stdin=subprocess_in, stdout=subprocess_out, stderr=subprocess_out, shell=False)
        
          while nr_of_tries <= max_nr_of_tries:
              # set path of input file
              subprocess_in_path = pjoin(run_path,"log",process+".in")
              # set path of ouput file (standard screen output)
              subprocess_out_path = pjoin(run_path,"log",process+"_try"+str(nr_of_tries)+".log")
              # set path of error file (error screen output)
#              subprocess_err_path = pjoin(run_path,"log",process+"_try"+str(nr_of_tries)+".err")
              with self.fileopen(subprocess_in_path,'r') as subprocess_in: # only opened in multicore mode
               with self.fileopen(subprocess_out_path,'w+') as subprocess_out: # only opened in multicore mode
#                with self.fileopen(subprocess_err_path,'w+') as subprocess_err: # only opened in multicore mode
                  # save start_time to compute difference to finishing time and print into log how long the jobs take
                  start_time = time.time()
                  finished_check_time   = start_time # needed to do additional check if run correctly finished every two minutes
                  finished_check_time_2 = start_time # needed to redo additional check if run correctly finished after 15 minutes
                  first_check = True
                  # determine current_time in convenient format to print it as starting time in log
                  current_time  = str(datetime.datetime.now()).split('.')[0]
                  if nr_of_tries == 1:
                      self.write_job_log_start(run_path,process,"started at %s ..."%(current_time))

                  if self.runmode == "multicore":
                      env = copy.copy(os.environ)
                      env["LIBC_FATAL_STDERR_"] = "1"
                      p = subprocess.Popen(fold.exe_path, stdin=subprocess_in, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)
                      signal.signal(signal.SIGINT,self.control_c_handler_child)
                  elif self.runmode == "cluster":
                      # this submits the job to the cluster, returns the job_id, writes the job_id to a file and saves the batch file
                      time_before_first_submit = time.time()
                      job_id = cluster.cluster_job_submit(run_path,process,subprocess_in_path,subprocess_out_path,subprocess_out_path)
                      signal.signal(signal.SIGINT,self.control_c_handler_child)            
                      while job_id == "resubmit":
                          job_id = cluster.cluster_job_submit(run_path,process,subprocess_in_path,subprocess_out_path,subprocess_out_path)
                          signal.signal(signal.SIGINT,self.control_c_handler_child)
                          if time.time() - time_before_first_submit > 120:
                              out.print_error("There seems to be a problem with the submission to the cluster. Job could not be submitted within 2 minutes, while re-submitting every 15 seconds. Please stop the code and solve the issue...")
                              
                      
                  stdout = []
                  stderr = []
                  checks_done = False
                  while True:
                      if self.runmode == "multicore":
                          reads = [p.stdout.fileno(), p.stderr.fileno()]
                          ret = select.select(reads, [], [])
                  
                          for fd in ret[0]:
                              if fd == p.stdout.fileno():
                                  read = p.stdout.readline().rstrip('\n')
                                  print >>subprocess_out, "%s" % read
                                  subprocess_out.flush()
                              if fd == p.stderr.fileno():
                                  read = p.stderr.readline().rstrip('\n')
                                  print >>subprocess_out, "%s" % read
                                  subprocess_out.flush()

                      # after 30 seconds do some checks on log file, kill and restart if there is a problem
                      if abs(time.time()-start_time)>60 and not checks_done: # I don't think these checks are still relevant, and only produce problems
                          # BUT: we still need the check_done signal, to set the right starting time
                          # check 1: see wether the log file is full of errors (>1000 errors)
                          # if self.log_full_of_errors(subprocess_out_path):
                          #     if self.runmode == "multicore":
                          #         p.kill()
                          #     elif self.runmode == "cluster":
                          #         cluster.cluster_job_remove(job_id)
                          #     out.print_warning("Job %s produced a bunch of errors, restarting job..."%(run_path+">>"+process))
                          #     break
                          # check 2: see wether at end of log file has specific keywod(eg, "n_original = 2")
                          # if self.log_keyword_end_of_file(subprocess_out_path,"n_original = 2"):
                          #     if self.runmode == "multicore":
                          #         p.kill()
                          #     elif self.runmode == "cluster":
                          #         cluster.cluster_job_remove(job_id)
                          #     out.print_warning("Job %s did not coninue (XXX at end of file), restarting job..."%(run_path+">>"+process))
                          #     break
                          checks_done = True
                      # this somehow (poll) checks wether the job p.subprocess finished, as long as it does not
                      # the script remains in the while loop, once it finished the while loop "break"s
                      if self.runmode == "multicore" and p.poll() != None:
                          break # stops while loop because job finished
                      if self.runmode == "cluster" and cluster.cluster_job_finished(job_id):
                          time.sleep(1)
                          if cluster.cluster_job_finished(job_id):
                              time.sleep(1)
#                              if cluster.cluster_job_finished(job_id):
#                                  time.sleep(2)
                              if cluster.cluster_job_finished(job_id):
                                  cluster.remove_job_from_folder(job_id,"active_jobs") # remove job from active jobs
                                  cluster.add_job_id_to_list(job_id,"job_ids_finished.list") # add job to finished job list
                                  break
                      elif self.runmode == "cluster":
                          # in cluster runs count the time from when the job has started running (otherwise there is no log file for the checks)
                          if self.runmode == "cluster" and not cluster.cluster_job_running(job_id) and not checks_done:
                              start_time = time.time() # as long as the job is not running reset the time
                          # this is the time in seconds how often the python jobs check wether the cluster job is finished
                          time.sleep(60)
                          # sometime job continues running even though it already finished its output, in this case kill the job and break the loop
                          if abs(time.time()-finished_check_time) > 120: # check every two minutes
                              if self.job_correctly_finished(run_path,process):
                                  # only print if it still not finished after 15 Minutes
                                  if first_check:
                                      finished_check_time_2 = time.time()
                                      first_check = False
#                                      out.print_warning("This is when it was discovered (ID %s) it first %s" % (job_id,str(datetime.datetime.now()).split('.')[0]))
                                  elif abs(time.time()-finished_check_time_2) > 300: # check after 5 Minutes again
#                                      out.print_warning("Run finished, but ID=%s still running on cluster. TIME: %s" % (job_id,str(datetime.datetime.now()).split('.')[0]))
                                      finished_check_time_2 = time.time()
#                                  cluster.cluster_job_remove(job_id)
#                                  break
                                  finished_check_time = time.time()
                              else:
                                  finished_check_time = time.time()
                  # this checks wether finished job has produced correct results ("final result" in execution folder)
                  if self.job_correctly_finished(run_path,process):
                      m, s = divmod(time.time()-start_time, 60)
                      h, m = divmod(m, 60)
                      # write that job successfully finished into log file (with tries and time)
                      self.write_job_log(run_path,process,"try %s successfully finished after %dh:%02dm:%02ds"%(nr_of_tries, h, m, s))
                      # try to remove from list of failed jobs (can happen when job was restarted)
                      try:
                          self.remove_job_failed_log(run_path,process)
                      except: 
                          pass
                      # add job to log of list of succseffully finished jobs
                      self.add_job_success_log(run_path,process)
                      break # stops while loop because finished job did produce correct results
                  # otherwise restart job, increase number of tries and print failed try into log
                  else:
#                      print job_id+" restarted"
#                      self.job_just_restarted_add()
                      m, s = divmod(time.time()-start_time, 60)
                      h, m = divmod(m, 60)
                      self.write_job_log(run_path,process,"try %s failed after %dh:%02dm:%02ds"%(nr_of_tries, h, m, s))
                      # if nr_of_tries > max_nr_of_tries/2: 
#                      exit()
                      #     time.sleep(5)
                      nr_of_tries += 1 # job will be restarted, therefore number of tries increased
                      # write that an error occured in this run
                      out.print_warning("Job%s failed; path: %s, channel: %s. Re-trying with different random seed..." % (" (ID %s)" % job_id if self.runmode == "cluster" else "",run_path,process))
                      # change random seed of run
                      try: # normal runs
                          param_file_path = pjoin(run_path,"log","file_parameter."+process+".dat")
                          parallel_folder_i = int(run_path.rsplit(".",1)[1])
                          run_dirs_in_path = [ f for f in os.listdir(run_path.rsplit('/', 1)[0]) if f.startswith("run.") and os.path.isdir(pjoin(run_path.rsplit('/', 1)[0],f)) ]
                          max_parallel_runs = int(len(run_dirs_in_path)) # result combination now uses always all folders which are there
                          zwahl = int(parameter_list.get("random_seed","0"))+max_parallel_runs*(nr_of_tries-1)+parallel_folder_i
                      except: # grid runs
                          param_file_path = pjoin(run_path,"log","file_parameter."+process+".dat")
                          zwahl = int(parameter_list.get("random_seed","0"))++randint(10,5000)
                      # adding max_parallel_runs makes sure there is no overlapping in random seed with the other runs; 
                      # NOTE (2do:?) there could be a overlapping between extrapolation and main run, but what do you want?
                      inp.input_set_entry(param_file_path,"zwahl",str(zwahl))
                      # save jobs that were restarted to a file which is printed at the end:
                      log.add_to_list("restarted_list.log",subprocess_out_path)
          # in case after all tries job still failed to produce correct results, add information to logs
          if nr_of_tries > max_nr_of_tries:
              current_time  = str(datetime.datetime.now()).split('.')[0]
              self.write_job_log_finish(run_path,process,"... failed %s"%(current_time))
              # try to remove from list of successful jobs (can happen when successful job was restarted)
              try:
                  self.remove_job_success_log(run_path,process)
              except: 
                  pass
              self.add_job_failed_log(run_path,process)
          # if one try succeeded add final line to log
          else:
              current_time  = str(datetime.datetime.now()).split('.')[0]
              self.write_job_log_finish(run_path,process,"...success %s"%(current_time))
      else:
        out.print_error("Runmode \"%s\" is not known, cannot submit processes" % self.runmode)
#}}}
#}}}
#{{{ def: start_job_on_cluster(self,run_info)
    def start_job_on_cluster(self,run_info,nr_of_tries):
    # execution of code done here by submitting jobs to cluster
        # submission for result run (not used as result run is done only locally currently)
        if run_info[0].startswith("infile.") or run_info[0].startswith("infile.distribution"):
#{{{
            results_combination_file = run_info[0]
            results_type = run_info[1] # "result" for total rates or "distribution" for distributions
            results_path = pjoin(process_dir,run_folder,"result")
            os.chdir(results_path) # switch directory to result_path
            subprocess_out_path = pjoin(results_path,run_info[0]+"."+results_type+".log") # set path of ouput file (standard screen output)
            # this submits the job to the cluster, returns the job_id, writes the job_id to a file and saves the batch file
            dummy = ""
            # use the same routine as for the usual run jobs so that the batchfile has the same structure
            time_before_first_submit = time.time()
            job_id = cluster.cluster_job_submit(results_combination_file,results_type,dummy,subprocess_out_path,subprocess_out_path)
            while job_id == "resubmit":
                job_id = cluster.cluster_job_submit(results_combination_file,results_type,dummy,subprocess_out_path,subprocess_out_path)
                if time.time() - time_before_first_submit > 600:
                    out.print_error("There seems to be a problem with the submission to the cluster. Job could not be submitted within 10 minutes, while re-submitting every minute. Please stop the code and solve the issue...")
            start_time = time.time()
#}}}
        # submission for the warmup, pre and mainrun, when program runs in multicore or cluster mode
        elif self.runmode == "multicore" or self.runmode == "cluster":
#{{{
            run_path = run_info[0]
            fold.exe_path = fold.exe_pathEW
#            if ".QCD" in self.get_contribution(run_path):
            if "RA.QCD" in self.get_contribution(run_path) or "RRA.QCD" in self.get_contribution(run_path):
                fold.exe_path = fold.exe_pathQCD
            process = run_info[1]
            os.chdir(run_path) # switch directory to run_path
            subprocess_in_path = pjoin(run_path,"log",process+".in") # path of input file
            subprocess_out_path = pjoin(run_path,"log",process+"_try"+str(nr_of_tries)+".log") # path of ouput file (standard screen output)
            time_before_first_submit = time.time()
            job_id = cluster.cluster_job_submit(run_path,process,subprocess_in_path,subprocess_out_path,subprocess_out_path)
            while job_id == "resubmit":
                job_id = cluster.cluster_job_submit(run_path,process,subprocess_in_path,subprocess_out_path,subprocess_out_path)
                if time.time() - time_before_first_submit > 120:
                    out.print_error("There seems to be a problem with the submission to the cluster. Job could not be submitted within 2 minutes, while re-submitting every 15 seconds. Please stop the code and solve the issue...")
            start_time = time.time()
#}}}
        else:
            out.print_error("Runmode \"%s\" is not known, cannot submit processes" % self.runmode)
        return job_id, start_time
#}}}
#{{{ def: get_parallel_pre_runs_for_contribution(self,run_dir)
    def get_parallel_pre_runs_for_contribution(self,run_dir):
    # this function returns the number of parallel jobs for the extrapolation run
        parallel = self.min_parallel_pre_run # default for ALL contributions
        # now change if specified for specific contribution
        try:
            settings = pre_run_settings[prc.process_name]
        except:
            settings = pre_run_settings["default"]
            pass
        for contribution in settings:
            if contribution in run_dir.rsplit('/',2)[1] and not "%s/LO/" % fold.run_folder_path in run_dir: # exclude LO from making prerun more parallel
                parallel = max(settings[contribution][0],parallel)
                if int(nr_cores) != -1 and nr_cores < parallel: # it makes no sense to parallelize higher than max_number of available cores (note: nr_cores = max_nr_parallel_jobs)
                    run_dir_up = run_dir.rsplit("/",1)[0]
                    if not run_dir_up in self.pre_parallel_printed:
                        if "NNLO" in self.order:
                            out.print_info("max_nr_parallel_jobs (in multicore: cores of machine) from MATRIX_configuration is %s, which is smaller than required parallelization (channel: %s; number of jobs for this channel: %s) for max_time_per_job in parameter.dat. Limiting the parallelization to max_nr_parallel_jobs... (max_time_per_job will not be met)" % (nr_cores,run_dir_up,parallel))
                            self.pre_parallel_printed.append(run_dir_up)
                    parallel = min(parallel,nr_cores)
        return parallel
#}}}
#{{{ def: get_pre_run_min_events_for_contribution(self,run_dir):
    def get_pre_run_min_events_for_contribution(self,run_dir):
    # this function returns the number of overall events for the extrapolation run
        min_events = self.min_events_per_channel # default for ALL contributions
        # now change if specified for specific contribution
        try:
            settings = pre_run_settings[prc.process_name]
        except:
            settings = pre_run_settings["default"]
            pass
        for contribution in settings:
            if contribution in run_dir and not "%s/LO/" % fold.run_folder_path in run_dir:
                min_events = settings[contribution][1]
        return min_events
#}}}
#{{{ def: read_runtimes(self)
    def read_runtimes(self):
    # this function reads out the results of the extrapolation runs and saves 
    # in the class-intern multidimensional dictionary depending on the run_dir+channel:         
        # hard-coded parameters for extrapolation that might be adjusted with experience:
        # minimal number of events per channel
#        self.min_events_per_channel = 50000 # set in class initialization
        # minimal number of parallelizations per channel (at least 2 if one goes wrong)
#        self.min_parallel_per_channel = 1 # set in class initialization
        # arbitrary factor that determines number of printouts
        factor1 = 2
        # arbitrary factor that determines maximal number of events (from minimal number of events)
        factor2 = 2
        # arbitray that reduces the max_time_per_job, for factor_max_time = 1 the jobs take 2-3 times longer than expected
        factor_max_time = 2.5
        # correction factor for estimated runtimes, dependending on contribution (has exactly the same effect as factor_max_time, but chosen specific to the contribution)
        factor_runtime = {}
        # factor_runtime["VT2"] = 2
        # factor_runtime["RVA"] = 2
        # Make sure in continued run you use the runtime.dat of the pre run (not of the main run) in cases
        # where the runtime.dat of the main run has already created (in that case runtime.dat of pre run was 
        # moved to runtime_pre_run.dat
        if continue_run:
            runtime_file_path = pjoin(fold.run_folder_path,"result","runtime_pre_run.dat")
            if not os.path.isfile(runtime_file_path):
                runtime_file_path = pjoin(fold.run_folder_path,"result","runtime.dat")
        else:
            runtime_file_path = pjoin(fold.run_folder_path,"result","runtime.dat")
        parallelization_file_path = pjoin(fold.log_folder_path,"parallelization.log")
        if not os.path.isfile(runtime_file_path):
            out.print_warning("File "+runtime_file_path+" in function read_runtimes does not exist! Trying to get runtimes from existing pre_run...")
            if self.pre_run_complete():
                self.extrapolate_runtimes()
            else:
                out.print_error_no_stop("There is no complete pre run! Please restart from pre run.")
                out.print_error_no_stop("The following pre runs have no \"final result\" in the execution file:")
                out.print_list(run.failed_run_list,"error")
                exit(0)
        with open(runtime_file_path, 'r') as runtime_file:
          with open(parallelization_file_path, 'w') as parallelization_file:
            parallelization_file.write("# folder                 subprocess     parallel    n_event_min    n_event_max           norm           deviation\n")
            for in_line in runtime_file:
                line=in_line.strip() # strip removes all spaces (including tabs and newlines)
                # if any line starts with %, # or is an emtpy line (disregarding spaces) it is a comment line and should be skipped
                if line=="" or line[0]=="%" or line[0]=="#": 
                    continue
                # split line by any number of spaces
                split_line = line.split()
                rel_run_dir = split_line[0]
                run_dir   = pjoin(fold.run_folder_path,rel_run_dir,"run.0")
                channel   = split_line[1]
                runtime   = int(split_line[2])
                n_event   = int(split_line[3])
                deviation = float(split_line[4])
                norm      = float(split_line[5])
                norm_err  = float(split_line[6])

                if "VT2.QCD" in run_dir and prc.process_name in VT2_use_default_runtime:
                    runtime   = VT2_use_default_runtime[prc.process_name][1]
                    n_events  = VT2_use_default_runtime[prc.process_name][0]
                    deviation = 0.001
                    norm      = ref_NNLO
                    norm_err  = ref_NNLO_err
                elif "%s /NNLO" % fold.run_folder_path in run_dir and "born" in run_dir:
                    ref_NNLO     = norm
                    ref_NNLO_err = norm_err
                
                for contribution in factor_runtime:
                    if contribution in run_dir:
                        runtime = runtime * factor_runtime[contribution]

                self.runtime_table[run_dir][channel]["runtime"] = runtime
                self.runtime_table[run_dir][channel]["n_event"] = n_event
                self.runtime_table[run_dir][channel]["deviation"] = deviation
                self.runtime_table[run_dir][channel]["sigma_normalization"] = norm
                self.runtime_table[run_dir][channel]["sigma_normalization_error"] = norm_err

                max_time_per_job = float(parameter_list["max_time_per_job"])*60*60/factor_max_time
                if rel_run_dir.split("/")[0] == "LO":
                    if rel_run_dir.split("/")[1].startswith("a"):
                        precision = float(parameter_list["precision_NLO_EW"])
                    else:
                        precision = float(parameter_list["precision_LO"])
                elif rel_run_dir.split("/")[0] == "NLO" or rel_run_dir.split(".")[0] == "NLO":
                    if rel_run_dir.split("/")[1] == self.get_coupling_order("LO","NLO") or rel_run_dir.split("/")[1].startswith("a"):
                        precision = float(parameter_list["precision_NLO_EW"])
                    else:
                        precision = float(parameter_list["precision_NLO_QCD"])
                elif rel_run_dir.split("/")[0] == "NNLO" or rel_run_dir.split(".")[0] == "NNLO" or rel_run_dir.split(".")[0] == "NNNLO":
                    if rel_run_dir.split("/")[1] == self.get_coupling_order("LO","NLO") or rel_run_dir.split("/")[1].startswith("a"):
                        precision = float(parameter_list["precision_added_EW"])
                    else:
                        precision = float(parameter_list["precision_NNLO_QCD"])
                else:
                    out.print_error("The run_dir in read_runtimes does not contain LO, NLO or NNLO: run_dir = %s" % run_dir)
                #                   math.ceil rundet auf!
                parallel_jobs = max(math.ceil(runtime/max_time_per_job * (0.001/precision)**2), self.min_parallel_per_channel)
                if int(nr_cores) != -1: # it makes no sense to parallelize higher than max_number of available cores (note: nr_cores = max_nr_parallel_jobs)
                    if nr_cores < parallel_jobs:
                        out.print_info("max_nr_parallel_jobs (in multicore: cores of machine) from MATRIX_configuration is %s, which is smaller than required parallelization (channel: %s; number of jobs for this channel: %s) for max_time_per_job in parameter.dat. Limiting the parallelization to max_nr_parallel_jobs... (max_time_per_job will not be met)" % (nr_cores,run_dir,parallel_jobs))
                    parallel_jobs = min(parallel_jobs,nr_cores)
                if n_event == 0: # this should rarely happen!
                    n_event = 1
                if self.min_events_per_channel > n_event * (0.001/precision)**2:
                    parallel_jobs = max(math.ceil(runtime/max_time_per_job * (0.001/precision)**2 * self.min_events_per_channel/(n_event*(0.001/precision)**2)), self.min_parallel_per_channel)
                    n_step = math.ceil(self.min_events_per_channel/factor1/parallel_jobs)
                else:
                    n_step = math.ceil(n_event/factor1  * (0.001/precision)**2/parallel_jobs)

                n_events_min = n_step * factor1
                n_events_max = n_events_min * factor2
                norm_deviation = deviation * precision/0.001 * math.sqrt(parallel_jobs)

                self.runtime_table[run_dir][channel]["parallel_jobs"] = int(parallel_jobs)
                self.runtime_table[run_dir][channel]["n_step"] = n_step 
                self.runtime_table[run_dir][channel]["n_events_min"] = n_events_min
                self.runtime_table[run_dir][channel]["n_events_max"] = n_events_max
                self.runtime_table[run_dir][channel]["sigma_normalization_deviation"] = norm_deviation
                # add parallelization to summary for each contribution (not each run folder or channel; that is already saved below)
                log.summary_add_parallel(parallel_jobs,run_dir.rsplit('/',1)[0])

                try:
                    parallel_jobs += 1
                except TypeError:
                    out.print_error("Number of parallel jobs is no Integer: parallel_jobs = %s." % parallel_jobs)
                try:
                    n_step += 1
                except TypeError:
                    out.print_error("Number of steps per iteration is no Integer n_step = %s." % n_step)

                # save determined parellelization to file
                length = 15
                line_out = rel_run_dir+" "*(25-len(rel_run_dir))
                line_out += channel+" "*(18-len(channel))
                line_out += " "*(5-len(str(int(parallel_jobs))))+str(int(parallel_jobs))
                line_out += " "*(length-len(str(n_events_min)))+str(n_events_min)
                line_out += " "*(length-len(str(n_events_max)))+str(n_events_max)
                line_out += " "*(length-len(str(norm)))+str(norm)
                line_out += " "*(20-len(str(norm_deviation)))+str(norm_deviation)+"\n"
                parallelization_file.write(line_out)
#}}}
#{{{ def: set_inputs_test_run(self)
    def set_inputs_test_run(self,test_dir):
    # this function sets the default inputs for the grid run
        # set new grid setup parameters
        inp.input_set_entry(pjoin(test_dir,"file_parameter.dat"),"zwahl",parameter_list.get("random_seed","0"))
        inp.input_set_entry(pjoin(test_dir,"file_parameter.dat"),"output_level","DEBUG")
#}}}
#{{{ def: set_inputs_grid_run(self)
    def set_inputs_grid_run(self,grid_dir):
    # this function sets the default inputs for the grid run
        # these values are hard-coded and may be adjust by experience:
        switch_IS_MC = 1
        # set new grid setup parameters
        if new_MATRIX_run:
            inp.input_set_entry(pjoin(grid_dir,"file_parameter.dat"),"MCweight_opt","3")
            inp.input_set_entry(pjoin(grid_dir,"file_parameter.dat"),"switch_MC","1")
            inp.input_set_entry(pjoin(grid_dir,"file_parameter.dat"),"switch_IS_MC",str(switch_IS_MC))
            inp.input_set_entry(pjoin(grid_dir,"file_parameter.dat"),"switch_IS_tau","1")
            inp.input_set_entry(pjoin(grid_dir,"file_parameter.dat"),"switch_IS_x1x2","1")
            if "/CA.QCD/" in grid_dir or "/RCA.QCD/" in grid_dir:
                # optimize z1z2 in CA and RCA
                inp.input_set_entry(pjoin(grid_dir,"file_parameter.dat"),"switch_IS_z1z2","1")
            else:
                inp.input_set_entry(pjoin(grid_dir,"file_parameter.dat"),"switch_IS_z1z2","0")
            if parameter_list.get("improve_mappings_for_single_V","0") == "1":
                inp.input_set_entry(pjoin(grid_dir,"file_parameter.dat"),"switch_MC_tau","1")
                inp.input_set_entry(pjoin(grid_dir,"file_parameter.dat"),"switch_MC_x_dipole","1")
            inp.input_set_entry(pjoin(grid_dir,"file_parameter.dat"),"switch_n_events_opt","2")
            inp.input_set_entry(pjoin(grid_dir,"file_parameter.dat"),"n_events_min","0")
            inp.input_set_entry(pjoin(grid_dir,"file_parameter.dat"),"switch_distribution","0")
            inp.input_set_entry(pjoin(grid_dir,"file_parameter.dat"),"switch_TSV","0")
            inp.input_set_entry(pjoin(grid_dir,"file_parameter.dat"),"switch_CV","1")
            inp.input_set_entry(pjoin(grid_dir,"file_parameter.dat"),"n_scales_CV","1")
            inp.input_set_entry(pjoin(grid_dir,"file_parameter.dat"),"zwahl",parameter_list.get("random_seed","0"))
            if parameter_list.get("reduce_workload",0) == "2":
                inp.input_set_entry(pjoin(grid_dir,"file_parameter.dat"),"output_level","WARN")
            if "L2RA" in grid_dir or "L2RT" in grid_dir:
                inp.input_set_entry(pjoin(grid_dir,"file_parameter.dat"),"OL stability_mode","21")
            if "L2RA" in grid_dir:
                inp.input_set_entry(pjoin(grid_dir,"file_parameter.dat"),"OL stability_kill2","1.e-2")
            if "L2RT" in grid_dir:
                inp.input_set_entry(pjoin(grid_dir,"file_parameter.dat"),"OL stability_kill2","1.")
#}}}
#{{{ def: set_inputs_pre_run(self)
    def set_inputs_pre_run(self,run_dir_i,parallel_folder_i):
    # this function sets the default inputs for the pre run, that extrapolates the runtimes
        # first: create new run_dir_i by copying the main_run_folder (run.0)
        run_dir_0 = run_dir_i.replace("run.%s" % parallel_folder_i,main_run_folder)
        if not os.path.exists(run_dir_i): # create parallel folder if not exists
            os.makedirs(run_dir_i)
            os.makedirs(pjoin(run_dir_i,"log"))
            shutil.copy(pjoin(run_dir_0,"file_parameter.dat"),run_dir_i)
            for basename in os.listdir(pjoin(run_dir_0,"log")):
                if basename.endswith('.in'):
                    pathname = pjoin(pjoin(run_dir_0,"log"), basename)
                    if os.path.isfile(pathname):
                        shutil.copy2(pathname, pjoin(run_dir_i,"log"))
        # file_parameter.dat is now fully created here: remove previous in order to remove previous settings
        try:
            os.remove(pjoin(run_dir_i,"file_parameter.dat"))
        except:
            pass
        # # set inputs for resummation first, because of type order for the weight path
#{{{ inputs for resummation
        # decide wether NLO or NNLO folder
        if "%s/NLO" % fold.run_folder_path in run_dir_i and parameter_list.get("add_NLL","0") == "1":# NLO folder
            for contribution in resummation_map_NLO:
                if contribution in run_dir_i:
                    contribution_map = resummation_map_NLO[contribution]
                    inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"user_switch do_resummation","1")
                    inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"type_perturbative_order","NLO")
                    inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"type_contribution",contribution_map)
                    inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"type_correction","QCD")
                    if contribution == "born":
                        inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"contribution_order_alpha_s",self.get_coupling_order_QCD("LO"))
                    elif contribution == "VT.QCD":
                        inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"contribution_order_alpha_s",self.get_coupling_order_QCD("NLO"))
                    inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"contribution_order_alpha_e",self.get_coupling_order_EW("LO"))
                    inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"contribution_interference","0")
                    self.get_coupling_order_EW("LO")
        elif "%s/NNLO" % fold.run_folder_path in run_dir_i and parameter_list.get("add_NNLL","0") == "1":# NNLO folder
            for contribution in resummation_map_NNLO:
                if contribution in run_dir_i:
                    contribution_map = resummation_map_NNLO[contribution]
                    inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"user_switch do_resummation","1")
                    inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"type_perturbative_order","NNLO")
                    inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"type_contribution",contribution_map)
                    inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"type_correction","QCD")
                    if contribution == "born":
                        inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"contribution_order_alpha_s",self.get_coupling_order_QCD("LO"))
                    elif contribution == "VT.QCD":
                        inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"contribution_order_alpha_s",self.get_coupling_order_QCD("NLO"))
                    elif contribution == "VT2.QCD":
                        inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"contribution_order_alpha_s",self.get_coupling_order_QCD("NNLO"))
                    inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"contribution_order_alpha_e",self.get_coupling_order_EW("LO"))
                    inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"contribution_interference","0")
                    self.get_coupling_order_EW("LO")
#}}}

        # these values are hard-coded and may be adjust by experience:
        n_events_min = math.ceil(self.get_pre_run_min_events_for_contribution(run_dir_0)/self.get_parallel_pre_runs_for_contribution(run_dir_0)/10)*10
        n_events_max = math.ceil(self.get_pre_run_min_events_for_contribution(run_dir_0)/self.get_parallel_pre_runs_for_contribution(run_dir_0)/10)*10
        n_step = math.ceil(self.get_pre_run_min_events_for_contribution(run_dir_0)/self.get_parallel_pre_runs_for_contribution(run_dir_0)/10)
        sigma_normalization = 1
        sigma_normalization_deviation = 0
        # set the grid that has to be used
        inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"MCweight_in_directory",os.path.relpath(self.grid_dirs_for_run_dirs[run_dir_0],pjoin(process_dir,run_folder)))
        # set the number of the run folder (zwahl=seed);
        inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"zwahl",str(int(parameter_list.get("random_seed","0"))+parallel_folder_i))
        # set new run parameters
        inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"n_events_min",str(n_events_min))
        inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"n_events_max",str(n_events_max))
        inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"n_step",str(n_step))
        inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"sigma_normalization",str(sigma_normalization))
        inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"sigma_normalization_deviation",str(sigma_normalization_deviation))
        if parameter_list.get("reduce_workload",0) == "2":
            inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"output_level","WARN")
        if parameter_list.get("improve_mappings_for_single_V","0") == "1":
            inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"switch_MC_tau","2")
            inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"switch_MC_x_dipole","2")
        if "RRA" in run_dir_i:
            inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"cut_technical","1.e-10")
            inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"switch_console_output_techcut_RA","0")
        if "L2RA" in run_dir_i or "L2RT" in run_dir_i:
            inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"OL stability_mode","21")
        if "L2RA" in run_dir_i:
            inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"OL stability_kill2","1.e-2")
        if "L2RT" in run_dir_i:
            inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"OL stability_kill2","1.")
#}}}
#{{{ def: set_inputs_main_run(self)
    def set_inputs_main_run(self,run_dir_i,channel,parallel_folder_i):
    # this function sets the default inputs for the pre run, that extrapolates the runtimes
        # first: create new run_dir_i by copying the main_run_folder (run.0)
        run_dir_0 = run_dir_i.replace("run.%s" % parallel_folder_i,main_run_folder)
        if not os.path.exists(run_dir_i): # create parallel folder if not exists
            os.makedirs(run_dir_i) 
            os.makedirs(pjoin(run_dir_i,"log"))
#            shutil.copy(pjoin(run_dir_0,"file_parameter.dat"),run_dir_i)
            for basename in os.listdir(pjoin(run_dir_0,"log")):
                if basename.endswith('.in'):
                    pathname = pjoin(pjoin(run_dir_0,"log"), basename)
                    if os.path.isfile(pathname):
                        shutil.copy2(pathname, pjoin(run_dir_i,"log"))
#            shutil.copytree(pjoin(run_dir_0,"log"),run_dir_i,symlinks=True) # keep symlinks !!!
#{{{ inputs for resummation
        # decide wether NLO or NNLO folder
        if "%s/NLO" % fold.run_folder_path in run_dir_i and parameter_list.get("add_NLL","0") == "1":# NLO folder
            for contribution in resummation_map_NLO:
                if contribution in run_dir_i:
                    contribution_map = resummation_map_NLO[contribution]
                    inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"user_switch do_resummation","1")
                    inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"type_perturbative_order","NLO")
                    inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"type_contribution",contribution_map)
                    inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"type_correction","QCD")
                    if contribution == "born":
                        inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"contribution_order_alpha_s",self.get_coupling_order_QCD("LO"))
                    elif contribution == "VT.QCD":
                        inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"contribution_order_alpha_s",self.get_coupling_order_QCD("NLO"))
                    inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"contribution_order_alpha_e",self.get_coupling_order_EW("LO"))
                    inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"contribution_interference","0")
                    self.get_coupling_order_EW("LO")
        elif "%s/NNLO" % fold.run_folder_path in run_dir_i and parameter_list.get("add_NNLL","0") == "1":# NNLO folder
            for contribution in resummation_map_NNLO:
                if contribution in run_dir_i:
                    contribution_map = resummation_map_NNLO[contribution]
                    inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"user_switch do_resummation","1")
                    inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"type_perturbative_order","NNLO")
                    inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"type_contribution",contribution_map)
                    inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"type_correction","QCD")
                    if contribution == "born":
                        inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"contribution_order_alpha_s",self.get_coupling_order_QCD("LO"))
                    elif contribution == "VT.QCD":
                        inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"contribution_order_alpha_s",self.get_coupling_order_QCD("NLO"))
                    elif contribution == "VT2.QCD":
                        inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"contribution_order_alpha_s",self.get_coupling_order_QCD("NNLO"))
                    inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"contribution_order_alpha_e",self.get_coupling_order_EW("LO"))
                    inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"contribution_interference","0")
                    self.get_coupling_order_EW("LO")
#}}}        
        # set the grid that has to be used
        inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"MCweight_in_directory",os.path.relpath(self.grid_dirs_for_run_dirs[run_dir_0],pjoin(process_dir,run_folder)))
        # set the number of the run folder (zwahl=seed);
        inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"zwahl",str(int(parameter_list.get("random_seed","0"))+parallel_folder_i))
        if parameter_list.get("reduce_workload",0) == "2":
            inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"output_level","WARN")
        if parameter_list.get("improve_mappings_for_single_V","0") == "1":
            inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"switch_MC_tau","2")
            inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"switch_MC_x_dipole","2")
        if "L2RA" in run_dir_i or "L2RT" in run_dir_i:
            inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"OL stability_mode","21")
        if "L2RA" in run_dir_i:
            inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"OL stability_kill2","1.e-2")
        if "L2RT" in run_dir_i:
            inp.input_set_entry(pjoin(run_dir_i,"file_parameter.dat"),"OL stability_kill2","1.")

        param_file_path = pjoin(run_dir_i,"log","file_parameter."+channel+".dat")
        # create 'channel'_file_parameter.dat for each channel in log folder if it does not exist
        if not os.path.isfile(param_file_path):
            open(param_file_path, 'a').close()
        # set new run parameters
        inp.input_set_entry(param_file_path,"n_events_min",self.runtime_table[run_dir_0][channel]["n_events_min"])
        inp.input_set_entry(param_file_path,"n_events_max",self.runtime_table[run_dir_0][channel]["n_events_max"])
        inp.input_set_entry(param_file_path,"n_step",self.runtime_table[run_dir_0][channel]["n_step"])
        inp.input_set_entry(param_file_path,"sigma_normalization",self.runtime_table[run_dir_0][channel]["sigma_normalization"])
        inp.input_set_entry(param_file_path,"sigma_normalization_deviation",self.runtime_table[run_dir_0][channel]["sigma_normalization_deviation"])
        # set the number of the run folder (zwahl=seed);
        inp.input_set_entry(param_file_path,"zwahl",str(int(parameter_list.get("random_seed","0"))+parallel_folder_i))
#}}}
#{{{ def: log_full_of_errors(self,subprocess_out_path)
    def log_full_of_errors(self,subprocess_out_path):
    # this function is for checking wether there are many errors in log file, so that job can be restarted
        outcome = False
        tmp_file = subprocess_out_path+".copy"
        shutil.copy(subprocess_out_path,tmp_file)
        count = 0
        with open(tmp_file) as f:
            for line in f:
                if "ERROR" in line.upper():
                    count += 1
        if count > 10:
            print "count: "+str(count)
        if count > 1000: outcome = True
        os.remove(tmp_file)
        return outcome
#}}}
#{{{ def: log_keyword_end_of_file(self,subprocess_out_path,keyword)
    def log_keyword_end_of_file(self,subprocess_out_path,keyword):
    # this function is for checking wether the last line of log has a specific key word, so that job can be restarted
        outcome = False
        tmp_file = subprocess_out_path+".copy"
        shutil.copy(subprocess_out_path,tmp_file)
        with open(tmp_file) as f:
            for line in f:
                pass
            last = line.strip()
        if last == keyword: outcome = True
#        print "last line of log: "+last
#        print "keyword: "+keyword
        os.remove(tmp_file)
        return outcome
#}}}
#{{{ def: job_correctly_finished(self,run_path,process)
    def job_correctly_finished(self,run_path,process):
    # this function checks wether the job in the given folder for the given process was correctly finished
        outcome = False
        # in this file is the indicator wether the job was successfully finished
        file_name = pjoin(run_path,"execution","execution_"+process+".dat")
        if os.path.isfile(file_name):
            with open(file_name, 'r') as finish_file:
                # read in the file into single string and remove leading and trailing spaces (including tabs, enters)
                finish_line = finish_file.readline().strip()
                # the first line in this file has to state "final result" if job was correctly finished
                if finish_line == "final result":
                    outcome = True
        return outcome
#}}}
#{{{ def: jobs_in_failed_list(self)
    def jobs_in_failed_list(self):
        outcome = False
        DIR = pjoin(process_dir,"log",run_folder,"failed")
        if len([name for name in os.listdir(DIR) if os.path.isfile(pjoin(DIR, name))])>0: outcome = True
        return outcome
#}}}
#{{{ def: grid_run_complete(self)
    def grid_run_complete(self):
        outcome = True
        failed_dir = pjoin(process_dir,"log",run_folder,"grid_run","failed")
        success_dir = pjoin(process_dir,"log",run_folder,"grid_run","successful")
        self.expected_nr_of_grid_runs = 0
        for grid_dir in self.grid_dirs: # normal dirs
            self.expected_nr_of_grid_runs += len(self.channels[grid_dir])
        if self.include_loop_induced:
            for grid_dir in self.loop_grid_dirs: # normal dirs
                self.expected_nr_of_grid_runs += len(self.loop_channels[grid_dir])
        if not os.path.isdir(failed_dir) or not os.path.isdir(success_dir):
            self.successful_nr_of_grid_runs = 0
            self.failed_nr_of_grid_runs = 0
            outcome = False
            return outcome
            
        self.successful_nr_of_grid_runs = len([name for name in os.listdir(success_dir) if os.path.isfile(pjoin(success_dir, name))])
        self.failed_nr_of_grid_runs = len([name for name in os.listdir(failed_dir) if os.path.isfile(pjoin(failed_dir, name))])
        if self.expected_nr_of_grid_runs != self.successful_nr_of_grid_runs: outcome = False

        # check if final result is in the grid folders
        for grid_dir in self.grid_dirs:
            for channel in self.channels[grid_dir]:
                if self.phase == 2 or continue_run:
                    if not self.job_correctly_finished(grid_dir,channel):
                        self.failed_run_list.append(pjoin(grid_dir,"log",channel))
                        outcome = False
        if self.include_loop_induced:
            for grid_dir in self.loop_grid_dirs: # loop dirs
                fold.add_dir_identifier(grid_dir,"grid")
                for channel in self.loop_channels[grid_dir]:
                    if not self.job_correctly_finished(grid_dir,channel):
                        self.failed_run_list.append(pjoin(grid_dir,"log",channel))
                        outcome = False

        return outcome
#}}}
#{{{ def: pre_run_complete(self)
    def pre_run_complete(self):
        outcome = True
        # check if final result is in the pre run folders
        for run_dir in self.run_dirs: # normal dirs
            for channel in self.channels[self.grid_dirs_for_run_dirs[run_dir]]: # loop through channels
                parallel_runs = self.get_parallel_pre_runs_for_contribution(run_dir)
                for k in range(0,parallel_runs): # for parallel running of same contributions
                    i=k
                    if not self.job_correctly_finished(run_dir.replace(main_run_folder,"run.%s" % i),channel):
                        self.failed_run_list.append(pjoin(run_dir.replace(main_run_folder,"run.%s" % i),"log",channel))
                        outcome = False
        if self.include_loop_induced:
            for run_dir in self.loop_run_dirs: # loop dirs
              for channel in self.loop_channels[self.grid_dirs_for_run_dirs[run_dir]]: # loop through channels
                parallel_runs = self.get_parallel_pre_runs_for_contribution(run_dir)
                for k in range(0,parallel_runs): # for parallel running of same contributions
                    i=k # this is to use run.0-parallel_runs_of_pre for the extrapolation run
                    if not self.job_correctly_finished(run_dir.replace(main_run_folder,"run.%s" % i),channel):
                        self.failed_run_list.append(pjoin(run_dir.replace(main_run_folder,"run.%s" % i),"log",channel))
                        outcome = False
        return outcome
#}}}
#{{{ def: main_run_complete(self)
    def main_run_complete(self):
        outcome = True
        # check if final result is in the main run folders
        for run_dir in self.run_dirs: # normal dirs
            for channel in self.channels[self.grid_dirs_for_run_dirs[run_dir]]: # loop through channels
                parallel_runs = self.runtime_table[run_dir][channel]["parallel_jobs"]
                for k in range(0,parallel_runs): # for parallel running of same contributions
                    i=k+self.get_parallel_pre_runs_for_contribution(run_dir) # this is to use run.parallel_runs_of_pre and onwards for the main runs
                    if not self.job_correctly_finished(run_dir.replace(main_run_folder,"run.%s" % i),channel):
                        self.failed_run_list.append(pjoin(run_dir.replace(main_run_folder,"run.%s" % i),"log",channel))
                        outcome = False
        if self.include_loop_induced:
            for run_dir in self.loop_run_dirs: # loop dirs
              for channel in self.loop_channels[self.grid_dirs_for_run_dirs[run_dir]]: # loop through channels
                parallel_runs = self.runtime_table[run_dir][channel]["parallel_jobs"]
                for k in range(0,parallel_runs): # for parallel running of same contributions
                    i=k+self.get_parallel_pre_runs_for_contribution(run_dir) # this is to use run.parallel_runs_of_pre and onwards for the main runs
                    if not self.job_correctly_finished(run_dir.replace(main_run_folder,"run.%s" % i),channel):
                        self.failed_run_list.append(pjoin(run_dir.replace(main_run_folder,"run.%s" % i),"log",channel))
                        outcome = False
        return outcome
#}}}
#{{{ def: write_job_log_start(self,run_path,process,string)
    def write_job_log_start(self,run_path,process,string):
    # this function creates a new log file for the given run_path and process, and writes a 
    # starting line form "path: run_path, process: process >>> string"
        log_folder = pjoin(process_dir,"log",run_folder)
        # use relative path of current run so that printout is not so long
        rel_path = os.path.relpath(run_path,pjoin(process_dir,run_folder))
        log_file = open(pjoin(log_folder,rel_path.replace("/","-")+">>"+process+".log"), 'w')
        try:
            log_file.write("path: %s, process: %s >>> %s\n"%(rel_path,process,string))
        finally:
            log_file.close()
#}}}
#{{{ def: write_job_log(self,run_path,process,string)
    def write_job_log(self,run_path,process,string):
    # this function writes into a (existing) log file and adds one line of the form "process >>> string"
        log_folder = pjoin(process_dir,"log",run_folder)
        # use relative path of current run so that printout is not so long
        rel_path = os.path.relpath(run_path,pjoin(process_dir,run_folder))
        log_file = open(pjoin(log_folder,rel_path.replace("/","-")+">>"+process+".log"), 'a')
        try:
            log_file.write("path: %s, process: %s >>> %s\n"%(rel_path,process,string))
        finally:
            log_file.close()
#}}}
#{{{ def: write_job_log_start(self,run_path,process,string)
    def write_job_log_finish(self,run_path,process,string):
    # this function creates a new log file for the given run_path and process, and writes a 
    # starting line form "path: run_path, process: process >>> string"
        log_folder = pjoin(process_dir,"log",run_folder)
        # use relative path of current run so that printout is not so long
        rel_path = os.path.relpath(run_path,pjoin(process_dir,run_folder))
        log_file = open(pjoin(log_folder,rel_path.replace("/","-")+">>"+process+".log"), 'a')
        try:
            log_file.write("path: %s, process: %s >>> %s\n"%(rel_path,process,string))
            log_file.write("-" * len("path: %s, process: %s >>> %s\n"%(rel_path,process,string))+"\n")
        finally:
            log_file.close()
#}}}
#{{{ def: add_job_success_log(self,run_path,process)
    def add_job_success_log(self,run_path,process):
    # this function adds a job to the list of successful jobs inside the log folder
        success_folder = pjoin(process_dir,"log",run_folder,"successful")
        # use relative path of current run so that printout is not so long
        rel_path = os.path.relpath(run_path,pjoin(process_dir,run_folder))
        success_file = open(pjoin(success_folder,rel_path.replace("/","-")+">>"+process+".success"), 'w')
        try:
            success_file.write("job done")
        finally:
            success_file.close()
#}}}
#{{{ def: add_job_failed_log(self,run_path,process)
    def add_job_failed_log(self,run_path,process):
    # this function adds a job to the list of failed jobs inside the log folder
        failed_folder = pjoin(process_dir,"log",run_folder,"failed")
        # use relative path of current run so that printout is not so long
        rel_path = os.path.relpath(run_path,pjoin(process_dir,run_folder))
        failed_file = open(pjoin(failed_folder,rel_path.replace("/","-")+">>"+process+".failed"), 'w')
        try:
            failed_file.write("job failed")
        finally:
            failed_file.close()
#}}}
#{{{ def: remove_job_success_log(self,run_path,process)
    def remove_job_success_log(self,run_path,process):
    # this function removes a job to the list of successful jobs inside the log folder
        success_folder = pjoin(process_dir,"log",run_folder,"successful")
        rel_path = os.path.relpath(run_path,pjoin(process_dir,run_folder))
        os.remove(pjoin(success_folder,rel_path.replace("/","-")+">>"+process+".success"))
#}}}
#{{{ def: remove_job_failed_log(self,run_path,process)
    def remove_job_failed_log(self,run_path,process):
    # this function removes a job to the list of failed jobs inside the log folder
        failed_folder = pjoin(process_dir,"log",run_folder,"failed")
        rel_path = os.path.relpath(run_path,pjoin(process_dir,run_folder))
        os.remove(pjoin(failed_folder,rel_path.replace("/","-")+">>"+process+".failed"))
#}}}
#{{{ def: read_channel_file(self,file_path)
# all channels for a specific folder are read from the channel file and returned
    def read_channel_file(self,file_path):
        contribution = self.get_contribution(file_path)
        channel={}
        with open(file_path) as f:
            channel = [x.strip('\n').replace("x","~") for x in f.readlines() if not self.exclude_channel(x,contribution)]
        return channel
#}}}
#{{{ def: get_dir_channels(self,path)
# returns a list which associates each directory with its channels
    def exclude_channel(self,channel,contribution):
        if (contribution == "L2RA.QCD" or contribution == "L2RT.QCD") and parameter_list.get("only_gg_chan",0) == "1" and not channel.startswith("gg"):
            return True
        if "b" in channel and parameter_list.get("flavor_scheme") == "1":
            return True
        if prc.process_name in ckm_processes: # for charged processes with CKM matrix exclude vanishing channels
            Wsplitting = ""
            for quark in ["u","d","c","s","t","b"]:
                if not channel.count(quark) % 2 == 0:
                    Wsplitting = Wsplitting + quark
                reverseWsplitting = Wsplitting[::-1]
                zero_splittings = []
                if "CKM" in model_list:
                    for number in model_list["CKM"]:
                        if float(model_list["CKM"][number]) == 0:
                            zero_splittings.append(model_mappings_to_MUNICH["CKM"][number][2:4])
                elif "VCKMIN" in model_list:
                    zero_splittings = ["ub","cb","td","ts","tb"] # 3rd generation mixing turned off
                    if abs(float(model_list["VCKMIN"][1])) < 0.0000000001:
                        zero_splittings.append("us")
                        zero_splittings.append("cd")
                    if abs(float(model_list["VCKMIN"][1]) - math.pi) < 0.0000000001:
                        zero_splittings.append("ud")
                        zero_splittings.append("cs")
                else:
                    off_diagonal = ["us","ub","cd","cb","td","ts"]
                    zero_splittings = off_diagonal
                if Wsplitting in zero_splittings or reverseWsplitting in zero_splittings:
                    return True
        return False
#}}}
#{{{ def: get_dir_channels(self,path)
# returns a list which associates each directory with its channels
    def get_dir_channels(self,dirs):
        channel={}
        for dir in dirs:
            chans = self.read_channel_file(pjoin(dir.rsplit('/',1)[0],"subprocesslist.dat"))
            channel[dir]=chans
        return channel
#}}}
# remove??? not used and not debugged after adding CKM
#{{{ def: get_channels(self,path)
# for cross checks get all channels
    def get_channels(self,path):
        channel=[]
        for file in os.listdir(path):
            if file.endswith(".in"):
                if parameter_list.get("flavor_scheme") == "1" and "b" in file: # exclude b's in 4FS (needed for WW and ZZ/WW)
                    continue
                if prc.process_name in ckm_processes: # for charged processes with CKM matrix exclude vanishing channels
                    Wsplitting = ""
                    for quark in ["u","d","c","s","t","b"]:
                        if not file.count(quark) % 2 == 0:
                            Wsplitting = Wsplitting + quark
                    reverseWsplitting = Wsplitting[::-1]
                    zero_splittings = []
                    if "CKM" in model_list:
                        for number in model_list["CKM"]:
                            if float(model_list["CKM"][number]) == 0:
                                zero_splittings.append(model_mappings_to_MUNICH["CKM"][number][2:3])
                    elif "VCKMIN" in SLHA_list:
                        zero_splittings = ["ub","cb","td","ts","tb"] # 3rd generation mixing turned off
                        if abs(float(model_list["VCKMIN"][1])) < 0.0000000001:
                            zero_splittings.append("us")
                            zero_splittings.append("cd")
                        if abs(float(model_list["VCKMIN"][1]) - math.pi) < 0.0000000001:
                            zero_splittings.append("ud")
                            zero_splittings.append("cs")
                    else:
                        off_diagonal = ["us","ub","cd","cb","td","ts"]
                        zero_splittings = off_diagonal
                    if Wsplitting in zero_splittings or reverseWsplitting in zero_splittings:
                        continue
                channel.append(file.replace(".in",""))
        return channel
#}}}
# remove??? not used and not debugged after adding CKM
#{{{ def: get_dir_channels_old(self,path)
# all channels for a specific folder are return
    def get_dir_channels_old(self,dirs):
        channel={}
        for dir in dirs:
            chans = self.get_channels(pjoin(dir,"log"))
            channel[dir]=chans
        return channel
#}}}
#{{{ def: get_all_channels_test(self)
# for cross checks define all channels
    def get_all_channels_test(self):
        # I think this routine is not needed anymore, remove?
        LO_channels = []
        NLO_channels = []
        NNLO_channels = []

#        LO_channels = self.get_channels(pjoin(process_dir,run_folder,"LO/03"))

        # LO_channels.append("dd~_emepa")
        # LO_channels.append("uu~_emepa")

        # NLO_channels.append("dd~_emepag")
        # NLO_channels.append("uu~_emepag")
        # NLO_channels.append("gd_emepad")
        # NLO_channels.append("gu_emepau")
        # NLO_channels.append("gd~_emepad~")
        # NLO_channels.append("gu~_emepau~")

        # NNLO_channels.append("dd_emepadd")
        # NNLO_channels.append("dd~_emepadd~")
        # NNLO_channels.append("dd~_emepauu~")
        # NNLO_channels.append("dd~_emepacc~")
        # NNLO_channels.append("dd~_emepass~")
        # NNLO_channels.append("dd~_emepagg")
        # NNLO_channels.append("uu_emepauu")
        # NNLO_channels.append("uu~_emepauu~")
        # NNLO_channels.append("uu~_emepadd~")
        # NNLO_channels.append("uu~_emepacc~")
        # NNLO_channels.append("uu~_emepass~")
        # NNLO_channels.append("uu~_emepagg")
        # NNLO_channels.append("du_emepadu")
        # NNLO_channels.append("du~_emepadu~")
        # NNLO_channels.append("d~u~_emepad~u~")
        # NNLO_channels.append("dc_emepadc")
        # NNLO_channels.append("dc~_emepadc~")
        # NNLO_channels.append("d~c~_emepad~c~")
        # NNLO_channels.append("ds_emepads")
        # NNLO_channels.append("ds~_emepads~")
        # NNLO_channels.append("d~s~_emepad~s~")
        # NNLO_channels.append("d~d~_emepad~d~")
        # NNLO_channels.append("ud~_emepaud~")
        # NNLO_channels.append("uc_emepauc")
        # NNLO_channels.append("uc~_emepauc~")
        # NNLO_channels.append("us~_emepaus~")
        # NNLO_channels.append("u~c~_emepau~c~")
        # NNLO_channels.append("u~u~_emepau~u~")
        # NNLO_channels.append("gu_emepagu")
        # NNLO_channels.append("gu~_emepagu~")
        # NNLO_channels.append("gd_emepagd")
        # NNLO_channels.append("gd~_emepagd~")
        # NNLO_channels.append("gg_emepadd~")
        # NNLO_channels.append("gg_emepauu~")

        return LO_channels, NLO_channels, NNLO_channels
#}}}
#{{{ def: get_dirs(self,folder_name,NLO_subtraction,order)
    def get_dirs(self,folder_name,NLO_subtraction,order):
    # new: use hard-coded paths to grid folders you want to be created
        out_dirs = []
        loop_out_dirs = []

        if "LO" in order or (folder_name == "grid" and "NLO" in order) or (folder_name == "grid" and "NNLO" in order and not int(parameter_list["loop_induced"]) < 0): # in grid run we always need LO
            out_dirs.append(pjoin(fold.run_folder_path,"LO",self.get_coupling_order("LO"),"born",folder_name))
            if parameter_list.get("photon_induced","0") == "1" and prc.process_name in LO_photon_induced_processes:
                out_dirs.append(pjoin(fold.run_folder_path,"LO","a"+self.get_coupling_order("LO"),"born",folder_name))
        if "NLO" in order or (folder_name == "grid" and "NNLO" in order and not int(parameter_list["loop_induced"]) < 0):
            # QCD part
            if parameter_list.get("run_NLO_QCD","0") == "1" or parameter_list.get("run_NNLO_QCD","0") == "1":
                if "CS" in NLO_subtraction:
                    out_dirs.append(pjoin(fold.run_folder_path,"NLO.CS",self.get_coupling_order("NLO"),"RA.QCD",folder_name))
                    out_dirs.append(pjoin(fold.run_folder_path,"NLO.CS",self.get_coupling_order("NLO"),"CA.QCD",folder_name))
                elif "QT" in NLO_subtraction:
                    out_dirs.append(pjoin(fold.run_folder_path,"NLO.QT",self.get_coupling_order("NLO"),"RT.QCD",folder_name))
                else:
                    out.print_error("NLO_subtraction \"%s\"in get_dirs routine does not exist. Stopping the code..." % NLO_subtraction)
            # EW part
            if parameter_list.get("run_NLO_EW","0") == "1" or parameter_list.get("add_NLO_EW","0") == "1":
                out_dirs.append(pjoin(fold.run_folder_path,"NLO.CS",self.get_coupling_order("LO","NLO"),"RA.QEW",folder_name))
                out_dirs.append(pjoin(fold.run_folder_path,"NLO.CS",self.get_coupling_order("LO","NLO"),"CA.QEW",folder_name))
                if parameter_list.get("photon_induced","0") == "1":
                    out_dirs.append(pjoin(fold.run_folder_path,"NLO.CS","a"+self.get_coupling_order("LO","NLO"),"RA.QEW",folder_name))
                    out_dirs.append(pjoin(fold.run_folder_path,"NLO.CS","a"+self.get_coupling_order("LO","NLO"),"CA.QEW",folder_name))
        if "NNLO" in order:
            if not int(parameter_list["loop_induced"]) < 0:
                out_dirs.append(pjoin(fold.run_folder_path,"NNLO.QT-CS",self.get_coupling_order("NNLO"),"RRA.QCD",folder_name))
                out_dirs.append(pjoin(fold.run_folder_path,"NNLO.QT-CS",self.get_coupling_order("NNLO"),"RCA.QCD",folder_name))
                if folder_name.startswith("grid") and "CS" in NLO_subtraction: # use RT contribution of QT subtraction only for the grids at NNLO
                    out_dirs.append(pjoin(fold.run_folder_path,"NLO.QT",self.get_coupling_order("NLO"),"RT.QCD",folder_name))
                # add loop-induced only if the contribution exists
            if os.path.exists(pjoin(fold.run_folder_path,"NNLO",self.get_coupling_order("NNLO"),"loop")):
                loop_out_dirs.append(pjoin(fold.run_folder_path,"NNLO",self.get_coupling_order("NNLO"),"loop",folder_name))
                if abs(int(parameter_list["loop_induced"])) == 2:
                    if "CS" in NLO_subtraction:
                        loop_out_dirs.append(pjoin(fold.run_folder_path,"NNNLO.CS",self.get_coupling_order("NNNLO"),"L2RA.QCD",folder_name))
                        loop_out_dirs.append(pjoin(fold.run_folder_path,"NNNLO.CS",self.get_coupling_order("NNNLO"),"L2CA.QCD",folder_name))
                    elif "QT" in NLO_subtraction:
                        loop_out_dirs.append(pjoin(fold.run_folder_path,"NNNLO.QT",self.get_coupling_order("NNNLO"),"L2RT.QCD",folder_name))
                    else:
                        out.print_error("NLO_subtraction \"%s\"in get_dirs routine does not exist. Stopping the code..." % NLO_subtraction)
        if folder_name.startswith("run"):
            if "NLO" in order:
                out_dirs.append(pjoin(fold.run_folder_path,"NLO",self.get_coupling_order("LO"),"born",folder_name))
                # QCD part
                if parameter_list.get("run_NLO_QCD","0") == "1":
                    if parameter_list.get("photon_induced","0") == "1" and prc.process_name in LO_photon_induced_processes:
                        out_dirs.append(pjoin(fold.run_folder_path,"NLO","a"+self.get_coupling_order("LO"),"born",folder_name))
                    if parameter_list.get("run_NLO_QCD","0") == "1":
                        if "CS" in NLO_subtraction:
                            out_dirs.append(pjoin(fold.run_folder_path,"NLO.CS",self.get_coupling_order("NLO"),"VA.QCD",folder_name))
                        elif "QT" in NLO_subtraction:
                            out_dirs.append(pjoin(fold.run_folder_path,"NLO.QT",self.get_coupling_order("NLO"),"VT.QCD",folder_name))
                            out_dirs.append(pjoin(fold.run_folder_path,"NLO.QT",self.get_coupling_order("NLO"),"CT.QCD",folder_name))
                        else:
                            out.print_error("NLO_subtraction \"%s\"in get_dirs routine does not exist. Stopping the code..." % NLO_subtraction)
                # EW part
                if parameter_list.get("run_NLO_EW","0") == "1":
                    out_dirs.append(pjoin(fold.run_folder_path,"NLO.CS",self.get_coupling_order("LO","NLO"),"VA.QEW",folder_name))
                    if parameter_list.get("photon_induced","0") == "1" and prc.process_name in LO_photon_induced_processes:
                        out_dirs.append(pjoin(fold.run_folder_path,"NLO.CS","a"+self.get_coupling_order("LO","NLO"),"VA.QEW",folder_name))
            if "NNLO" in order:
                if not int(parameter_list["loop_induced"]) < 0:
                    out_dirs.append(pjoin(fold.run_folder_path,"NNLO",self.get_coupling_order("LO"),"born",folder_name))
                    if parameter_list.get("photon_induced","0") == "1" and prc.process_name in LO_photon_induced_processes:
                        out_dirs.append(pjoin(fold.run_folder_path,"NNLO","a"+self.get_coupling_order("LO"),"born",folder_name))
                    if "CS" in NLO_subtraction:
                        out_dirs.append(pjoin(fold.run_folder_path,"NNLO.CS",self.get_coupling_order("NLO"),"RA.QCD",folder_name))
                        out_dirs.append(pjoin(fold.run_folder_path,"NNLO.CS",self.get_coupling_order("NLO"),"CA.QCD",folder_name))
                        out_dirs.append(pjoin(fold.run_folder_path,"NNLO.CS",self.get_coupling_order("NLO"),"VA.QCD",folder_name))
                    elif "QT" in NLO_subtraction:
                        out_dirs.append(pjoin(fold.run_folder_path,"NNLO.QT",self.get_coupling_order("NLO"),"RT.QCD",folder_name))
                        out_dirs.append(pjoin(fold.run_folder_path,"NNLO.QT",self.get_coupling_order("NLO"),"CT.QCD",folder_name))
                        out_dirs.append(pjoin(fold.run_folder_path,"NNLO.QT",self.get_coupling_order("NLO"),"VT.QCD",folder_name))
                    else:
                        out.print_error("NLO_subtraction \"%s\"in get_dirs routine does not exist. Stopping the code..." % NLO_subtraction)
                    out_dirs.append(pjoin(fold.run_folder_path,"NNLO.QT-CS",self.get_coupling_order("NNLO"),"RVA.QCD",folder_name))
                    out_dirs.append(pjoin(fold.run_folder_path,"NNLO.QT-CS",self.get_coupling_order("NNLO"),"CT2.QCD",folder_name))
                    out_dirs.append(pjoin(fold.run_folder_path,"NNLO.QT-CS",self.get_coupling_order("NNLO"),"VT2.QCD",folder_name))
                    if parameter_list.get("add_NLO_EW","0") == "1":
                        out_dirs.append(pjoin(fold.run_folder_path,"NNLO.CS",self.get_coupling_order("LO","NLO"),"RA.QEW",folder_name))
                        out_dirs.append(pjoin(fold.run_folder_path,"NNLO.CS",self.get_coupling_order("LO","NLO"),"CA.QEW",folder_name))
                        out_dirs.append(pjoin(fold.run_folder_path,"NNLO.CS",self.get_coupling_order("LO","NLO"),"VA.QEW",folder_name))
                        if parameter_list.get("photon_induced","0") == "1":
                            out_dirs.append(pjoin(fold.run_folder_path,"NNLO.CS","a"+self.get_coupling_order("LO","NLO"),"RA.QEW",folder_name))
                            out_dirs.append(pjoin(fold.run_folder_path,"NNLO.CS","a"+self.get_coupling_order("LO","NLO"),"CA.QEW",folder_name))
                            if prc.process_name in LO_photon_induced_processes:
                                out_dirs.append(pjoin(fold.run_folder_path,"NNLO.CS","a"+self.get_coupling_order("LO","NLO"),"VA.QEW",folder_name))
                if os.path.exists(pjoin(fold.run_folder_path,"NNLO",self.get_coupling_order("NNLO"),"loop")):
                    if abs(int(parameter_list["loop_induced"])) == 2:
                        if "CS" in NLO_subtraction:
                            loop_out_dirs.append(pjoin(fold.run_folder_path,"NNNLO.CS",self.get_coupling_order("NNNLO"),"L2VA.QCD",folder_name))
                        elif "QT" in NLO_subtraction:
                            loop_out_dirs.append(pjoin(fold.run_folder_path,"NNNLO.QT",self.get_coupling_order("NNNLO"),"L2CT.QCD",folder_name))
                            loop_out_dirs.append(pjoin(fold.run_folder_path,"NNNLO.QT",self.get_coupling_order("NNNLO"),"L2VT.QCD",folder_name))
                        else:
                            out.print_error("NLO_subtraction \"%s\"in get_dirs routine does not exist. Stopping the code..." % NLO_subtraction)
        return out_dirs, loop_out_dirs
#}}}
#{{{ def: get_named_dirs(self,folder_name,NLO_subtraction,order,switch)
    def get_named_dirs(self,folder_name,NLO_subtraction,order,switch):
    # get all folders that contain the desired folder_name, but only 
    # with the desired NLO subtraction.
    # switch = 1 for grid folders, switch = 2 for main run
        out_dirs = []
        loop_out_dirs = []
        startinglevel = pjoin(process_dir,run_folder).count(os.sep)
        for root, dirnames, filenames in os.walk(pjoin(process_dir,run_folder)):
            level = root.count(os.sep) - startinglevel
            # only keep folders <= order for the warmup (switch=1: grid setup) run
            if switch == 1 and not "NNLO" in order and "/NNLO" in root:
                continue
            if switch == 1 and not "NLO" in order and not "NNLO" in order and "/NLO" in root:
                continue
            # only keep folders == order for cross section computation (switch=2: main run)
            rel_root = "/"+os.path.relpath(root,pjoin(process_dir,run_folder))
            if switch == 2 and not any("/"+n_order in rel_root for n_order in order):
                continue
            # only select folders which use the NLO_subtraction which was set before
            # this is true for both grid and run folders
            if ".QT" in root or ".CS" in root:
                if not NLO_subtraction in root:
                # only if not NNLO or for switch!=1 (not in grid run), because NLO.QT/*/RT.QCD grids required for NNLO RVA.QCD and RCA.QCD
                    if not switch == 1 or not "NNLO" in order:
                        continue 
            # consider grid folder only on specific directory depth (level==3)
            # relative to the process main run folder
            if level==3:
                for folder in fnmatch.filter(dirnames,folder_name):
                    out_dir = pjoin(root,folder)
                    # check if dir is a run_dir or grid_dir
                    # NOTE: is_grid_dir and is_run_dir currently identical
                    no_grid_dir = (switch == 1 and not self.is_grid_dir(out_dir))
                    no_run_dir = (switch == 2 and not self.is_run_dir(out_dir))
                    if no_grid_dir or no_run_dir:
                        out.print_error("Selected grid directory %s does not appear to a run folder and cannot be used for the grid run." % out_dir) 
                    if ("loop" in root):
                        loop_out_dirs.append(out_dir)                    
                    else:
                        out_dirs.append(out_dir)
        return out_dirs, loop_out_dirs
#}}}
#{{{ def: get_named_files(self,folder_name,NLO_subtraction,order,switch)
    def get_named_files(self,folder_path,string,level_in):
    # get all files that contain a given string at a specific level in the folder structure
        out_files = []
        startinglevel = folder_path.count(os.sep)
        for root, dirnames, filenames in os.walk(folder_path):
            level = root.count(os.sep) - startinglevel
            if level == level_in:
                for files in filenames:
                    out_file = pjoin(root,files)
                    if string in out_file:
                        out_files.append(out_file)
        return out_files
#}}}
#{{{ def: is_run_dir(self,path)
    def is_run_dir(self,path):
        if os.path.isfile(pjoin(path,"file_parameter.dat")):
            return True
        else:
            return False
#}}}
#{{{ def: is_grid_dir(self,path)
    def is_grid_dir(self,path):
        # 2do: add additional check wether file_parameter.dat has the right inputs
        if os.path.isfile(pjoin(path,"file_parameter.dat")):
            return True
        else:
            return False
#}}}
#{{{ def: fileopen(self,filename,rw_option)
    def fileopen(self,filename,rw_option):
    # dummy routine to open in/out/err files or not depending on wether multicore or cluster is used
        if self.runmode == "multicore":
            return open(filename, rw_option)
        else:
            return Dummyopen()
#}}}
#{{{ def: job_just_restarted_add(self)
    def job_just_restarted_add(self):
        # this signals that a job has just been restarted by creating the just.restarted file in logs
        log_folder = pjoin(process_dir,"log",run_folder)
        just_restarted_file = pjoin(log_folder,"just.restarted")
        open(just_restarted_file, 'a').close()
#}}}
#{{{ def: job_just_restarted_remove(self)
    def job_just_restarted_remove(self):
        # this removes the just_restarted signal by removing the just.restarted file in logs
        log_folder = pjoin(process_dir,"log",run_folder)
        just_restarted_file = pjoin(log_folder,"just.restarted")
        try:
            os.remove(just_restarted_file)
        except:
            pass
#}}}
#{{{ def: job_just_restarted(self)
    def job_just_restarted(self):
        # this checks wether a job has just been restarted by checking if just.restarted file exists
        log_folder = pjoin(process_dir,"log",run_folder)
        just_restarted_file = pjoin(log_folder,"just.restarted")
        if os.path.isfile(just_restarted_file):
            return True
        else:
            return False
#}}}
#{{{ def: get_coupling_order_QCD(self,order)
    def get_coupling_order_QCD(self,order):
        # this checks wether a job has just been restarted by checking if just.restarted file exists
        coupling_order_LO_QCD   = os.listdir(pjoin(fold.run_folder_path,"LO"))[0].lstrip("a")[:1]
        coupling_order_NLO_QCD  = str(int(coupling_order_LO_QCD) + 1)
        coupling_order_NNLO_QCD = str(int(coupling_order_LO_QCD) + 2)
        coupling_order_NNNLO_QCD = str(int(coupling_order_LO_QCD) + 3)
        if order == "LO":
            return coupling_order_LO_QCD
        if order == "NLO":
            return coupling_order_NLO_QCD
        if order == "NNLO":
            return coupling_order_NNLO_QCD
        if order == "NNNLO":
            return coupling_order_NNNLO_QCD
#}}}
#{{{ def: get_coupling_order_EW(self,order)
    def get_coupling_order_EW(self,order):
        # this checks wether a job has just been restarted by checking if just.restarted file exists
        coupling_order_LO_EW   = os.listdir(pjoin(fold.run_folder_path,"LO"))[0].lstrip("a")[-1:]
        coupling_order_NLO_EW  = str(int(coupling_order_LO_EW) + 1)
        coupling_order_NNLO_EW = str(int(coupling_order_LO_EW) + 2)
        if order == "LO":
            return coupling_order_LO_EW
        if order == "NLO":
            return coupling_order_NLO_EW
        if order == "NNLO":
            return coupling_order_NNLO_EW
#}}}
#{{{ def: get_coupling_order(self,order,order_EW = "LO")
    def get_coupling_order(self,order,order_EW = "LO"):
        # this checks wether a job has just been restarted by checking if just.restarted file exists
        coupling_order = self.get_coupling_order_QCD(order)+self.get_coupling_order_EW(order_EW)
        return coupling_order
#}}}
#### the following are simple fuctions to determine different properties from the path
#{{{ def: get_properties(self,path)
    def get_properties(self,path):
    # determines the coupling strucure (2. folder from run_folder)
        order=subtraction=coupling=contribution=run_dir=""
        split=path.replace(pjoin(process_dir,run_folder)+"/","").split("/")
        try:
            try:
                order=split[0].split(".")[0]
                subtraction=split[0].split(".")[1]
            except:
                order=split[0]
            coupling=split[1]
            contribution=split[2]
            run_dir=split[3]
        except:
            pass
        return order,subtraction,coupling,contribution,run_dir
#}}}
#{{{ def: get_order(self,path)
    def get_order(self,path):
    # determines the coupling strucure (2. folder from run_folder)
        order,subtraction,coupling,contribution,run_dir=self.get_properties(path)
        return order
#}}}
#{{{ def: get_subtraction(self,path)
    def get_subtraction(self,path):
    # determines the coupling strucure (2. folder from run_folder)
        order,subtraction,coupling,contribution,run_dir=self.get_properties(path)
        return subtraction
#}}}
#{{{ def: get_coupling(self,path)
    def get_coupling(self,path):
    # determines the coupling strucure (2. folder from run_folder)
        order,subtraction,coupling,contribution,run_dir=self.get_properties(path)
        return coupling
#}}}
#{{{ def: get_contribution(self,path)
    def get_contribution(self,path):
    # determines the contribution (2. folder from run_folder)
        order,subtraction,coupling,contribution,run_dir=self.get_properties(path)
        return contribution
#}}}
#{{{ def: get_run_dir(self,path)
    def get_run_dir(self,path):
    # determines the coupling strucure (2. folder from run_folder)
        order,subtraction,coupling,contribution,run_dir=self.get_properties(path)
        return run_dir
#}}}
#{{{ def: control_c_handler_child(self,signal,frame)
    def control_c_handler_child(self,signal,frame):
        # this handels the ctrl-c for the subprocesses
        sys.exit(0)
#}}}
#}}}


#####################################
# Main part of the MATRIX execution #
#####################################
#{{{ program banner
print ""
banner = banner("|","|",67,11)
banner.print_matrix()
print ""
#}}}
if args.tar_run: args.run_mode = "tar_run"
if args.setup_run: args.run_mode = "setup_run"
if args.delete_run: args.run_mode = "delete_run"
continue_run = args.continue_run
    
if continue_run:
    out.print_warning("You are trying to continue a run; MAKE SURE THE INPUTS ARE CONSISTENT !!!")
#{{{ initialize classes
inp = inputs()       # class for handling the input
#}}}
#{{{ read MATRIX configuration file
config_list={}
inp.input_read_parameter_dat(pjoin("input","MATRIX_configuration"),config_list)
# set default editor
edit_input.editor = config_list.get("default_editor","")
#}}}
#{{{ determine run folder
current_dir = start_dir

#if len(sys.argv)>=2:
#    run_folder = ssys.argv[1]
if args.run_folder != "":
    run_folder = args.run_folder
# remove trailing "/" if any
    if run_folder[-1] == "/":
      run_folder = run_folder[:-1]
    if not run_name.check_run_folder(run_folder):
        out.print_error("Chosen run folder "+run_folder+" is not suitable. Exiting...")
else:
    # determine next run_XX folder
    next_run_folder = "run_%02d" % int(run_name.get_highest_existing_run()+1)    
    # readin folder used for the run until folder suits check
    out.print_read("Type name of folder for this run (has to start with \"run_\"). \"ENTER\" to create and use \"%s\". Press TAB or type \"list\" to show existing runs. Type \"exit\" or \"quit\" to stop. Any other folder will be created." % next_run_folder)
    while True:
        run_folder = run_name.readin_run_folder()
        if run_folder == "quit" or run_folder == "exit":
            exit(0)
        elif run_folder == "list":
            run_name.list_run_folders()
        elif run_name.check_run_folder(run_folder): break
    # if run_folder empty next_run_folder will be used
    if run_folder == "": run_folder = next_run_folder
#}}}
#{{{ create new run folder, or overwrite the old if already exists
default_run_path = pjoin(process_dir,default_run)
if not os.path.exists(default_run_path):
    tar = tarfile.open(default_run_path+".tar")
    tar.extractall()
    tar.close()
# define all the relevant folders for the matrix code
fold.run_folder_path    = pjoin(process_dir,run_folder)                    # this is the main folder with the MUNICH code, where the runs are done
fold.input_folder_path  = pjoin(process_dir,"input",run_folder)           # MATRIX inputs
fold.log_folder_path    = pjoin(process_dir,"log",run_folder)             # MATRIX logs
fold.result_folder_path = pjoin(process_dir,"result",run_folder)          # MATRIX results
fold.default_input_path = pjoin(process_dir,"input",args.input_dir)        # folder of the MATRIX inputs
fold.check_default_input_path() # checks wether default path is suitable (is a folder and contains parameter/model/distribution.dat)
fold.input_file_dir     = pjoin(munich_dir,"input_files",prc.process_name) # default folder for the MUNICH inputs to be copied in the main folder (file_parameter/model/distribution.dat)

if os.path.exists(pjoin(default_run_path,"file_parameter.dat")) and os.path.exists(pjoin(default_run_path,"file_model.dat")) and os.path.exists(pjoin(default_run_path,"file_distribution.dat")):
    fold.input_file_dir = default_run_path
# set paths for classes that have no access to the fold instance
out.run_folder = run_folder # needed for log-file printout
edit_input.input_folder = fold.input_folder_path
# test if code in run_folder is already running
if log.code_running():
    out.print_error("An instance of MATRIX appears to be already running in this folder (%s). If that is not the case, remove file %s and retry the run." % (fold.run_folder_path,pjoin(process_dir,"log",run_folder,"main.running")))
# if argument chosen to change name of run, do it here and stop
if args.new_name:
    if not run_name.check_run_folder(args.new_name):
        out.print_error("Cannot change name of run to \"%s\", which is no suitable name for a run folder. Exiting..." % args.new_name)
    if os.path.exists(pjoin(process_dir,args.new_name)):
        out.print_error("Cannot change name of run to \"%s\", because run folder %s already exists. Exiting..." % (args.new_name, pjoin(process_dir,args.new_name)))
    if os.path.exists(fold.run_folder_path):
        fold.rename_run_to(args.new_name)
        exit(0)
    else:
        out.print_error("Cannot change name of run folder %s, which does not exists. Exiting..." % fold.run_folder_path)
# if argument chosen to copy from existing run, do it here and stop
if args.existing_run:
    if not os.path.exists(pjoin(process_dir,args.existing_run)):
        out.print_error("Cannot copy run from \"%s\", because run folder %s does not exist. Exiting..." % (args.existing_run, pjoin(process_dir,args.existing_run)))
    if not os.path.exists(fold.run_folder_path):
        fold.copy_run_from(args.existing_run)
        exit(0)
    else:
        out.print_error("Cannot copy tun to run folder \"%s\", which already exists. Exiting..." % fold.run_folder_path)
# if full run remove the old folder
if os.path.exists(fold.run_folder_path):
    #if not len(sys.argv)>=3 or not sys.argv[2] in run_modes:
    if args.run_mode == "" or not args.run_mode in run_modes:
        out.print_warning("Run folder selected: %s; Previous run in this folder will be overwritten; abort script NOW if that is not wanted." % fold.run_folder_path)
    #elif len(sys.argv)>=3 and sys.argv[2] in ["run","run_grid","setup_run"]:
    elif args.run_mode in ["run","run_without_pre","run_grid","setup_run"]:
        out.print_warning("Run folder selected: %s; Previous run in this folder will be overwritten." % fold.run_folder_path)

if not os.path.exists(fold.input_folder_path):
    shutil.copytree(fold.default_input_path,fold.input_folder_path,symlinks=True)
# overwrite (remove+add) input folder when given frome a different default location
elif not args.input_dir == default_input and os.path.exists(fold.input_folder_path):
    shutil.rmtree(fold.input_folder_path)
    shutil.copytree(fold.default_input_path,fold.input_folder_path,symlinks=True)
try:
    os.remove(pjoin(fold.input_folder_path,"runtime.dat"))
except:
    pass
# create results folder
if not os.path.exists(fold.result_folder_path):
   os.makedirs(fold.result_folder_path)

#}}}
#{{{ call command line to choose inputs or start run in chosen run_mode

#if len(sys.argv)>=3:
if args.run_mode != "":
    if args.run_mode in run_modes:
        run_mode = args.run_mode
    else:
        out.print_error("Run mode \"%s\" is not known."%args.run_mode)
else:
    if __name__ == '__main__':
        edit_input.prompt="|============>> "
        edit_input.cmdloop()
        run_mode = edit_input.run_mode

# create log folder if first time
try:
    os.makedirs(pjoin(process_dir,"log"))
except:
    pass
sys.stdout = output_saver(pjoin(process_dir,"log",run_folder+".log")) # must be here because autocompletion does not work otherwise
#}}}
# if run should be removed, delete the input/logs/results and stop the run
if run_mode == "delete_run":
    fold.remove_run()
    exit(0)
# if run should be tarred, tar also the input/log/result folder and stop the run
elif run_mode == "tar_run":
    if os.path.exists(fold.run_folder_path):
        fold.tar_run()
        exit(0)
    else:
        out.print_error("Cannot create .tar archive of run folder %s, that does not exist. Exiting..." % fold.run_folder_path)
#{{{ adjust folder structure according to run_mode

# otherwise create/delete some folders according to run mode
# do this only if the previous log was not saved
removed = False
if run_mode in ["run","run_without_pre","setup_run"] and not continue_run:
    if os.path.exists(fold.run_folder_path):
        shutil.rmtree(fold.run_folder_path)
        removed = True
        out.print_info("Old Run folder overwritten: %s." % fold.run_folder_path)
    
if not os.path.exists(fold.run_folder_path):
    shutil.copytree(default_run_path,fold.run_folder_path,symlinks=True)
    shutil.copy(pjoin(fold.input_file_dir,"file_parameter.dat"),fold.run_folder_path)
    shutil.copy(pjoin(fold.input_file_dir,"file_model.dat"),fold.run_folder_path)
    shutil.copy(pjoin(fold.input_file_dir,"file_distribution.dat"),fold.run_folder_path)
    if not removed:
        out.print_info("New Run folder created: %s." % fold.run_folder_path)
#}}}

#{{{ wrap all inputs from MATRIX to MUNICH

parameter_list    = {}
model_list        = multidim_dict(2) # multidimensonal dictionary
distribution_list = multidim_dict(2) # multidimensonal dictionary
# read parameter.dat file
inp.input_read_parameter_dat(pjoin(fold.input_folder_path,"parameter.dat"),parameter_list)
# check the parameters just read
#if not run_mode in ["run_results"]:
inp.input_check_parameter_consistencies_from_list(pjoin(fold.run_folder_path,"file_parameter.dat"),parameter_list)
# 2do: not needed separately?
#    inp.input_user_cuts_default_and_consistency(pjoin(fold.run_folder_path,"file_parameter.dat"),parameter_list)
# write file_parameter.dat
inp.input_set_file_parameter_from_list(pjoin(fold.run_folder_path,"file_parameter.dat"),parameter_list)

# read model.dat file
inp.input_read_SLHA(pjoin(fold.input_folder_path,"model.dat"),model_list)
# write file_model.dat
inp.input_set_file_model_from_SLHA(pjoin(fold.run_folder_path,"file_model.dat"),model_list)
# read distribution file and check for unique identifiers (distributionname)
inp.input_read_and_check_distribution_dat(pjoin(fold.input_folder_path,"distribution.dat"),distribution_list)

# read distribution.dat (by simply writing the whole file into content)
content = inp.input_read_distribution_dat(pjoin(fold.input_folder_path,"distribution.dat"))
# copy the distribution file first, not to append several times
shutil.copy(pjoin(fold.input_file_dir,"file_distribution.dat"),fold.run_folder_path)
# write file_distribution.dat (simply appending distribution.dat from content)
inp.input_set_file_distribution_dat(pjoin(fold.run_folder_path,"file_distribution.dat"),content)

#SIMONE
#copy file with double differential distributions input
shutil.copyfile(pjoin(fold.input_folder_path,"dddistribution.dat"), pjoin(fold.run_folder_path,"file_dddistribution.dat"))  

# save the result and log files if specified in the input
if parameter_list["save_previous_result"] == "1" and not run_mode in ["run_gnuplot"]:
    res.save_previous()
if parameter_list["save_previous_log"] == "1":
    if not run_mode in ["run_results","run_gnuplot"]: # makes no sense in results run where you don't reproduce new logs
        log.save_previous(run_mode)
    try:
        os.makedirs(pjoin(fold.log_folder_path,"successful"))
    except:
        pass
    try:
        os.makedirs(pjoin(fold.log_folder_path,"failed"))
    except:
        pass
# save the inputs in the result folder AFTER the previous run was saved
if not run_mode in ["run_results","run_gnuplot"]: # makes no sense in results run where you might have changed the actual inputs
    res.save_input_with_result()

#}}}
#{{{ adjust folder structure according to run_mode

# otherwise create/delete some folders according to run mode
# do this only if the previous log was not saved
if parameter_list["save_previous_log"] == "0":
    if run_mode in ["run","run_grid","setup_run","run_without_pre"] :
        if os.path.exists(fold.log_folder_path):
            shutil.rmtree(fold.log_folder_path)
        os.makedirs(fold.log_folder_path)
        os.makedirs(pjoin(fold.log_folder_path,"successful"))
        os.makedirs(pjoin(fold.log_folder_path,"failed"))
    elif run_mode in ["run_results","run_gnuplot"]:
        pass
    else:
        if run_mode in ["run_pre","run_pre_and_main"]:
            try:
                shutil.rmtree(pjoin(fold.log_folder_path,"pre_run"))
            except:
                pass
        if run_mode in ["run_main","run_pre_and_main","run_main_without_pre"]:
            try:
                shutil.rmtree(pjoin(fold.log_folder_path,"main_run"))
            except:
                pass
        try:
            for the_file in os.listdir(fold.log_folder_path):
                file_path = pjoin(fold.log_folder_path, the_file)
                try:
                    if os.path.isfile(file_path):
                        os.unlink(file_path)
                except Exception, e:
                    print e
        except:
            pass
        try:
            shutil.rmtree(pjoin(fold.log_folder_path,"successful"))
        except:
            pass
        try:
            shutil.rmtree(pjoin(fold.log_folder_path,"failed"))
        except:
            pass
        os.makedirs(pjoin(fold.log_folder_path,"successful"))
        os.makedirs(pjoin(fold.log_folder_path,"failed"))

if run_mode in ["run_without_pre","run_main_without_pre"]:
    out.print_warning("You have chosen to run without doing a dedicated runtime extrapolation (pre run) for this run. A pre-generated runtime.dat file is used, which has been created on a different machine and might not fit to the present setup. Be careful regarding the desired precision, it might not be met without a dedicated runtime extrapolation for this run.")
    if parameter_list.get("include_pre_in_results") == "1":
        out.print_error("You have chosen to run without pre run. Cannot include pre run in result combination. Change include_pre_in_results=1 in paramter.dat file and restart. Exiting...")
    else:
        parameter_list["include_pre_in_results"] = "0"
#}}}
# add first log: main code is running (no second instance will be able to start)
log.code_running_add() # that must be after log folder might be deleted
#{{{ handle and set inputs from parameter.dat file
# resummation settings
if parameter_list.get("add_NLL","0") == "1" or parameter_list.get("add_NNLL","0") == "1":
     # change born and VT parallelization, since much slower with resummation
    default_settings = pre_run_settings["default"]
    default_settings["born"] = [50,50000]
    default_settings["VT.QCD"] = [50,50000]
    pre_run_settings["default"] = default_settings
# determines the orders set to be run
order = []
if int(parameter_list["run_LO"]) == 1:
    order.append("LO")
if int(parameter_list.get("run_NLO_QCD","0")) == 1 or int(parameter_list.get("run_NLO_EW","0")) == 1:
    order.append("NLO")
if int(parameter_list.get("run_NNLO_QCD","0")) == 1 or int(parameter_list.get("add_NLO_EW","0")) == 1:
    order.append("NNLO")
if not order:
    out.print_error("No order has been set to be run.")
# switch for loop-induced contribution
if int(parameter_list["loop_induced"]) == 0:
    include_loop_induced = False # switch to turn of loop induced folders
elif int(parameter_list["loop_induced"]) == 1 or int(parameter_list["loop_induced"]) == 2 or int(parameter_list["loop_induced"]) == -1 or int(parameter_list["loop_induced"]) == -2:
    include_loop_induced = True # switch to turn of loop induced folders
else:
    out.print_error("Input \"loop_induced\" in parameter.dat can only be 0, 1, 2, -1 or -2, but it was set to "+parameter_list["loop_induced"])
# switch for NLO subtraction method
if int(parameter_list["NLO_subtraction_method"]) == 0: # use both !!! 2do: non-functionable for results and runtimes !!!
    NLO_subtraction = ["CS","QT"]
    out.print_error_no_stop("Simultaneous computation of CS and QT subtraction at NLO not supported at the moment.")
    out.print_error("Input \"NLO_subtraction\" in parameter.dat can only be 1 or 2, but it was set to "+parameter_list["NLO_subtraction_method"])
elif int(parameter_list["NLO_subtraction_method"]) == 1: # use CS
    NLO_subtraction = ["CS"]
elif int(parameter_list["NLO_subtraction_method"]) == 2: # use QT
    NLO_subtraction = ["QT"]
else:
    out.print_error("Input \"NLO_subtraction\" in parameter.dat can only be 1 or 2, but it was set to "+parameter_list["NLO_subtraction_method"])
#}}}
#{{{ set inputs from MATRIX_configuration file
# default values:
nr_cores     = multiprocessing.cpu_count()
nr_nodes     = -1 # (-1) means no limit
runmode      = "multicore"
max_restarts = 1 # set maximal number of restarts in warmup and main run

# default inputs:
if not "parallel_job_limit" in config_list:
    config_list["parallel_job_limit"] = 5000 # run at most 5000 parallel jobs without user interaction
if not "max_jobs_in_cluster_queue" in config_list:
    config_list["max_jobs_in_cluster_queue"] = -1 # no limit on the jobs in the cluster queue

# for lhapdf we need all orders <= highest order
if "NNLO" in order:
    order_lhapdf = ["LO","NLO","NNLO"]
elif "NLO" in order:
    order_lhapdf = ["LO","NLO"]
elif "LO" in order:
    order_lhapdf = ["LO"]

# initialize lhapdf class
pdf = lhapdf(config_list,parameter_list["LHAPDF_LO"],parameter_list["LHAPDF_NLO"],parameter_list["LHAPDF_NNLO"],order_lhapdf) # class for handling the pdf sets
# print LHAPDF version 2do: comment here and uncomment below?
pdf.print_lhapdf_version()
# check if pdf any pdf set is missing
if pdf.get_missing_sets(): # returns list with sets that are missing
#    pdf.print_lhapdf_version() # only print LHAPDF version when new sets are downloaded
    pdf.download_pdf_sets(pdf.get_missing_sets()) 

# stop here after making sure that PDF sets are available
if run_mode == "setup_run":
    out.print_info("Chosen to only set up the run folder. Exiting...")
    exit(0)

# set exe_path to folder class
fold.exe_path = pjoin(config_list.get("path_to_executable",pjoin(munich_dir,"bin")),prc.process_name)
# hack until QCD works in EW executable
fold.exe_pathEW  = pjoin(config_list.get("path_to_executable",pjoin(munich_dir,"bin")),prc.process_name)
fold.exe_pathQCD = pjoin(config_list.get("path_to_executable",pjoin(munich_dir,"bin")),prc.process_name)#.rstrip("EW")
if not config_list.get("path_to_executable","/").startswith("/"):
    out.print_error("path_to_exectuable %s in MATRIX_configuration MUST be a full path starting with \"/\". Exiting..." % fold.exe_path)
if not os.path.exists(fold.exe_path):
    out.print_error("Executable %s does not exist. Was the process correctly compiled?" % fold.exe_path)
# remove the symlink in the run folder
try:
    os.remove(pjoin(fold.run_folder_path,prc.process_name))
except:
    pass
# create new symlink in the run folder
try:
    os.symlink(fold.exe_path,pjoin(fold.run_folder_path,prc.process_name))
except:
    pass


# overwrite values that have been set
if "mode" in config_list:
    if int(config_list["mode"]) == 0:
        runmode = "multicore"
    elif int(config_list["mode"]) == 1:
        runmode = "cluster"
        #create cluster folder in cluster runs
        cluster_folder_path = pjoin(fold.run_folder_path,"cluster")
        # do not do this in result run
        if not run_mode in ["run_results","run_gnuplot"]:
            if os.path.exists(cluster_folder_path):
                shutil.rmtree(pjoin(cluster_folder_path))
            # the batch_file and the running/pending/finished lists should be removed in order to avoid confusion 
            # with old job ids, therefore we renew the whole cluster folder every run
            os.makedirs(cluster_folder_path)
            os.makedirs(pjoin(cluster_folder_path,"batch_files"))
            os.makedirs(pjoin(cluster_folder_path,"active_jobs"))
            try:
                cluster_name = config_list["cluster_name"] 
            except:
                out.print_error("Cluster mode (mode=1) chosen, but no cluster_name specified in MATRIX configuration file.")
    else:
        out.print_error("Variable mode in MATRIX configuration file can only be 0 or 1")
if "max_nr_parallel_jobs" in config_list:
    nr_cores = int(config_list["max_nr_parallel_jobs"])
    nr_nodes = int(config_list["max_nr_parallel_jobs"])
if "max_restarts" in config_list:
    max_restarts = int(config_list["max_restarts"])
if runmode == "cluster":
    nr_cores = nr_nodes # use same variable (nr_cores) for cores and nodes
#}}}
#{{{ hard-coded parameters
# inputs that have to be clear/set before job running
set_parallel_runs = 0 # no parallel runs (0) 2do: implement variable number of parallel runs depending on precision; copy main_run_folder
if new_MATRIX_run:
    grid_folder = "grid"  # one could make these arrays 
    main_run_folder = "run.0" # in order to request more folders
else:
    grid_folder = "grid.IS0"  # one could make these arrays 
    main_run_folder = "run.0" # in order to request more folders
#}}}
#{{{ assigning grids to subprocesses
# manual dictionary which connects level 3 folders with required grids, needs to be changed 
# if, eg, K+P terms should get different phase-space, or when QED is considered as well

grid_assignment = {}
if new_MATRIX_run:
    grid_assignment["VA.QCD"]="born"
    grid_assignment["VT.QCD"]="born" # for QT subraction
    grid_assignment["CT.QCD"]="born" # for QT subraction
    grid_assignment["CT2.QCD"]="born"
    grid_assignment["RVA.QCD"]="RT.QCD"
    grid_assignment["VT2.QCD"]="born"
    grid_assignment["L2VA.QCD"]="loop"
    grid_assignment["L2VT.QCD"]="loop" # for QT subraction
    grid_assignment["L2CT.QCD"]="loop" # for QT subraction
    grid_assignment["VA.QEW"]="born"
else:
    grid_assignment["CA.QCD"]="born"
    grid_assignment["VA.QCD"]="born"
    grid_assignment["VT.QCD"]="born" # for QT subraction
    grid_assignment["CT.QCD"]="born" # for QT subraction
    grid_assignment["CT2.QCD"]="born"
    grid_assignment["RCA.QCD"]="RT.QCD"
    grid_assignment["RVA.QCD"]="RT.QCD"
    grid_assignment["VT2.QCD"]="born"
# all other run information is extracted from folder structure and existing files
#}}}

#{{{ 2do !!!

# 2do, missing: check for errors and exceptions while running, are the grids correctly  !!!
#               produced? has something gone wrong? sanity checks! need input for that! !!!

#2do: finish matrix run_log class folder_structure, cluster_log, results

# 2do: add some sanity checks to make sure all data is there

# input/run_folder/*files* have to be checked when creating the folder
# create new inputs when not existing
# warn when running without input folder
#
# - write function to cleanup single folder
# - write function to cleanup all folders
# - how should program behave, continue runs, replace everything, switch inbetween?
#
# - check from time to time wether process is still alive !!!
#

#}}}

# define some local functions so that we can factorize the run execution a bit more
#{{{ def: check_parallel()
def check_parallel():
    # this routine checks wether the number of parallel jobs is too large
    if log.total_parallel >= 100000:
        out.print_error("You are trying to run %s parallel jobs. This is a fail-safe which prevents you from running more than 100000 jobs. Please increase the max_time_per_job or the required precision in the parameter.dat file and try again. Exiting..." % log.total_parallel)
    elif int(log.total_parallel) > int(config_list["parallel_job_limit"]):
        out.print_warning("You are trying to run %s parallel jobs. This exceeds the parallel_job_limit of %s in the MATRIX_configuration file." % (int(log.total_parallel),int(config_list["parallel_job_limit"])))
        out.print_warning("Do you want to continue with the main run anyway?")
        out.print_read("Press \"ENTER\" to continue or enter any text to stop running.")
        # read user input
        input = raw_input("|============>> ")
        # abort script if not pressed ENTER
        if input.strip() == "":
            out.print_info("Increase the parallel_job_limit in the MATRIX_configuration file to avoid this warning in later runs.")
            out.print_info("Continuing despite the large number of parallel runs...")
        else:
            out.print_error_no_stop("To reduce the number of parallel jobs, please increase the max_time_per_job or the required precision in the parameter.dat file and try again.")
            out.print_error("Stopping the code...")
#}}}
#{{{ def: print_restarted_runs()
def print_restarted_runs():
    # this routine checks wethere there are restarted runs and prints them out
    if log.list_exists("restarted_list.log"):
        out.print_warning("The following runs had to be restarted with different random seed (see log-file why original runs failed):")
        out.print_list(pjoin(fold.log_folder_path,"restarted_list.log"),"warning")
#}}}
#{{{ def: rerun_warmup()
def rerun_warmup(maximal_reruns):
    # this routine reruns the warmup in case there are jobs that failed
    reruns = 1 # init reruns to be one
    while run.jobs_in_failed_list() and reruns <= maximal_reruns:
        log.clear_list("restarted_list.log") # clear the list of restarted jobs during previous run
        run.warmup(2) # second phase of warmup: runs that needed to be redone
        print_restarted_runs() # if there are any, print out the runs that were restarted with different random seed
        if run.errors_flag: out.print_error("Exception error while re-running warmup (%s time). Stopping the code..."%reruns)
        reruns+=1
#}}}
#{{{ def: check_warmup()
def check_warmup():
    # this routine checks if there are still failed grid runs, prints warnings and waits for user input
    if run.jobs_in_failed_list():    
        out.print_warning("The following warmup runs (grid setup) have not correctly finished (see log-file):")
        # list the runs that failed
        out.print_failed_runs()
        out.print_warning("Jobs in the main run that require the corresponding grids will also fail.")
        out.print_warning("Do you want to continue with the main run anyway?")
        out.print_read("Press \"ENTER\" to continue or enter any text to stop running.")
        # read user input
        input = raw_input("|============>> ")
        # abort script if not pressed ENTER
        if input.strip() == "":
            out.print_info("continuing despite some failed grid runs...")
        else:
            out.print_error("Stopping the code...")
    else:
        out.print_info("All runs successfully finished.")
#}}}
#{{{ def: check_grid_log()
def check_grid_log():
    # this routine checks in the logs wether the grid run has successfully finished and start a dialog if not
    if not run.grid_run_complete():
        if run.failed_run_list:
            out.print_warning("The following runs have no \"final result\" in the execution file:")
            out.print_list(run.failed_run_list,"warning")
        # dialog to abort or to run anyway (eg, if you have the grid run complete but the log files are missing)
        out.print_warning("Number of successful grid runs is not as expected (see grid_run folder in log folder %s)." % fold.log_folder_path)
        out.print_warning("Expected number of successful grid runs: %s" % run.expected_nr_of_grid_runs)
        out.print_warning("Number of successful grid runs: %s" % run.successful_nr_of_grid_runs)
        out.print_warning("Number of failed grid runs: %s" % run.failed_nr_of_grid_runs)
        if run.successful_nr_of_grid_runs < run.expected_nr_of_grid_runs:
            out.print_warning("Jobs in the main run that require the corresponding grids will also fail.")
            out.print_warning("Do you want to continue with the main run anyway?")
            out.print_read("Press \"ENTER\" to continue or enter any text to stop running.")
            # read user input
            input = raw_input("|============>> ")
            # abort script if not pressed ENTER
            if input.strip() == "":
                out.print_info("continuing despite some failed grid runs...")
            else:
                out.print_error("Stopping the code...")
        elif run.successful_nr_of_grid_runs > run.expected_nr_of_grid_runs:
            out.print_warning("Apparently there are more grid runs than expected. Make sure things are consistent. Continuing...")
#}}}
#{{{ def: rerun_pre_run()
def rerun_pre_run(maximal_reruns):
    # this routine reruns the pre runs in case there are there are jobs that failed
    reruns = 1 # init reruns to be one
    while run.jobs_in_failed_list() and reruns <= maximal_reruns:
        log.clear_list("restarted_list.log") # clear the list of restarted jobs during previous run
        run.main_run(-2) # second phase of pre run: runs that needed to be redone
        print_restarted_runs() # if there are any, print out the runs that were restarted with different random seed
        if run.errors_flag: out.print_warning("Exception error while re-running extrapolation processes(%s time)." % reruns)
        reruns+=1
#}}}
#{{{ def: check_pre_run()
def check_pre_run():
    # this routine checks if there are still failed pre runs, prints an error and stops or continues with success message
    if run.pre_run_complete():
            out.print_info("All runs successfully finished.")
    else:
        if run.jobs_in_failed_list():
            out.print_error_no_stop("Parts of the runtime extrapolation (pre run) failed. The following runs have not correctly finished (see log-file):")
            out.print_failed_runs() # list the runs that failed
            exit(0)
        else:
            out.print_error_no_stop("Some parts of the runtime extrapolation (pre run) did not finish correctly. The following runs have no \"final result\" in the execution file:")
            out.print_list(run.failed_run_list,"error")
            exit(0)
#}}}
#{{{ def: rerun_main_run()
def rerun_main_run(maximal_reruns):
    # this routine reruns the main runs in case there are there are jobs that failed
    reruns = 1 # init reruns to be one
    while run.jobs_in_failed_list() and reruns <= maximal_reruns:
        log.clear_list("restarted_list.log") # clear the list of restarted jobs during previous run
        run.main_run(2) # second phase of main run: runs that needed to be redone
        print_restarted_runs() # if there are any, print out the runs that were restarted with different random seed
        if run.errors_flag: out.print_warning("Exception error while re-running main processes(%s time)." % restarts)
        reruns+=1
#}}}
#{{{ def: check_main_run()
def check_main_run():
    # this routine checks if there are still failed main runs, prints an error and stops or continues with success message
    if run.main_run_complete():
            out.print_info("All runs successfully finished.")
    else:
        if run.jobs_in_failed_list():
            out.print_error_no_stop("Parts of the cross section computation (main run) failed. The following runs have not correctly finished (see log-file):")
            out.print_failed_runs() # list the runs that failed
            exit(0)
        else:
            out.print_error_no_stop("Some parts of the cross section computation (main run) did not finish correctly. The following runs have no \"final result\" in the execution file:")
            out.print_list(run.failed_run_list,"error")
            exit(0)
#}}}
#{{{ def: run_gnuplot()
def run_gnuplot():
    # this routine calls the gnuplot class to automatically plot all distributions
    out.print_info("Plotting results with gnuplot...")
    try:

        if (int(parameter_list["loop_induced"])) > 0:
        
            first_distribution_path = glob.glob(pjoin(fold.result_folder_path,max(order, key=len)+"-run/distributions*"))[0]
           
        elif (int(parameter_list["loop_induced"])) <= 0:
    
            first_distribution_path = glob.glob(pjoin(fold.result_folder_path,max(order, key=len)+"-run/distributions*"))[0]
 

        all_plots = glob.glob(pjoin(first_distribution_path,"*.dat"))
  #      print all_plots , 'all_plots'
    
    except:
        all_plots = ""
    if not all_plots: out.print_error_no_stop("Cannot find any distributions in %s. Continuing anyways..." % pjoin(fold.result_folder_path,max(order, key=len)+"-run/distributions/*.dat"))
    gnu = gnuplot(fold.result_folder_path,True)
    gnu.clean_gnuplot_folder()
#    gnu.clean_gnuplot_folder()  # should be already empty
    for plot in all_plots:
        gnu = gnuplot(fold.result_folder_path,True)

        plot_LO = plot.rsplit("__")[0].replace("/"+max(order, key=len)+"-run/","/LO-run/") +"__LO/" + plot.rsplit("__")[1].rsplit("/")[1] + "__LO.dat"
        if "LO" in order:
            gnu.add_curve(plot_LO)
        plot_NLO = plot.rsplit("__")[0].replace("/"+max(order, key=len)+"-run/","/NLO-run/") +"__NLO_QCD/" + plot.rsplit("__")[1].rsplit("/")[1] + "__NLO_QCD.dat"
        if "NLO" in order:
            gnu.add_curve(plot_NLO)
        plot_NNLO_LO_gg = plot.rsplit("__")[0].replace("/"+max(order, key=len)+"-run/","/NNLO-run/") +"__NNLO_LOgg_QCD/" + plot.rsplit("__")[1].rsplit("/")[1] + "__NNLO_LOgg_QCD.dat"
        if "NNLO" in order and (int(parameter_list["loop_induced"])) > 0:
            gnu.add_curve(plot_NNLO_LO_gg)
        plot_loopNLOgg = plot.rsplit("__")[0].replace("/"+max(order, key=len)+"-run/","/NNLO-run/") +"__loopNLOgg_QCD/" + plot.rsplit("__")[1].rsplit("/")[1] + "__loopNLOgg_QCD.dat"
        plot_loopLOgg = plot.rsplit("__")[0].replace("/"+max(order, key=len)+"-run/","/NNLO-run/") +"__loopLOgg_QCD/" + plot.rsplit("__")[1].rsplit("/")[1] + "__loopLOgg_QCD.dat"
        if "NNLO" in order and (int(parameter_list["loop_induced"])) < 0:
            gnu.add_curve(plot_loopLOgg)
        plot_nNNLO = plot.rsplit("__")[0].replace("/"+max(order, key=len)+"-run/","/NNLO-run/") +"__nNNLO_QCD/" + plot.rsplit("__")[1].rsplit("/")[1] + "__nNNLO_QCD.dat"
        if "NNLO" in order and (int(parameter_list["loop_induced"])) > 1:
            gnu.add_curve(plot_loopNLOgg)
            gnu.add_curve(plot_nNNLO)
        plot_NNLO = plot.rsplit("__")[0].replace("/"+max(order, key=len)+"-run/","/NNLO-run/") +"__NNLO_QCD/" + plot.rsplit("__")[1].rsplit("/")[1] + "__NNLO_QCD.dat"
        if "NNLO" in order and (int(parameter_list["loop_induced"])) == 0:
            gnu.add_curve(plot_NNLO)
        if gnu.get_name().startswith("total_rate"):# or gnu.get_name().startswith("n_jet"): # these plots are always done per default, treat them as special case
            continue
        elif gnu.get_name().startswith("n_jet"): # for njets, treat it as special case and combine it with total rate in "-1" bin
            gnu.set_plot_properties("logscale_y",False)
            gnu.set_plot_properties("ylabel","{/Symbol s}")
            gnu.set_plot_properties("xlabel","")
            gnu.set_plot_properties("xtics_ratio","(\"total rate\" -0.5,\"0-jet\" 0.5,\"1-jet\" 1.5,\"2-jet\" 2.5)")
            gnu.set_plot_properties("norm_label","WRONG total rate (within same order) [%]")
            gnu.set_plot_properties("xmin",-1)
            gnu.set_plot_properties("ymin_ratio",0)
            gnu.set_plot_properties("ymax_ratio",1)
            gnu.set_plot_properties("ytics_ratio","(\"0\" 0,\"20\" 0.2,\"40\" 0.4,\"60\" 0.6,\"80\" 0.8,\"100\" 1)")
            
        elif gnu.get_name().startswith("pTveto"): # treat this later as special case inside the code (with distributiontype)
            continue
        # first you have to add all curves, then specify plot properties !!!
        # either give title directly
        #    gnu.set_plot_properties("title","this is the title in upper right corner")
        # or you set process, collider AND energy, and it is automatically created

        if parameter_list["coll_choice"]=="1":
            collider = "LHC"
        elif parameter_list["coll_choice"]=="2":
            collider = "Tevatron"
        else:
            out.print_error("collider_choice in parameter_list in routine collect_results is neither 1 nor 2.")
        energy = float(parameter_list["E"])*2/1000
        process = prc.get_nice_process_name()

        gnu.set_plot_properties("process",process)
        gnu.set_plot_properties("collider",collider)
        gnu.set_plot_properties("energy","%s TeV" % energy)
        #    gnu.set_plot_properties("reference","1111.1111")
        gnu.plot()
        # try:
        #     gnu.plot()
        # except:
        #     pass
    time.sleep(3)
    # combine the pdfs in gnuplot folder in one single pdf file
    # get all pdfs in gnuplot folder
    all_pdfs = sorted(glob.glob(pjoin(fold.result_folder_path,"gnuplot/*.pdf")))
    command = "pdfunite"
    if which(command):
       if all_plots and all_pdfs:
           # Appending all pdfs
           for pdf in all_pdfs:
               command += " \"%s\"" % pdf
               # Writing all the collected pages to a file
           combined_pdf_file = pjoin(fold.result_folder_path,"gnuplot","all_plots.pdf")
           command += " \"%s\"" % combined_pdf_file
           out.print_info("Combining all pdf files into single file \"all_plots.pdf\"...")
           print subprocess.Popen(command, shell=True, stdout=subprocess.PIPE).stdout.read()
    else:
        out.print_warning("Command \"pdfunite\" does not exist on this system. Skipping combination of *.pdf files...")
#}}}


##############################
# Here start the actual runs #
##############################

if not run_mode in ["run_results","run_gnuplot"]:
#### initialize cluster class
    if runmode == "cluster":
        cluster = get_cluster_class_from_name[cluster_name](config_list,verbose)
#### initialize run instance
run = run_class(runmode,grid_folder,main_run_folder,NLO_subtraction,order,set_parallel_runs,grid_assignment,include_loop_induced,config_list)
if run_mode in ["run","run_grid","run_without_pre"]:
#### run warmup to set up grids
    run.clear_warmup() # clear previous runs in grid dirs
    run.warmup(1) # run warmup
    print_restarted_runs() # if there are any, print out the runs that were restarted with different random seed
    if run.errors_flag: out.print_warning("Exception error in python jobs while running warmup.") # check if there were exception errors
    rerun_warmup(max_restarts) # if there are jobs that failed
    check_warmup() # if there are still failed grid runs, print warnings and wait for user input
    log.move_to_folder(pjoin(fold.log_folder_path,"grid_run")) # move all log files into a new created grid_run folder inside the log folder
if run_mode in ["run","run_pre","run_pre_and_main"]:
#### extrapolation run (pre run)
    check_grid_log() # check logs if grid_run has been done 
    log.clear_list("restarted_list.log") # remove previous items from list with restarted jobs
    if run_mode in ["run_pre","run_pre_and_main"]: run.clear_pre_run() # clear pre run folders of previous runs (already be clean in other runmodes)
    run.main_run(-1) # start pre run
    print_restarted_runs() # if there are any, print out the runs that were restarted with different random seed
    if run.errors_flag: out.print_warning("Exception error while doing extrapolation runs.") # check if there were exception errors
    rerun_pre_run(max_restarts) # if there are jobs that failed
    check_pre_run() # if there are still failed pre runs, print error and stop the code
    log.move_to_folder(pjoin(fold.log_folder_path,"pre_run")) # move all log files into a new created pre_run folder inside the log folder
    run.extrapolate_runtimes() # extrapolation of runtimes by combining pre run result 
#    try:
    run.print_pre_run()
#    except:
#        pass
    run.read_runtimes() # read in output of extrapolation run; read it already here to save the information
    check_parallel() # check wether the parallelization is not too high
if run_mode in ["run_without_pre","run_main_without_pre"]:
    shutil.copy(pjoin(fold.default_input_path,"runtime.dat"),pjoin(fold.run_folder_path,"result")) # copy pre-generated runtime.dat file
    run.read_runtimes() # read in output of extrapolation run when running main alone (no information from before)
    check_parallel() # check wether the parallelization is not too high
    run.clear_main_run() # clear main run folders of previous runs (already be clean in other runmodes)
if run_mode in ["run","run_main","run_pre_and_main","run_without_pre","run_main_without_pre"]:
#### cross section run (main run)
    #
    # 2do: check logs if pre_run has been done IN THIS CASE YOU SHOULD EXCLUDE ALL PRE RUNS FROM RESULTS
    # 2do: check wether runtimes file exists THIS IS ENOUGH, RIGHT?
    # 2do: introduce an equivalent to check_grid_log(): check_pre_run_log()
    #      if not run.pre_run_complete(): #and not run_mode == "run": why should this not also be asked in general run?
    #      dialog to abort or to run anyway (eg, if you have the grid run complete but the log files are missing)
    if run_mode == "run_main":
        run.read_runtimes() # read in output of extrapolation run when running main alone (no information from before)
        check_parallel() # check wether the parallelization is not too high
        run.clear_main_run() # clear main run folders of previous runs (already be clean in other runmodes)
    log.clear_list("restarted_list.log") # remove previous items from list with restarted jobs
    run.main_run(1) # start main run
    print_restarted_runs() # if there are any, print out the runs that were restarted with different random seed
    if run.errors_flag: out.print_warning("Exception error while running main processes.") # check if there were exception errors
    rerun_main_run(max_restarts) # if there are jobs that failed
    check_main_run() # if there are still failed main runs, print error and stop the code
    log.move_to_folder(pjoin(fold.log_folder_path,"main_run")) # move all log files into a new created main_run folder inside the log folder
if run_mode in ["run","run_pre_and_main","run_main","run_results","run_without_pre","run_main_without_pre"]:
#### collect and combine results
    run.clear_results() # remove previous results (if there are any)
    citation_list_run = cite.get_citation_list_run(order,NLO_subtraction)
    cite.write_citations(pjoin(fold.result_folder_path,"CITATIONS.bib"),citation_list_run) # write citations to file
    run.combine_results() # combine results and distributions and copy them to result folder
    out_distributions = ""
    if int(parameter_list["switch_distribution"]) == 1:
        out_distributions = " (including the distributions)" # print wether there are distributions
        run_gnuplot() # generate gnuplot output
    run.print_results_onscreen_and_to_summary_file() # write results on the screen and into summary file
    out.print_result("All results%s can be found in:" % out_distributions)
    out.print_result(fold.result_folder_path)
elif run_mode == "run_gnuplot":
    try:
        os.makedirs(pjoin(fold.result_folder_path,"gnuplot"))
    except:
        shutil.rmtree(pjoin(fold.result_folder_path,"gnuplot"))
        os.makedirs(pjoin(fold.result_folder_path,"gnuplot"))
    run_gnuplot() # generate gnuplot output
    
